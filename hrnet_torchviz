digraph {
	graph [size="905.1,905.1"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	140587589136048 [label="
 ()" fillcolor=darkolivegreen1]
	140587849424464 [label=MeanBackward0]
	140587849424528 -> 140587849424464
	140587849424528 [label=MkldnnConvolutionBackward]
	140587849424656 -> 140587849424528
	140587849424656 [label=ReluBackward1]
	140588144525456 -> 140587849424656
	140588144525456 [label=NativeBatchNormBackward]
	140588144525712 -> 140588144525456
	140588144525712 [label=MkldnnConvolutionBackward]
	140588144525904 -> 140588144525712
	140588144525904 [label=CatBackward]
	140588144526096 -> 140588144525904
	140588144526096 [label=ReluBackward1]
	140588144526416 -> 140588144526096
	140588144526416 [label=AddBackward0]
	140588144526608 -> 140588144526416
	140588144526608 [label=AddBackward0]
	140588144526480 -> 140588144526608
	140588144526480 [label=AddBackward0]
	140588144526864 -> 140588144526480
	140588144526864 [label=ReluBackward1]
	140588144527056 -> 140588144526864
	140588144527056 [label=AddBackward0]
	140588144527312 -> 140588144527056
	140588144527312 [label=NativeBatchNormBackward]
	140588144526544 -> 140588144527312
	140588144526544 [label=MkldnnConvolutionBackward]
	140588144527696 -> 140588144526544
	140588144527696 [label=ReluBackward1]
	140588144527760 -> 140588144527696
	140588144527760 [label=NativeBatchNormBackward]
	140588144528080 -> 140588144527760
	140588144528080 [label=MkldnnConvolutionBackward]
	140588144527504 -> 140588144528080
	140588144527504 [label=ReluBackward1]
	140588144528400 -> 140588144527504
	140588144528400 [label=AddBackward0]
	140588144528592 -> 140588144528400
	140588144528592 [label=NativeBatchNormBackward]
	140588144528336 -> 140588144528592
	140588144528336 [label=MkldnnConvolutionBackward]
	140588144528976 -> 140588144528336
	140588144528976 [label=ReluBackward1]
	140588144529040 -> 140588144528976
	140588144529040 [label=NativeBatchNormBackward]
	140588144529296 -> 140588144529040
	140588144529296 [label=MkldnnConvolutionBackward]
	140588144528784 -> 140588144529296
	140588144528784 [label=ReluBackward1]
	140588144537936 -> 140588144528784
	140588144537936 [label=AddBackward0]
	140588144538128 -> 140588144537936
	140588144538128 [label=NativeBatchNormBackward]
	140588144537872 -> 140588144538128
	140588144537872 [label=MkldnnConvolutionBackward]
	140588144538512 -> 140588144537872
	140588144538512 [label=ReluBackward1]
	140588144538576 -> 140588144538512
	140588144538576 [label=NativeBatchNormBackward]
	140588144538832 -> 140588144538576
	140588144538832 [label=MkldnnConvolutionBackward]
	140588144538320 -> 140588144538832
	140588144538320 [label=ReluBackward1]
	140588144539280 -> 140588144538320
	140588144539280 [label=AddBackward0]
	140588144539408 -> 140588144539280
	140588144539408 [label=NativeBatchNormBackward]
	140588144539600 -> 140588144539408
	140588144539600 [label=MkldnnConvolutionBackward]
	140588144539920 -> 140588144539600
	140588144539920 [label=ReluBackward1]
	140587841931024 -> 140588144539920
	140587841931024 [label=NativeBatchNormBackward]
	140587039160208 -> 140587841931024
	140587039160208 [label=MkldnnConvolutionBackward]
	140588144539728 -> 140587039160208
	140588144539728 [label=ReluBackward1]
	140587304754960 -> 140588144539728
	140587304754960 [label=AddBackward0]
	140587038820048 -> 140587304754960
	140587038820048 [label=AddBackward0]
	140587037651024 -> 140587038820048
	140587037651024 [label=AddBackward0]
	140587588026704 -> 140587037651024
	140587588026704 [label=ReluBackward1]
	140587720593040 -> 140587588026704
	140587720593040 [label=AddBackward0]
	140587720590480 -> 140587720593040
	140587720590480 [label=NativeBatchNormBackward]
	140587842056464 -> 140587720590480
	140587842056464 [label=MkldnnConvolutionBackward]
	140588144540240 -> 140587842056464
	140588144540240 [label=ReluBackward1]
	140588144540304 -> 140588144540240
	140588144540304 [label=NativeBatchNormBackward]
	140588144540624 -> 140588144540304
	140588144540624 [label=MkldnnConvolutionBackward]
	140587720545744 -> 140588144540624
	140587720545744 [label=ReluBackward1]
	140588144540944 -> 140587720545744
	140588144540944 [label=AddBackward0]
	140588144541072 -> 140588144540944
	140588144541072 [label=NativeBatchNormBackward]
	140588144541264 -> 140588144541072
	140588144541264 [label=MkldnnConvolutionBackward]
	140588144541584 -> 140588144541264
	140588144541584 [label=ReluBackward1]
	140588144554064 -> 140588144541584
	140588144554064 [label=NativeBatchNormBackward]
	140588144554256 -> 140588144554064
	140588144554256 [label=MkldnnConvolutionBackward]
	140588144541392 -> 140588144554256
	140588144541392 [label=ReluBackward1]
	140588144554704 -> 140588144541392
	140588144554704 [label=AddBackward0]
	140588144554832 -> 140588144554704
	140588144554832 [label=NativeBatchNormBackward]
	140588144555024 -> 140588144554832
	140588144555024 [label=MkldnnConvolutionBackward]
	140588144555344 -> 140588144555024
	140588144555344 [label=ReluBackward1]
	140588144555408 -> 140588144555344
	140588144555408 [label=NativeBatchNormBackward]
	140588144555664 -> 140588144555408
	140588144555664 [label=MkldnnConvolutionBackward]
	140588144555152 -> 140588144555664
	140588144555152 [label=ReluBackward1]
	140588144556112 -> 140588144555152
	140588144556112 [label=AddBackward0]
	140588144556240 -> 140588144556112
	140588144556240 [label=NativeBatchNormBackward]
	140588144556432 -> 140588144556240
	140588144556432 [label=MkldnnConvolutionBackward]
	140588144556752 -> 140588144556432
	140588144556752 [label=ReluBackward1]
	140588144556816 -> 140588144556752
	140588144556816 [label=NativeBatchNormBackward]
	140588144557072 -> 140588144556816
	140588144557072 [label=MkldnnConvolutionBackward]
	140588144556560 -> 140588144557072
	140588144556560 [label=ReluBackward1]
	140588144557520 -> 140588144556560
	140588144557520 [label=AddBackward0]
	140588144557648 -> 140588144557520
	140588144557648 [label=AddBackward0]
	140588144557840 -> 140588144557648
	140588144557840 [label=AddBackward0]
	140588144562320 -> 140588144557840
	140588144562320 [label=ReluBackward1]
	140588144562384 -> 140588144562320
	140588144562384 [label=AddBackward0]
	140588144562576 -> 140588144562384
	140588144562576 [label=NativeBatchNormBackward]
	140588144562768 -> 140588144562576
	140588144562768 [label=MkldnnConvolutionBackward]
	140588144563088 -> 140588144562768
	140588144563088 [label=ReluBackward1]
	140588144563152 -> 140588144563088
	140588144563152 [label=NativeBatchNormBackward]
	140588144563408 -> 140588144563152
	140588144563408 [label=MkldnnConvolutionBackward]
	140588144562896 -> 140588144563408
	140588144562896 [label=ReluBackward1]
	140588144563856 -> 140588144562896
	140588144563856 [label=AddBackward0]
	140588144563984 -> 140588144563856
	140588144563984 [label=NativeBatchNormBackward]
	140588144564176 -> 140588144563984
	140588144564176 [label=MkldnnConvolutionBackward]
	140588144564496 -> 140588144564176
	140588144564496 [label=ReluBackward1]
	140588144564560 -> 140588144564496
	140588144564560 [label=NativeBatchNormBackward]
	140588144564816 -> 140588144564560
	140588144564816 [label=MkldnnConvolutionBackward]
	140588144564304 -> 140588144564816
	140588144564304 [label=ReluBackward1]
	140588144565264 -> 140588144564304
	140588144565264 [label=AddBackward0]
	140588144565392 -> 140588144565264
	140588144565392 [label=NativeBatchNormBackward]
	140588144565584 -> 140588144565392
	140588144565584 [label=MkldnnConvolutionBackward]
	140588144565904 -> 140588144565584
	140588144565904 [label=ReluBackward1]
	140588144565968 -> 140588144565904
	140588144565968 [label=NativeBatchNormBackward]
	140588144578704 -> 140588144565968
	140588144578704 [label=MkldnnConvolutionBackward]
	140588144565712 -> 140588144578704
	140588144565712 [label=ReluBackward1]
	140588144579024 -> 140588144565712
	140588144579024 [label=AddBackward0]
	140588144579152 -> 140588144579024
	140588144579152 [label=NativeBatchNormBackward]
	140588144579344 -> 140588144579152
	140588144579344 [label=MkldnnConvolutionBackward]
	140588144579664 -> 140588144579344
	140588144579664 [label=ReluBackward1]
	140588144579728 -> 140588144579664
	140588144579728 [label=NativeBatchNormBackward]
	140588144579984 -> 140588144579728
	140588144579984 [label=MkldnnConvolutionBackward]
	140588144579472 -> 140588144579984
	140588144579472 [label=ReluBackward1]
	140588144580432 -> 140588144579472
	140588144580432 [label=AddBackward0]
	140588144580560 -> 140588144580432
	140588144580560 [label=AddBackward0]
	140588144580752 -> 140588144580560
	140588144580752 [label=ReluBackward1]
	140588144580944 -> 140588144580752
	140588144580944 [label=AddBackward0]
	140588144581136 -> 140588144580944
	140588144581136 [label=NativeBatchNormBackward]
	140588144581328 -> 140588144581136
	140588144581328 [label=MkldnnConvolutionBackward]
	140588144581648 -> 140588144581328
	140588144581648 [label=ReluBackward1]
	140588144581712 -> 140588144581648
	140588144581712 [label=NativeBatchNormBackward]
	140588144581968 -> 140588144581712
	140588144581968 [label=MkldnnConvolutionBackward]
	140588144581456 -> 140588144581968
	140588144581456 [label=ReluBackward1]
	140588144582416 -> 140588144581456
	140588144582416 [label=AddBackward0]
	140588144582352 -> 140588144582416
	140588144582352 [label=NativeBatchNormBackward]
	140588144595152 -> 140588144582352
	140588144595152 [label=MkldnnConvolutionBackward]
	140588144595408 -> 140588144595152
	140588144595408 [label=ReluBackward1]
	140588144595472 -> 140588144595408
	140588144595472 [label=NativeBatchNormBackward]
	140588144595728 -> 140588144595472
	140588144595728 [label=MkldnnConvolutionBackward]
	140588144595024 -> 140588144595728
	140588144595024 [label=ReluBackward1]
	140588144596176 -> 140588144595024
	140588144596176 [label=AddBackward0]
	140588144596368 -> 140588144596176
	140588144596368 [label=NativeBatchNormBackward]
	140588144596112 -> 140588144596368
	140588144596112 [label=MkldnnConvolutionBackward]
	140588144596752 -> 140588144596112
	140588144596752 [label=ReluBackward1]
	140588144596816 -> 140588144596752
	140588144596816 [label=NativeBatchNormBackward]
	140588144597072 -> 140588144596816
	140588144597072 [label=MkldnnConvolutionBackward]
	140588144596560 -> 140588144597072
	140588144596560 [label=ReluBackward1]
	140588144597520 -> 140588144596560
	140588144597520 [label=AddBackward0]
	140588144597648 -> 140588144597520
	140588144597648 [label=NativeBatchNormBackward]
	140588144597840 -> 140588144597648
	140588144597840 [label=MkldnnConvolutionBackward]
	140588144598160 -> 140588144597840
	140588144598160 [label=ReluBackward1]
	140588144598224 -> 140588144598160
	140588144598224 [label=NativeBatchNormBackward]
	140588144598480 -> 140588144598224
	140588144598480 [label=MkldnnConvolutionBackward]
	140588144597968 -> 140588144598480
	140588144597968 [label=ReluBackward1]
	140588144598928 -> 140588144597968
	140588144598928 [label=AddBackward0]
	140588144607376 -> 140588144598928
	140588144607376 [label=AddBackward0]
	140588144607504 -> 140588144607376
	140588144607504 [label=ReluBackward1]
	140588144607696 -> 140588144607504
	140588144607696 [label=AddBackward0]
	140588144607888 -> 140588144607696
	140588144607888 [label=NativeBatchNormBackward]
	140588144608080 -> 140588144607888
	140588144608080 [label=MkldnnConvolutionBackward]
	140588144608400 -> 140588144608080
	140588144608400 [label=ReluBackward1]
	140588144608464 -> 140588144608400
	140588144608464 [label=NativeBatchNormBackward]
	140588144608720 -> 140588144608464
	140588144608720 [label=MkldnnConvolutionBackward]
	140588144608208 -> 140588144608720
	140588144608208 [label=ReluBackward1]
	140588144609168 -> 140588144608208
	140588144609168 [label=AddBackward0]
	140588144609296 -> 140588144609168
	140588144609296 [label=NativeBatchNormBackward]
	140588144609488 -> 140588144609296
	140588144609488 [label=MkldnnConvolutionBackward]
	140588144609808 -> 140588144609488
	140588144609808 [label=ReluBackward1]
	140588144609872 -> 140588144609808
	140588144609872 [label=NativeBatchNormBackward]
	140588144610128 -> 140588144609872
	140588144610128 [label=MkldnnConvolutionBackward]
	140588144609616 -> 140588144610128
	140588144609616 [label=ReluBackward1]
	140588144610576 -> 140588144609616
	140588144610576 [label=AddBackward0]
	140588144610704 -> 140588144610576
	140588144610704 [label=NativeBatchNormBackward]
	140588144610896 -> 140588144610704
	140588144610896 [label=MkldnnConvolutionBackward]
	140588144611216 -> 140588144610896
	140588144611216 [label=ReluBackward1]
	140588144615504 -> 140588144611216
	140588144615504 [label=NativeBatchNormBackward]
	140588144615696 -> 140588144615504
	140588144615696 [label=MkldnnConvolutionBackward]
	140588144611024 -> 140588144615696
	140588144611024 [label=ReluBackward1]
	140588144616144 -> 140588144611024
	140588144616144 [label=AddBackward0]
	140588144616272 -> 140588144616144
	140588144616272 [label=NativeBatchNormBackward]
	140588144616464 -> 140588144616272
	140588144616464 [label=MkldnnConvolutionBackward]
	140588144616784 -> 140588144616464
	140588144616784 [label=ReluBackward1]
	140588144616848 -> 140588144616784
	140588144616848 [label=NativeBatchNormBackward]
	140588144617104 -> 140588144616848
	140588144617104 [label=MkldnnConvolutionBackward]
	140588144616592 -> 140588144617104
	140588144616592 [label=ReluBackward1]
	140588144617552 -> 140588144616592
	140588144617552 [label=AddBackward0]
	140588144617680 -> 140588144617552
	140588144617680 [label=AddBackward0]
	140588144617872 -> 140588144617680
	140588144617872 [label=ReluBackward1]
	140588144618064 -> 140588144617872
	140588144618064 [label=AddBackward0]
	140588144618256 -> 140588144618064
	140588144618256 [label=NativeBatchNormBackward]
	140588144618448 -> 140588144618256
	140588144618448 [label=MkldnnConvolutionBackward]
	140588144618768 -> 140588144618448
	140588144618768 [label=ReluBackward1]
	140588144618832 -> 140588144618768
	140588144618832 [label=NativeBatchNormBackward]
	140588144619088 -> 140588144618832
	140588144619088 [label=MkldnnConvolutionBackward]
	140588144618576 -> 140588144619088
	140588144618576 [label=ReluBackward1]
	140588144631952 -> 140588144618576
	140588144631952 [label=AddBackward0]
	140588144632016 -> 140588144631952
	140588144632016 [label=NativeBatchNormBackward]
	140588144632208 -> 140588144632016
	140588144632208 [label=MkldnnConvolutionBackward]
	140588144632528 -> 140588144632208
	140588144632528 [label=ReluBackward1]
	140588144632592 -> 140588144632528
	140588144632592 [label=NativeBatchNormBackward]
	140588144632848 -> 140588144632592
	140588144632848 [label=MkldnnConvolutionBackward]
	140588144632336 -> 140588144632848
	140588144632336 [label=ReluBackward1]
	140588144633296 -> 140588144632336
	140588144633296 [label=AddBackward0]
	140588144633424 -> 140588144633296
	140588144633424 [label=NativeBatchNormBackward]
	140588144633616 -> 140588144633424
	140588144633616 [label=MkldnnConvolutionBackward]
	140588144633936 -> 140588144633616
	140588144633936 [label=ReluBackward1]
	140588144634000 -> 140588144633936
	140588144634000 [label=NativeBatchNormBackward]
	140588144634256 -> 140588144634000
	140588144634256 [label=MkldnnConvolutionBackward]
	140588144633744 -> 140588144634256
	140588144633744 [label=ReluBackward1]
	140588144634704 -> 140588144633744
	140588144634704 [label=AddBackward0]
	140588144634832 -> 140588144634704
	140588144634832 [label=NativeBatchNormBackward]
	140588144635024 -> 140588144634832
	140588144635024 [label=MkldnnConvolutionBackward]
	140588144635344 -> 140588144635024
	140588144635344 [label=ReluBackward1]
	140588144635408 -> 140588144635344
	140588144635408 [label=NativeBatchNormBackward]
	140588144635664 -> 140588144635408
	140588144635664 [label=MkldnnConvolutionBackward]
	140588144635152 -> 140588144635664
	140588144635152 [label=ReluBackward1]
	140588144644368 -> 140588144635152
	140588144644368 [label=AddBackward0]
	140588144644496 -> 140588144644368
	140588144644496 [label=AddBackward0]
	140588144644688 -> 140588144644496
	140588144644688 [label=ReluBackward1]
	140588144644880 -> 140588144644688
	140588144644880 [label=AddBackward0]
	140588144645072 -> 140588144644880
	140588144645072 [label=NativeBatchNormBackward]
	140588144645264 -> 140588144645072
	140588144645264 [label=MkldnnConvolutionBackward]
	140588144645584 -> 140588144645264
	140588144645584 [label=ReluBackward1]
	140588144645648 -> 140588144645584
	140588144645648 [label=NativeBatchNormBackward]
	140588144645904 -> 140588144645648
	140588144645904 [label=MkldnnConvolutionBackward]
	140588144645392 -> 140588144645904
	140588144645392 [label=ReluBackward1]
	140588144646352 -> 140588144645392
	140588144646352 [label=AddBackward0]
	140588144646480 -> 140588144646352
	140588144646480 [label=NativeBatchNormBackward]
	140588144646672 -> 140588144646480
	140588144646672 [label=MkldnnConvolutionBackward]
	140588144646992 -> 140588144646672
	140588144646992 [label=ReluBackward1]
	140588144647056 -> 140588144646992
	140588144647056 [label=NativeBatchNormBackward]
	140588144647312 -> 140588144647056
	140588144647312 [label=MkldnnConvolutionBackward]
	140588144646800 -> 140588144647312
	140588144646800 [label=ReluBackward1]
	140588144647760 -> 140588144646800
	140588144647760 [label=AddBackward0]
	140588144647888 -> 140588144647760
	140588144647888 [label=NativeBatchNormBackward]
	140588144647696 -> 140588144647888
	140588144647696 [label=MkldnnConvolutionBackward]
	140588144652560 -> 140588144647696
	140588144652560 [label=ReluBackward1]
	140588144652624 -> 140588144652560
	140588144652624 [label=NativeBatchNormBackward]
	140588144652880 -> 140588144652624
	140588144652880 [label=MkldnnConvolutionBackward]
	140588144648144 -> 140588144652880
	140588144648144 [label=ReluBackward1]
	140588144653328 -> 140588144648144
	140588144653328 [label=AddBackward0]
	140588144653456 -> 140588144653328
	140588144653456 [label=NativeBatchNormBackward]
	140588144653648 -> 140588144653456
	140588144653648 [label=MkldnnConvolutionBackward]
	140588144653968 -> 140588144653648
	140588144653968 [label=ReluBackward1]
	140588144654032 -> 140588144653968
	140588144654032 [label=NativeBatchNormBackward]
	140588144654288 -> 140588144654032
	140588144654288 [label=MkldnnConvolutionBackward]
	140588144653776 -> 140588144654288
	140588144653776 [label=ReluBackward1]
	140588144654736 -> 140588144653776
	140588144654736 [label=AddBackward0]
	140588144654864 -> 140588144654736
	140588144654864 [label=ReluBackward1]
	140588144655056 -> 140588144654864
	140588144655056 [label=AddBackward0]
	140588144655248 -> 140588144655056
	140588144655248 [label=NativeBatchNormBackward]
	140588144655440 -> 140588144655248
	140588144655440 [label=MkldnnConvolutionBackward]
	140588144655760 -> 140588144655440
	140588144655760 [label=ReluBackward1]
	140588144655824 -> 140588144655760
	140588144655824 [label=NativeBatchNormBackward]
	140588144656080 -> 140588144655824
	140588144656080 [label=MkldnnConvolutionBackward]
	140588144655568 -> 140588144656080
	140588144655568 [label=ReluBackward1]
	140588144668880 -> 140588144655568
	140588144668880 [label=AddBackward0]
	140588144669008 -> 140588144668880
	140588144669008 [label=NativeBatchNormBackward]
	140588144669200 -> 140588144669008
	140588144669200 [label=MkldnnConvolutionBackward]
	140588144669520 -> 140588144669200
	140588144669520 [label=ReluBackward1]
	140588144669584 -> 140588144669520
	140588144669584 [label=NativeBatchNormBackward]
	140588144669840 -> 140588144669584
	140588144669840 [label=MkldnnConvolutionBackward]
	140588144669328 -> 140588144669840
	140588144669328 [label=ReluBackward1]
	140588144670288 -> 140588144669328
	140588144670288 [label=AddBackward0]
	140588144670416 -> 140588144670288
	140588144670416 [label=NativeBatchNormBackward]
	140588144670608 -> 140588144670416
	140588144670608 [label=MkldnnConvolutionBackward]
	140588144670928 -> 140588144670608
	140588144670928 [label=ReluBackward1]
	140588144670992 -> 140588144670928
	140588144670992 [label=NativeBatchNormBackward]
	140588144671248 -> 140588144670992
	140588144671248 [label=MkldnnConvolutionBackward]
	140588144670736 -> 140588144671248
	140588144670736 [label=ReluBackward1]
	140588144671696 -> 140588144670736
	140588144671696 [label=AddBackward0]
	140588144671824 -> 140588144671696
	140588144671824 [label=NativeBatchNormBackward]
	140588144672016 -> 140588144671824
	140588144672016 [label=MkldnnConvolutionBackward]
	140588144672336 -> 140588144672016
	140588144672336 [label=ReluBackward1]
	140588144672400 -> 140588144672336
	140588144672400 [label=NativeBatchNormBackward]
	140588144672592 -> 140588144672400
	140588144672592 [label=MkldnnConvolutionBackward]
	140588144672144 -> 140588144672592
	140588144672144 [label=ReluBackward1]
	140588144685456 -> 140588144672144
	140588144685456 [label=NativeBatchNormBackward]
	140588144685584 -> 140588144685456
	140588144685584 [label=MkldnnConvolutionBackward]
	140588144685904 -> 140588144685584
	140588144685904 [label=ReluBackward1]
	140588144685968 -> 140588144685904
	140588144685968 [label=AddBackward0]
	140588144686224 -> 140588144685968
	140588144686224 [label=NativeBatchNormBackward]
	140588144686416 -> 140588144686224
	140588144686416 [label=MkldnnConvolutionBackward]
	140588144686736 -> 140588144686416
	140588144686736 [label=ReluBackward1]
	140588144686800 -> 140588144686736
	140588144686800 [label=NativeBatchNormBackward]
	140588144687056 -> 140588144686800
	140588144687056 [label=MkldnnConvolutionBackward]
	140588144687376 -> 140588144687056
	140588144687376 [label=ReluBackward1]
	140588144687440 -> 140588144687376
	140588144687440 [label=NativeBatchNormBackward]
	140588144687696 -> 140588144687440
	140588144687696 [label=MkldnnConvolutionBackward]
	140588144686544 -> 140588144687696
	140588144686544 [label=ReluBackward1]
	140588144688144 -> 140588144686544
	140588144688144 [label=AddBackward0]
	140588144688272 -> 140588144688144
	140588144688272 [label=NativeBatchNormBackward]
	140588144688464 -> 140588144688272
	140588144688464 [label=MkldnnConvolutionBackward]
	140588144688784 -> 140588144688464
	140588144688784 [label=ReluBackward1]
	140588144688848 -> 140588144688784
	140588144688848 [label=NativeBatchNormBackward]
	140588144693392 -> 140588144688848
	140588144693392 [label=MkldnnConvolutionBackward]
	140588144693584 -> 140588144693392
	140588144693584 [label=ReluBackward1]
	140588144693648 -> 140588144693584
	140588144693648 [label=NativeBatchNormBackward]
	140588144693904 -> 140588144693648
	140588144693904 [label=MkldnnConvolutionBackward]
	140588144688592 -> 140588144693904
	140588144688592 [label=ReluBackward1]
	140588144694352 -> 140588144688592
	140588144694352 [label=AddBackward0]
	140588144694480 -> 140588144694352
	140588144694480 [label=NativeBatchNormBackward]
	140588144694672 -> 140588144694480
	140588144694672 [label=MkldnnConvolutionBackward]
	140588144694992 -> 140588144694672
	140588144694992 [label=ReluBackward1]
	140588144695056 -> 140588144694992
	140588144695056 [label=NativeBatchNormBackward]
	140588144695312 -> 140588144695056
	140588144695312 [label=MkldnnConvolutionBackward]
	140588144695632 -> 140588144695312
	140588144695632 [label=ReluBackward1]
	140588144695696 -> 140588144695632
	140588144695696 [label=NativeBatchNormBackward]
	140588144695952 -> 140588144695696
	140588144695952 [label=MkldnnConvolutionBackward]
	140588144694800 -> 140588144695952
	140588144694800 [label=ReluBackward1]
	140588144696400 -> 140588144694800
	140588144696400 [label=AddBackward0]
	140588144696528 -> 140588144696400
	140588144696528 [label=NativeBatchNormBackward]
	140588144696720 -> 140588144696528
	140588144696720 [label=MkldnnConvolutionBackward]
	140588144697040 -> 140588144696720
	140588144697040 [label=ReluBackward1]
	140588144697104 -> 140588144697040
	140588144697104 [label=NativeBatchNormBackward]
	140588144709776 -> 140588144697104
	140588144709776 [label=MkldnnConvolutionBackward]
	140588144710032 -> 140588144709776
	140588144710032 [label=ReluBackward1]
	140588144710096 -> 140588144710032
	140588144710096 [label=NativeBatchNormBackward]
	140588144710352 -> 140588144710096
	140588144710352 [label=MkldnnConvolutionBackward]
	140588144710672 -> 140588144710352
	140588144710672 [label=ReluBackward1]
	140588144710736 -> 140588144710672
	140588144710736 [label=NativeBatchNormBackward]
	140588144710992 -> 140588144710736
	140588144710992 [label=MkldnnConvolutionBackward]
	140588144711312 -> 140588144710992
	140588144711312 [label=ReluBackward1]
	140588144711376 -> 140588144711312
	140588144711376 [label=NativeBatchNormBackward]
	140588144711632 -> 140588144711376
	140588144711632 [label=MkldnnConvolutionBackward]
	140588144711952 -> 140588144711632
	140587842769536 [label="conv1.weight
 (64, 3, 3, 3)" fillcolor=lightblue]
	140587842769536 -> 140588144711952
	140588144711952 [label=AccumulateGrad]
	140588144711888 -> 140588144711376
	140587842770336 [label="bn1.weight
 (64)" fillcolor=lightblue]
	140587842770336 -> 140588144711888
	140588144711888 [label=AccumulateGrad]
	140588144711824 -> 140588144711376
	140587589069632 [label="bn1.bias
 (64)" fillcolor=lightblue]
	140587589069632 -> 140588144711824
	140588144711824 [label=AccumulateGrad]
	140588144711504 -> 140588144710992
	140587843182256 [label="conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	140587843182256 -> 140588144711504
	140588144711504 [label=AccumulateGrad]
	140588144711248 -> 140588144710736
	140587589132768 [label="bn2.weight
 (64)" fillcolor=lightblue]
	140587589132768 -> 140588144711248
	140588144711248 [label=AccumulateGrad]
	140588144711184 -> 140588144710736
	140587589135888 [label="bn2.bias
 (64)" fillcolor=lightblue]
	140587589135888 -> 140588144711184
	140588144711184 [label=AccumulateGrad]
	140588144710864 -> 140588144710352
	140587720508032 [label="layer1.0.conv1.weight
 (48, 64, 1, 1)" fillcolor=lightblue]
	140587720508032 -> 140588144710864
	140588144710864 [label=AccumulateGrad]
	140588144710608 -> 140588144710096
	140587720508992 [label="layer1.0.bn1.weight
 (48)" fillcolor=lightblue]
	140587720508992 -> 140588144710608
	140588144710608 [label=AccumulateGrad]
	140588144710544 -> 140588144710096
	140587720507792 [label="layer1.0.bn1.bias
 (48)" fillcolor=lightblue]
	140587720507792 -> 140588144710544
	140588144710544 [label=AccumulateGrad]
	140588144710224 -> 140588144709776
	140587720510432 [label="layer1.0.conv2.weight
 (48, 48, 3, 3)" fillcolor=lightblue]
	140587720510432 -> 140588144710224
	140588144710224 [label=AccumulateGrad]
	140588144709968 -> 140588144697104
	140587720510912 [label="layer1.0.bn2.weight
 (48)" fillcolor=lightblue]
	140587720510912 -> 140588144709968
	140588144709968 [label=AccumulateGrad]
	140588144709904 -> 140588144697104
	140587842615728 [label="layer1.0.bn2.bias
 (48)" fillcolor=lightblue]
	140587842615728 -> 140588144709904
	140588144709904 [label=AccumulateGrad]
	140588144697232 -> 140588144696720
	140587588120000 [label="layer1.0.conv3.weight
 (192, 48, 1, 1)" fillcolor=lightblue]
	140587588120000 -> 140588144697232
	140588144697232 [label=AccumulateGrad]
	140588144696976 -> 140588144696528
	140587588119680 [label="layer1.0.bn3.weight
 (192)" fillcolor=lightblue]
	140587588119680 -> 140588144696976
	140588144696976 [label=AccumulateGrad]
	140588144696912 -> 140588144696528
	140587588119600 [label="layer1.0.bn3.bias
 (192)" fillcolor=lightblue]
	140587588119600 -> 140588144696912
	140588144696912 [label=AccumulateGrad]
	140588144696848 -> 140588144696400
	140588144696848 [label=NativeBatchNormBackward]
	140588144696336 -> 140588144696848
	140588144696336 [label=MkldnnConvolutionBackward]
	140588144710672 -> 140588144696336
	140588144710288 -> 140588144696336
	140587039191440 [label="layer1.0.downsample.0.weight
 (192, 64, 1, 1)" fillcolor=lightblue]
	140587039191440 -> 140588144710288
	140588144710288 [label=AccumulateGrad]
	140588144697296 -> 140588144696848
	140587720509552 [label="layer1.0.downsample.1.weight
 (192)" fillcolor=lightblue]
	140587720509552 -> 140588144697296
	140588144697296 [label=AccumulateGrad]
	140588144697168 -> 140588144696848
	140587720511072 [label="layer1.0.downsample.1.bias
 (192)" fillcolor=lightblue]
	140587720511072 -> 140588144697168
	140588144697168 [label=AccumulateGrad]
	140588144696272 -> 140588144695952
	140587038933072 [label="layer1.1.conv1.weight
 (48, 192, 1, 1)" fillcolor=lightblue]
	140587038933072 -> 140588144696272
	140588144696272 [label=AccumulateGrad]
	140588144696208 -> 140588144695696
	140587304722336 [label="layer1.1.bn1.weight
 (48)" fillcolor=lightblue]
	140587304722336 -> 140588144696208
	140588144696208 [label=AccumulateGrad]
	140588144696144 -> 140588144695696
	140587850554176 [label="layer1.1.bn1.bias
 (48)" fillcolor=lightblue]
	140587850554176 -> 140588144696144
	140588144696144 [label=AccumulateGrad]
	140588144695824 -> 140588144695312
	140587304797760 [label="layer1.1.conv2.weight
 (48, 48, 3, 3)" fillcolor=lightblue]
	140587304797760 -> 140588144695824
	140588144695824 [label=AccumulateGrad]
	140588144695568 -> 140588144695056
	140587850561328 [label="layer1.1.bn2.weight
 (48)" fillcolor=lightblue]
	140587850561328 -> 140588144695568
	140588144695568 [label=AccumulateGrad]
	140588144695504 -> 140588144695056
	140587850561248 [label="layer1.1.bn2.bias
 (48)" fillcolor=lightblue]
	140587850561248 -> 140588144695504
	140588144695504 [label=AccumulateGrad]
	140588144695184 -> 140588144694672
	140587850562288 [label="layer1.1.conv3.weight
 (192, 48, 1, 1)" fillcolor=lightblue]
	140587850562288 -> 140588144695184
	140588144695184 [label=AccumulateGrad]
	140588144694928 -> 140588144694480
	140587850562368 [label="layer1.1.bn3.weight
 (192)" fillcolor=lightblue]
	140587850562368 -> 140588144694928
	140588144694928 [label=AccumulateGrad]
	140588144694864 -> 140588144694480
	140587850562528 [label="layer1.1.bn3.bias
 (192)" fillcolor=lightblue]
	140587850562528 -> 140588144694864
	140588144694864 [label=AccumulateGrad]
	140588144694800 -> 140588144694352
	140588144694224 -> 140588144693904
	140587850576832 [label="layer1.2.conv1.weight
 (48, 192, 1, 1)" fillcolor=lightblue]
	140587850576832 -> 140588144694224
	140588144694224 [label=AccumulateGrad]
	140588144694160 -> 140588144693648
	140587850577072 [label="layer1.2.bn1.weight
 (48)" fillcolor=lightblue]
	140587850577072 -> 140588144694160
	140588144694160 [label=AccumulateGrad]
	140588144694096 -> 140588144693648
	140587850576912 [label="layer1.2.bn1.bias
 (48)" fillcolor=lightblue]
	140587850576912 -> 140588144694096
	140588144694096 [label=AccumulateGrad]
	140588144693776 -> 140588144693392
	140587850571280 [label="layer1.2.conv2.weight
 (48, 48, 3, 3)" fillcolor=lightblue]
	140587850571280 -> 140588144693776
	140588144693776 [label=AccumulateGrad]
	140588144693520 -> 140588144688848
	140587850568480 [label="layer1.2.bn2.weight
 (48)" fillcolor=lightblue]
	140587850568480 -> 140588144693520
	140588144693520 [label=AccumulateGrad]
	140588144693456 -> 140588144688848
	140587850570720 [label="layer1.2.bn2.bias
 (48)" fillcolor=lightblue]
	140587850570720 -> 140588144693456
	140588144693456 [label=AccumulateGrad]
	140588144688976 -> 140588144688464
	140587850570800 [label="layer1.2.conv3.weight
 (192, 48, 1, 1)" fillcolor=lightblue]
	140587850570800 -> 140588144688976
	140588144688976 [label=AccumulateGrad]
	140588144688720 -> 140588144688272
	140587850567920 [label="layer1.2.bn3.weight
 (192)" fillcolor=lightblue]
	140587850567920 -> 140588144688720
	140588144688720 [label=AccumulateGrad]
	140588144688656 -> 140588144688272
	140587850557552 [label="layer1.2.bn3.bias
 (192)" fillcolor=lightblue]
	140587850557552 -> 140588144688656
	140588144688656 [label=AccumulateGrad]
	140588144688592 -> 140588144688144
	140588144688016 -> 140588144687696
	140587850557312 [label="layer1.3.conv1.weight
 (48, 192, 1, 1)" fillcolor=lightblue]
	140587850557312 -> 140588144688016
	140588144688016 [label=AccumulateGrad]
	140588144687952 -> 140588144687440
	140587850558512 [label="layer1.3.bn1.weight
 (48)" fillcolor=lightblue]
	140587850558512 -> 140588144687952
	140588144687952 [label=AccumulateGrad]
	140588144687888 -> 140588144687440
	140587850556592 [label="layer1.3.bn1.bias
 (48)" fillcolor=lightblue]
	140587850556592 -> 140588144687888
	140588144687888 [label=AccumulateGrad]
	140588144687568 -> 140588144687056
	140587850558592 [label="layer1.3.conv2.weight
 (48, 48, 3, 3)" fillcolor=lightblue]
	140587850558592 -> 140588144687568
	140588144687568 [label=AccumulateGrad]
	140588144687312 -> 140588144686800
	140587850558112 [label="layer1.3.bn2.weight
 (48)" fillcolor=lightblue]
	140587850558112 -> 140588144687312
	140588144687312 [label=AccumulateGrad]
	140588144687248 -> 140588144686800
	140587850557792 [label="layer1.3.bn2.bias
 (48)" fillcolor=lightblue]
	140587850557792 -> 140588144687248
	140588144687248 [label=AccumulateGrad]
	140588144686928 -> 140588144686416
	140587850558752 [label="layer1.3.conv3.weight
 (192, 48, 1, 1)" fillcolor=lightblue]
	140587850558752 -> 140588144686928
	140588144686928 [label=AccumulateGrad]
	140588144686672 -> 140588144686224
	140587850558912 [label="layer1.3.bn3.weight
 (192)" fillcolor=lightblue]
	140587850558912 -> 140588144686672
	140588144686672 [label=AccumulateGrad]
	140588144686608 -> 140588144686224
	140587850558992 [label="layer1.3.bn3.bias
 (192)" fillcolor=lightblue]
	140587850558992 -> 140588144686608
	140588144686608 [label=AccumulateGrad]
	140588144686544 -> 140588144685968
	140588144686096 -> 140588144685584
	140587720541200 [label="transition1.0.0.weight
 (48, 192, 3, 3)" fillcolor=lightblue]
	140587720541200 -> 140588144686096
	140588144686096 [label=AccumulateGrad]
	140588144685840 -> 140588144685456
	140587720543360 [label="transition1.0.1.weight
 (48)" fillcolor=lightblue]
	140587720543360 -> 140588144685840
	140588144685840 [label=AccumulateGrad]
	140588144685776 -> 140588144685456
	140587720543840 [label="transition1.0.1.bias
 (48)" fillcolor=lightblue]
	140587720543840 -> 140588144685776
	140588144685776 [label=AccumulateGrad]
	140588144685328 -> 140588144672592
	140587048543648 [label="stage2.0.branches.0.0.conv1.weight
 (48, 48, 3, 3)" fillcolor=lightblue]
	140587048543648 -> 140588144685328
	140588144685328 [label=AccumulateGrad]
	140588144685264 -> 140588144672400
	140587048543888 [label="stage2.0.branches.0.0.bn1.weight
 (48)" fillcolor=lightblue]
	140587048543888 -> 140588144685264
	140588144685264 [label=AccumulateGrad]
	140588144685200 -> 140588144672400
	140587048543968 [label="stage2.0.branches.0.0.bn1.bias
 (48)" fillcolor=lightblue]
	140587048543968 -> 140588144685200
	140588144685200 [label=AccumulateGrad]
	140588144672528 -> 140588144672016
	140587048544368 [label="stage2.0.branches.0.0.conv2.weight
 (48, 48, 3, 3)" fillcolor=lightblue]
	140587048544368 -> 140588144672528
	140588144672528 [label=AccumulateGrad]
	140588144672272 -> 140588144671824
	140587048544608 [label="stage2.0.branches.0.0.bn2.weight
 (48)" fillcolor=lightblue]
	140587048544608 -> 140588144672272
	140588144672272 [label=AccumulateGrad]
	140588144672208 -> 140588144671824
	140587048544688 [label="stage2.0.branches.0.0.bn2.bias
 (48)" fillcolor=lightblue]
	140587048544688 -> 140588144672208
	140588144672208 [label=AccumulateGrad]
	140588144672144 -> 140588144671696
	140588144671568 -> 140588144671248
	140587048545008 [label="stage2.0.branches.0.1.conv1.weight
 (48, 48, 3, 3)" fillcolor=lightblue]
	140587048545008 -> 140588144671568
	140588144671568 [label=AccumulateGrad]
	140588144671504 -> 140588144670992
	140587048545248 [label="stage2.0.branches.0.1.bn1.weight
 (48)" fillcolor=lightblue]
	140587048545248 -> 140588144671504
	140588144671504 [label=AccumulateGrad]
	140588144671440 -> 140588144670992
	140587048545328 [label="stage2.0.branches.0.1.bn1.bias
 (48)" fillcolor=lightblue]
	140587048545328 -> 140588144671440
	140588144671440 [label=AccumulateGrad]
	140588144671120 -> 140588144670608
	140587048545728 [label="stage2.0.branches.0.1.conv2.weight
 (48, 48, 3, 3)" fillcolor=lightblue]
	140587048545728 -> 140588144671120
	140588144671120 [label=AccumulateGrad]
	140588144670864 -> 140588144670416
	140587048545968 [label="stage2.0.branches.0.1.bn2.weight
 (48)" fillcolor=lightblue]
	140587048545968 -> 140588144670864
	140588144670864 [label=AccumulateGrad]
	140588144670800 -> 140588144670416
	140587048546048 [label="stage2.0.branches.0.1.bn2.bias
 (48)" fillcolor=lightblue]
	140587048546048 -> 140588144670800
	140588144670800 [label=AccumulateGrad]
	140588144670736 -> 140588144670288
	140588144670160 -> 140588144669840
	140587048612080 [label="stage2.0.branches.0.2.conv1.weight
 (48, 48, 3, 3)" fillcolor=lightblue]
	140587048612080 -> 140588144670160
	140588144670160 [label=AccumulateGrad]
	140588144670096 -> 140588144669584
	140587048612320 [label="stage2.0.branches.0.2.bn1.weight
 (48)" fillcolor=lightblue]
	140587048612320 -> 140588144670096
	140588144670096 [label=AccumulateGrad]
	140588144670032 -> 140588144669584
	140587048612400 [label="stage2.0.branches.0.2.bn1.bias
 (48)" fillcolor=lightblue]
	140587048612400 -> 140588144670032
	140588144670032 [label=AccumulateGrad]
	140588144669712 -> 140588144669200
	140587048612800 [label="stage2.0.branches.0.2.conv2.weight
 (48, 48, 3, 3)" fillcolor=lightblue]
	140587048612800 -> 140588144669712
	140588144669712 [label=AccumulateGrad]
	140588144669456 -> 140588144669008
	140587048613040 [label="stage2.0.branches.0.2.bn2.weight
 (48)" fillcolor=lightblue]
	140587048613040 -> 140588144669456
	140588144669456 [label=AccumulateGrad]
	140588144669392 -> 140588144669008
	140587048613120 [label="stage2.0.branches.0.2.bn2.bias
 (48)" fillcolor=lightblue]
	140587048613120 -> 140588144669392
	140588144669392 [label=AccumulateGrad]
	140588144669328 -> 140588144668880
	140588144668752 -> 140588144656080
	140587048613520 [label="stage2.0.branches.0.3.conv1.weight
 (48, 48, 3, 3)" fillcolor=lightblue]
	140587048613520 -> 140588144668752
	140588144668752 [label=AccumulateGrad]
	140588144656336 -> 140588144655824
	140587048613760 [label="stage2.0.branches.0.3.bn1.weight
 (48)" fillcolor=lightblue]
	140587048613760 -> 140588144656336
	140588144656336 [label=AccumulateGrad]
	140588144656272 -> 140588144655824
	140587048613840 [label="stage2.0.branches.0.3.bn1.bias
 (48)" fillcolor=lightblue]
	140587048613840 -> 140588144656272
	140588144656272 [label=AccumulateGrad]
	140588144655952 -> 140588144655440
	140587048614240 [label="stage2.0.branches.0.3.conv2.weight
 (48, 48, 3, 3)" fillcolor=lightblue]
	140587048614240 -> 140588144655952
	140588144655952 [label=AccumulateGrad]
	140588144655696 -> 140588144655248
	140587048614480 [label="stage2.0.branches.0.3.bn2.weight
 (48)" fillcolor=lightblue]
	140587048614480 -> 140588144655696
	140588144655696 [label=AccumulateGrad]
	140588144655632 -> 140588144655248
	140587048614560 [label="stage2.0.branches.0.3.bn2.bias
 (48)" fillcolor=lightblue]
	140587048614560 -> 140588144655632
	140588144655632 [label=AccumulateGrad]
	140588144655568 -> 140588144655056
	140588144655184 -> 140588144654736
	140588144655184 [label=UpsampleBilinear2DBackward1]
	140587849424400 -> 140588144655184
	140587849424400 [label=NativeBatchNormBackward]
	140588144655504 -> 140587849424400
	140588144655504 [label=MkldnnConvolutionBackward]
	140588144669776 -> 140588144655504
	140588144669776 [label=ReluBackward1]
	140588144669264 -> 140588144669776
	140588144669264 [label=AddBackward0]
	140588144671184 -> 140588144669264
	140588144671184 [label=NativeBatchNormBackward]
	140588144668944 -> 140588144671184
	140588144668944 [label=MkldnnConvolutionBackward]
	140588144671376 -> 140588144668944
	140588144671376 [label=ReluBackward1]
	140588144672720 -> 140588144671376
	140588144672720 [label=NativeBatchNormBackward]
	140588144671760 -> 140588144672720
	140588144671760 [label=MkldnnConvolutionBackward]
	140588144670544 -> 140588144671760
	140588144670544 [label=ReluBackward1]
	140588144685712 -> 140588144670544
	140588144685712 [label=AddBackward0]
	140588144686352 -> 140588144685712
	140588144686352 [label=NativeBatchNormBackward]
	140588144685136 -> 140588144686352
	140588144685136 [label=MkldnnConvolutionBackward]
	140588144686864 -> 140588144685136
	140588144686864 [label=ReluBackward1]
	140588144687824 -> 140588144686864
	140588144687824 [label=NativeBatchNormBackward]
	140588144688208 -> 140588144687824
	140588144688208 [label=MkldnnConvolutionBackward]
	140588144687184 -> 140588144688208
	140588144687184 [label=ReluBackward1]
	140588144694032 -> 140588144687184
	140588144694032 [label=AddBackward0]
	140588144694608 -> 140588144694032
	140588144694608 [label=NativeBatchNormBackward]
	140588144693328 -> 140588144694608
	140588144693328 [label=MkldnnConvolutionBackward]
	140588144695120 -> 140588144693328
	140588144695120 [label=ReluBackward1]
	140588144696080 -> 140588144695120
	140588144696080 [label=NativeBatchNormBackward]
	140588144709712 -> 140588144696080
	140588144709712 [label=MkldnnConvolutionBackward]
	140588144695440 -> 140588144709712
	140588144695440 [label=ReluBackward1]
	140588144710480 -> 140588144695440
	140588144710480 [label=AddBackward0]
	140588144711440 -> 140588144710480
	140588144711440 [label=NativeBatchNormBackward]
	140588144710928 -> 140588144711440
	140588144710928 [label=MkldnnConvolutionBackward]
	140588144712464 -> 140588144710928
	140588144712464 [label=ReluBackward1]
	140588144712528 -> 140588144712464
	140588144712528 [label=NativeBatchNormBackward]
	140588144712848 -> 140588144712528
	140588144712848 [label=MkldnnConvolutionBackward]
	140588144712144 -> 140588144712848
	140588144712144 [label=ReluBackward1]
	140588144713168 -> 140588144712144
	140588144713168 [label=NativeBatchNormBackward]
	140588144713360 -> 140588144713168
	140588144713360 [label=MkldnnConvolutionBackward]
	140588144685904 -> 140588144713360
	140588144713552 -> 140588144713360
	140587048542288 [label="transition1.1.0.0.weight
 (96, 192, 3, 3)" fillcolor=lightblue]
	140587048542288 -> 140588144713552
	140588144713552 [label=AccumulateGrad]
	140588144713488 -> 140588144713168
	140587048542448 [label="transition1.1.0.1.weight
 (96)" fillcolor=lightblue]
	140587048542448 -> 140588144713488
	140588144713488 [label=AccumulateGrad]
	140588144713104 -> 140588144713168
	140587048542528 [label="transition1.1.0.1.bias
 (96)" fillcolor=lightblue]
	140587048542528 -> 140588144713104
	140588144713104 [label=AccumulateGrad]
	140588144713040 -> 140588144712848
	140587048615120 [label="stage2.0.branches.1.0.conv1.weight
 (96, 96, 3, 3)" fillcolor=lightblue]
	140587048615120 -> 140588144713040
	140588144713040 [label=AccumulateGrad]
	140588144712976 -> 140588144712528
	140587048615360 [label="stage2.0.branches.1.0.bn1.weight
 (96)" fillcolor=lightblue]
	140587048615360 -> 140588144712976
	140588144712976 [label=AccumulateGrad]
	140588144712720 -> 140588144712528
	140587048615440 [label="stage2.0.branches.1.0.bn1.bias
 (96)" fillcolor=lightblue]
	140587048615440 -> 140588144712720
	140588144712720 [label=AccumulateGrad]
	140588144712656 -> 140588144710928
	140587048615840 [label="stage2.0.branches.1.0.conv2.weight
 (96, 96, 3, 3)" fillcolor=lightblue]
	140587048615840 -> 140588144712656
	140588144712656 [label=AccumulateGrad]
	140588144712016 -> 140588144711440
	140587048677616 [label="stage2.0.branches.1.0.bn2.weight
 (96)" fillcolor=lightblue]
	140587048677616 -> 140588144712016
	140588144712016 [label=AccumulateGrad]
	140588144712336 -> 140588144711440
	140587048677696 [label="stage2.0.branches.1.0.bn2.bias
 (96)" fillcolor=lightblue]
	140587048677696 -> 140588144712336
	140588144712336 [label=AccumulateGrad]
	140588144712144 -> 140588144710480
	140588144711120 -> 140588144709712
	140587048678096 [label="stage2.0.branches.1.1.conv1.weight
 (96, 96, 3, 3)" fillcolor=lightblue]
	140587048678096 -> 140588144711120
	140588144711120 [label=AccumulateGrad]
	140588144711568 -> 140588144696080
	140587048678336 [label="stage2.0.branches.1.1.bn1.weight
 (96)" fillcolor=lightblue]
	140587048678336 -> 140588144711568
	140588144711568 [label=AccumulateGrad]
	140588144710160 -> 140588144696080
	140587048678416 [label="stage2.0.branches.1.1.bn1.bias
 (96)" fillcolor=lightblue]
	140587048678416 -> 140588144710160
	140588144710160 [label=AccumulateGrad]
	140588144695760 -> 140588144693328
	140587048678816 [label="stage2.0.branches.1.1.conv2.weight
 (96, 96, 3, 3)" fillcolor=lightblue]
	140587048678816 -> 140588144695760
	140588144695760 [label=AccumulateGrad]
	140588144695888 -> 140588144694608
	140587048679056 [label="stage2.0.branches.1.1.bn2.weight
 (96)" fillcolor=lightblue]
	140587048679056 -> 140588144695888
	140588144695888 [label=AccumulateGrad]
	140588144696656 -> 140588144694608
	140587048679136 [label="stage2.0.branches.1.1.bn2.bias
 (96)" fillcolor=lightblue]
	140587048679136 -> 140588144696656
	140588144696656 [label=AccumulateGrad]
	140588144695440 -> 140588144694032
	140588144693840 -> 140588144688208
	140587048679536 [label="stage2.0.branches.1.2.conv1.weight
 (96, 96, 3, 3)" fillcolor=lightblue]
	140587048679536 -> 140588144693840
	140588144693840 [label=AccumulateGrad]
	140588144688528 -> 140588144687824
	140587048679776 [label="stage2.0.branches.1.2.bn1.weight
 (96)" fillcolor=lightblue]
	140587048679776 -> 140588144688528
	140588144688528 [label=AccumulateGrad]
	140588144688912 -> 140588144687824
	140587048679856 [label="stage2.0.branches.1.2.bn1.bias
 (96)" fillcolor=lightblue]
	140587048679856 -> 140588144688912
	140588144688912 [label=AccumulateGrad]
	140588144687504 -> 140588144685136
	140587048680256 [label="stage2.0.branches.1.2.conv2.weight
 (96, 96, 3, 3)" fillcolor=lightblue]
	140587048680256 -> 140588144687504
	140588144687504 [label=AccumulateGrad]
	140588144687632 -> 140588144686352
	140587048680496 [label="stage2.0.branches.1.2.bn2.weight
 (96)" fillcolor=lightblue]
	140587048680496 -> 140588144687632
	140588144687632 [label=AccumulateGrad]
	140588144689104 -> 140588144686352
	140587048680576 [label="stage2.0.branches.1.2.bn2.bias
 (96)" fillcolor=lightblue]
	140587048680576 -> 140588144689104
	140588144689104 [label=AccumulateGrad]
	140588144687184 -> 140588144685712
	140588144685392 -> 140588144671760
	140587048680976 [label="stage2.0.branches.1.3.conv1.weight
 (96, 96, 3, 3)" fillcolor=lightblue]
	140587048680976 -> 140588144685392
	140588144685392 [label=AccumulateGrad]
	140588144672080 -> 140588144672720
	140587048681216 [label="stage2.0.branches.1.3.bn1.weight
 (96)" fillcolor=lightblue]
	140587048681216 -> 140588144672080
	140588144672080 [label=AccumulateGrad]
	140588144672464 -> 140588144672720
	140587048681296 [label="stage2.0.branches.1.3.bn1.bias
 (96)" fillcolor=lightblue]
	140587048681296 -> 140588144672464
	140588144672464 [label=AccumulateGrad]
	140588144671056 -> 140588144668944
	140587048726848 [label="stage2.0.branches.1.3.conv2.weight
 (96, 96, 3, 3)" fillcolor=lightblue]
	140587048726848 -> 140588144671056
	140588144671056 [label=AccumulateGrad]
	140588144670352 -> 140588144671184
	140587048727088 [label="stage2.0.branches.1.3.bn2.weight
 (96)" fillcolor=lightblue]
	140587048727088 -> 140588144670352
	140588144670352 [label=AccumulateGrad]
	140588144670224 -> 140588144671184
	140587048727168 [label="stage2.0.branches.1.3.bn2.bias
 (96)" fillcolor=lightblue]
	140587048727168 -> 140588144670224
	140588144670224 [label=AccumulateGrad]
	140588144670544 -> 140588144669264
	140588144668816 -> 140588144655504
	140587048727808 [label="stage2.0.fuse_layers.0.1.0.weight
 (48, 96, 1, 1)" fillcolor=lightblue]
	140587048727808 -> 140588144668816
	140588144668816 [label=AccumulateGrad]
	140588144656208 -> 140587849424400
	140587048728048 [label="stage2.0.fuse_layers.0.1.1.weight
 (48)" fillcolor=lightblue]
	140587048728048 -> 140588144656208
	140588144656208 [label=AccumulateGrad]
	140588144655888 -> 140587849424400
	140587048728128 [label="stage2.0.fuse_layers.0.1.1.bias
 (48)" fillcolor=lightblue]
	140587048728128 -> 140588144655888
	140588144655888 [label=AccumulateGrad]
	140588144654608 -> 140588144654288
	140587309073504 [label="stage3.0.branches.0.0.conv1.weight
 (48, 48, 3, 3)" fillcolor=lightblue]
	140587309073504 -> 140588144654608
	140588144654608 [label=AccumulateGrad]
	140588144654544 -> 140588144654032
	140587309073744 [label="stage3.0.branches.0.0.bn1.weight
 (48)" fillcolor=lightblue]
	140587309073744 -> 140588144654544
	140588144654544 [label=AccumulateGrad]
	140588144654480 -> 140588144654032
	140587309073824 [label="stage3.0.branches.0.0.bn1.bias
 (48)" fillcolor=lightblue]
	140587309073824 -> 140588144654480
	140588144654480 [label=AccumulateGrad]
	140588144654160 -> 140588144653648
	140587309074224 [label="stage3.0.branches.0.0.conv2.weight
 (48, 48, 3, 3)" fillcolor=lightblue]
	140587309074224 -> 140588144654160
	140588144654160 [label=AccumulateGrad]
	140588144653904 -> 140588144653456
	140587309074464 [label="stage3.0.branches.0.0.bn2.weight
 (48)" fillcolor=lightblue]
	140587309074464 -> 140588144653904
	140588144653904 [label=AccumulateGrad]
	140588144653840 -> 140588144653456
	140587309074544 [label="stage3.0.branches.0.0.bn2.bias
 (48)" fillcolor=lightblue]
	140587309074544 -> 140588144653840
	140588144653840 [label=AccumulateGrad]
	140588144653776 -> 140588144653328
	140588144653200 -> 140588144652880
	140587309074944 [label="stage3.0.branches.0.1.conv1.weight
 (48, 48, 3, 3)" fillcolor=lightblue]
	140587309074944 -> 140588144653200
	140588144653200 [label=AccumulateGrad]
	140588144653136 -> 140588144652624
	140587309075184 [label="stage3.0.branches.0.1.bn1.weight
 (48)" fillcolor=lightblue]
	140587309075184 -> 140588144653136
	140588144653136 [label=AccumulateGrad]
	140588144653072 -> 140588144652624
	140587309075264 [label="stage3.0.branches.0.1.bn1.bias
 (48)" fillcolor=lightblue]
	140587309075264 -> 140588144653072
	140588144653072 [label=AccumulateGrad]
	140588144652752 -> 140588144647696
	140587309075664 [label="stage3.0.branches.0.1.conv2.weight
 (48, 48, 3, 3)" fillcolor=lightblue]
	140587309075664 -> 140588144652752
	140588144652752 [label=AccumulateGrad]
	140588144652368 -> 140588144647888
	140587309075904 [label="stage3.0.branches.0.1.bn2.weight
 (48)" fillcolor=lightblue]
	140587309075904 -> 140588144652368
	140588144652368 [label=AccumulateGrad]
	140588144652432 -> 140588144647888
	140587309075984 [label="stage3.0.branches.0.1.bn2.bias
 (48)" fillcolor=lightblue]
	140587309075984 -> 140588144652432
	140588144652432 [label=AccumulateGrad]
	140588144648144 -> 140588144647760
	140588144647632 -> 140588144647312
	140587309076384 [label="stage3.0.branches.0.2.conv1.weight
 (48, 48, 3, 3)" fillcolor=lightblue]
	140587309076384 -> 140588144647632
	140588144647632 [label=AccumulateGrad]
	140588144647568 -> 140588144647056
	140587309142256 [label="stage3.0.branches.0.2.bn1.weight
 (48)" fillcolor=lightblue]
	140587309142256 -> 140588144647568
	140588144647568 [label=AccumulateGrad]
	140588144647504 -> 140588144647056
	140587309142336 [label="stage3.0.branches.0.2.bn1.bias
 (48)" fillcolor=lightblue]
	140587309142336 -> 140588144647504
	140588144647504 [label=AccumulateGrad]
	140588144647184 -> 140588144646672
	140587309142736 [label="stage3.0.branches.0.2.conv2.weight
 (48, 48, 3, 3)" fillcolor=lightblue]
	140587309142736 -> 140588144647184
	140588144647184 [label=AccumulateGrad]
	140588144646928 -> 140588144646480
	140587309142976 [label="stage3.0.branches.0.2.bn2.weight
 (48)" fillcolor=lightblue]
	140587309142976 -> 140588144646928
	140588144646928 [label=AccumulateGrad]
	140588144646864 -> 140588144646480
	140587309143056 [label="stage3.0.branches.0.2.bn2.bias
 (48)" fillcolor=lightblue]
	140587309143056 -> 140588144646864
	140588144646864 [label=AccumulateGrad]
	140588144646800 -> 140588144646352
	140588144646224 -> 140588144645904
	140587309143456 [label="stage3.0.branches.0.3.conv1.weight
 (48, 48, 3, 3)" fillcolor=lightblue]
	140587309143456 -> 140588144646224
	140588144646224 [label=AccumulateGrad]
	140588144646160 -> 140588144645648
	140587309143696 [label="stage3.0.branches.0.3.bn1.weight
 (48)" fillcolor=lightblue]
	140587309143696 -> 140588144646160
	140588144646160 [label=AccumulateGrad]
	140588144646096 -> 140588144645648
	140587309143776 [label="stage3.0.branches.0.3.bn1.bias
 (48)" fillcolor=lightblue]
	140587309143776 -> 140588144646096
	140588144646096 [label=AccumulateGrad]
	140588144645776 -> 140588144645264
	140587309144176 [label="stage3.0.branches.0.3.conv2.weight
 (48, 48, 3, 3)" fillcolor=lightblue]
	140587309144176 -> 140588144645776
	140588144645776 [label=AccumulateGrad]
	140588144645520 -> 140588144645072
	140587309144416 [label="stage3.0.branches.0.3.bn2.weight
 (48)" fillcolor=lightblue]
	140587309144416 -> 140588144645520
	140588144645520 [label=AccumulateGrad]
	140588144645456 -> 140588144645072
	140587309144496 [label="stage3.0.branches.0.3.bn2.bias
 (48)" fillcolor=lightblue]
	140587309144496 -> 140588144645456
	140588144645456 [label=AccumulateGrad]
	140588144645392 -> 140588144644880
	140588144645008 -> 140588144644496
	140588144645008 [label=UpsampleBilinear2DBackward1]
	140588144645200 -> 140588144645008
	140588144645200 [label=NativeBatchNormBackward]
	140588144644304 -> 140588144645200
	140588144644304 [label=MkldnnConvolutionBackward]
	140588144646288 -> 140588144644304
	140588144646288 [label=ReluBackward1]
	140588144646032 -> 140588144646288
	140588144646032 [label=AddBackward0]
	140588144648016 -> 140588144646032
	140588144648016 [label=NativeBatchNormBackward]
	140588144648080 -> 140588144648016
	140588144648080 [label=MkldnnConvolutionBackward]
	140588144652816 -> 140588144648080
	140588144652816 [label=ReluBackward1]
	140588144654800 -> 140588144652816
	140588144654800 [label=NativeBatchNormBackward]
	140588144653584 -> 140588144654800
	140588144653584 [label=MkldnnConvolutionBackward]
	140588144647440 -> 140588144653584
	140588144647440 [label=ReluBackward1]
	140588144655120 -> 140588144647440
	140588144655120 [label=AddBackward0]
	140588144654416 -> 140588144655120
	140588144654416 [label=NativeBatchNormBackward]
	140588144669648 -> 140588144654416
	140588144669648 [label=MkldnnConvolutionBackward]
	140588144688400 -> 140588144669648
	140588144688400 [label=ReluBackward1]
	140588144685520 -> 140588144688400
	140588144685520 [label=NativeBatchNormBackward]
	140588144686480 -> 140588144685520
	140588144686480 [label=MkldnnConvolutionBackward]
	140588144671632 -> 140588144686480
	140588144671632 [label=ReluBackward1]
	140588144696464 -> 140588144671632
	140588144696464 [label=AddBackward0]
	140588144694416 -> 140588144696464
	140588144694416 [label=NativeBatchNormBackward]
	140588144712784 -> 140588144694416
	140588144712784 [label=MkldnnConvolutionBackward]
	140588144712912 -> 140588144712784
	140588144712912 [label=ReluBackward1]
	140588144713616 -> 140588144712912
	140588144713616 [label=NativeBatchNormBackward]
	140588144713296 -> 140588144713616
	140588144713296 [label=MkldnnConvolutionBackward]
	140588144712272 -> 140588144713296
	140588144712272 [label=ReluBackward1]
	140584538546512 -> 140588144712272
	140584538546512 [label=AddBackward0]
	140584538546704 -> 140584538546512
	140584538546704 [label=NativeBatchNormBackward]
	140584538546448 -> 140584538546704
	140584538546448 [label=MkldnnConvolutionBackward]
	140584538547088 -> 140584538546448
	140584538547088 [label=ReluBackward1]
	140584538547152 -> 140584538547088
	140584538547152 [label=NativeBatchNormBackward]
	140584538547472 -> 140584538547152
	140584538547472 [label=MkldnnConvolutionBackward]
	140584538546896 -> 140584538547472
	140584538546896 [label=ReluBackward1]
	140584538547792 -> 140584538546896
	140584538547792 [label=AddBackward0]
	140584538547984 -> 140584538547792
	140584538547984 [label=NativeBatchNormBackward]
	140584538548112 -> 140584538547984
	140584538548112 [label=MkldnnConvolutionBackward]
	140588144654864 -> 140584538548112
	140584538548304 -> 140584538548112
	140587048728768 [label="stage2.0.fuse_layers.1.0.0.0.weight
 (96, 48, 3, 3)" fillcolor=lightblue]
	140587048728768 -> 140584538548304
	140584538548304 [label=AccumulateGrad]
	140584538548240 -> 140584538547984
	140587048729008 [label="stage2.0.fuse_layers.1.0.0.1.weight
 (96)" fillcolor=lightblue]
	140587048729008 -> 140584538548240
	140584538548240 [label=AccumulateGrad]
	140584538547920 -> 140584538547984
	140587048729088 [label="stage2.0.fuse_layers.1.0.0.1.bias
 (96)" fillcolor=lightblue]
	140587048729088 -> 140584538547920
	140584538547920 [label=AccumulateGrad]
	140588144669776 -> 140584538547792
	140584538547664 -> 140584538547472
	140587309145056 [label="stage3.0.branches.1.0.conv1.weight
 (96, 96, 3, 3)" fillcolor=lightblue]
	140587309145056 -> 140584538547664
	140584538547664 [label=AccumulateGrad]
	140584538547600 -> 140584538547152
	140587309145296 [label="stage3.0.branches.1.0.bn1.weight
 (96)" fillcolor=lightblue]
	140587309145296 -> 140584538547600
	140584538547600 [label=AccumulateGrad]
	140584538547344 -> 140584538547152
	140587309145376 [label="stage3.0.branches.1.0.bn1.bias
 (96)" fillcolor=lightblue]
	140587309145376 -> 140584538547344
	140584538547344 [label=AccumulateGrad]
	140584538547280 -> 140584538546448
	140587309145776 [label="stage3.0.branches.1.0.conv2.weight
 (96, 96, 3, 3)" fillcolor=lightblue]
	140587309145776 -> 140584538547280
	140584538547280 [label=AccumulateGrad]
	140584538547024 -> 140584538546704
	140587309146016 [label="stage3.0.branches.1.0.bn2.weight
 (96)" fillcolor=lightblue]
	140587309146016 -> 140584538547024
	140584538547024 [label=AccumulateGrad]
	140584538546960 -> 140584538546704
	140587309199440 [label="stage3.0.branches.1.0.bn2.bias
 (96)" fillcolor=lightblue]
	140587309199440 -> 140584538546960
	140584538546960 [label=AccumulateGrad]
	140584538546896 -> 140584538546512
	140584538546384 -> 140588144713296
	140587309199840 [label="stage3.0.branches.1.1.conv1.weight
 (96, 96, 3, 3)" fillcolor=lightblue]
	140587309199840 -> 140584538546384
	140584538546384 [label=AccumulateGrad]
	140588144713424 -> 140588144713616
	140587309200080 [label="stage3.0.branches.1.1.bn1.weight
 (96)" fillcolor=lightblue]
	140587309200080 -> 140588144713424
	140588144713424 [label=AccumulateGrad]
	140584538546320 -> 140588144713616
	140587309200160 [label="stage3.0.branches.1.1.bn1.bias
 (96)" fillcolor=lightblue]
	140587309200160 -> 140584538546320
	140584538546320 [label=AccumulateGrad]
	140588144712592 -> 140588144712784
	140587309200560 [label="stage3.0.branches.1.1.conv2.weight
 (96, 96, 3, 3)" fillcolor=lightblue]
	140587309200560 -> 140588144712592
	140588144712592 [label=AccumulateGrad]
	140588144710800 -> 140588144694416
	140587309200800 [label="stage3.0.branches.1.1.bn2.weight
 (96)" fillcolor=lightblue]
	140587309200800 -> 140588144710800
	140588144710800 [label=AccumulateGrad]
	140588144711760 -> 140588144694416
	140587309200880 [label="stage3.0.branches.1.1.bn2.bias
 (96)" fillcolor=lightblue]
	140587309200880 -> 140588144711760
	140588144711760 [label=AccumulateGrad]
	140588144712272 -> 140588144696464
	140588144695248 -> 140588144686480
	140587309201280 [label="stage3.0.branches.1.2.conv1.weight
 (96, 96, 3, 3)" fillcolor=lightblue]
	140587309201280 -> 140588144695248
	140588144695248 [label=AccumulateGrad]
	140588144688080 -> 140588144685520
	140587309201520 [label="stage3.0.branches.1.2.bn1.weight
 (96)" fillcolor=lightblue]
	140587309201520 -> 140588144688080
	140588144688080 [label=AccumulateGrad]
	140588144693712 -> 140588144685520
	140587309201600 [label="stage3.0.branches.1.2.bn1.bias
 (96)" fillcolor=lightblue]
	140587309201600 -> 140588144693712
	140588144693712 [label=AccumulateGrad]
	140588144686032 -> 140588144669648
	140587309202000 [label="stage3.0.branches.1.2.conv2.weight
 (96, 96, 3, 3)" fillcolor=lightblue]
	140587309202000 -> 140588144686032
	140588144686032 [label=AccumulateGrad]
	140588144670672 -> 140588144654416
	140587309202240 [label="stage3.0.branches.1.2.bn2.weight
 (96)" fillcolor=lightblue]
	140587309202240 -> 140588144670672
	140588144670672 [label=AccumulateGrad]
	140588144669136 -> 140588144654416
	140587309202320 [label="stage3.0.branches.1.2.bn2.bias
 (96)" fillcolor=lightblue]
	140587309202320 -> 140588144669136
	140588144669136 [label=AccumulateGrad]
	140588144671632 -> 140588144655120
	140588144654672 -> 140588144653584
	140587309202720 [label="stage3.0.branches.1.3.conv1.weight
 (96, 96, 3, 3)" fillcolor=lightblue]
	140587309202720 -> 140588144654672
	140588144654672 [label=AccumulateGrad]
	140588144656016 -> 140588144654800
	140587309202960 [label="stage3.0.branches.1.3.bn1.weight
 (96)" fillcolor=lightblue]
	140587309202960 -> 140588144656016
	140588144656016 [label=AccumulateGrad]
	140588144653712 -> 140588144654800
	140587309203040 [label="stage3.0.branches.1.3.bn1.bias
 (96)" fillcolor=lightblue]
	140587309203040 -> 140588144653712
	140588144653712 [label=AccumulateGrad]
	140588144653008 -> 140588144648080
	140587309264976 [label="stage3.0.branches.1.3.conv2.weight
 (96, 96, 3, 3)" fillcolor=lightblue]
	140587309264976 -> 140588144653008
	140588144653008 [label=AccumulateGrad]
	140588144652496 -> 140588144648016
	140587309265216 [label="stage3.0.branches.1.3.bn2.weight
 (96)" fillcolor=lightblue]
	140587309265216 -> 140588144652496
	140588144652496 [label=AccumulateGrad]
	140588144652688 -> 140588144648016
	140587309265296 [label="stage3.0.branches.1.3.bn2.bias
 (96)" fillcolor=lightblue]
	140587309265296 -> 140588144652688
	140588144652688 [label=AccumulateGrad]
	140588144647440 -> 140588144646032
	140588144647120 -> 140588144644304
	140587309329296 [label="stage3.0.fuse_layers.0.1.0.weight
 (48, 96, 1, 1)" fillcolor=lightblue]
	140587309329296 -> 140588144647120
	140588144647120 [label=AccumulateGrad]
	140588144647824 -> 140588144645200
	140587309329536 [label="stage3.0.fuse_layers.0.1.1.weight
 (48)" fillcolor=lightblue]
	140587309329536 -> 140588144647824
	140588144647824 [label=AccumulateGrad]
	140588144646416 -> 140588144645200
	140587309329616 [label="stage3.0.fuse_layers.0.1.1.bias
 (48)" fillcolor=lightblue]
	140587309329616 -> 140588144646416
	140588144646416 [label=AccumulateGrad]
	140588144644816 -> 140588144644368
	140588144644816 [label=UpsampleBilinear2DBackward1]
	140588144653392 -> 140588144644816
	140588144653392 [label=NativeBatchNormBackward]
	140588144655376 -> 140588144653392
	140588144655376 [label=MkldnnConvolutionBackward]
	140588144671952 -> 140588144655376
	140588144671952 [label=ReluBackward1]
	140588144686160 -> 140588144671952
	140588144686160 [label=AddBackward0]
	140588144694288 -> 140588144686160
	140588144694288 [label=NativeBatchNormBackward]
	140588144713680 -> 140588144694288
	140588144713680 [label=MkldnnConvolutionBackward]
	140588144645328 -> 140588144713680
	140588144645328 [label=ReluBackward1]
	140588144646736 -> 140588144645328
	140588144646736 [label=NativeBatchNormBackward]
	140584538546256 -> 140588144646736
	140584538546256 [label=MkldnnConvolutionBackward]
	140588144694736 -> 140584538546256
	140588144694736 [label=ReluBackward1]
	140584538547536 -> 140588144694736
	140584538547536 [label=AddBackward0]
	140584538547216 -> 140584538547536
	140584538547216 [label=NativeBatchNormBackward]
	140584538546576 -> 140584538547216
	140584538546576 [label=MkldnnConvolutionBackward]
	140584538548624 -> 140584538546576
	140584538548624 [label=ReluBackward1]
	140584538548496 -> 140584538548624
	140584538548496 [label=NativeBatchNormBackward]
	140584538548944 -> 140584538548496
	140584538548944 [label=MkldnnConvolutionBackward]
	140584538547856 -> 140584538548944
	140584538547856 [label=ReluBackward1]
	140584538549264 -> 140584538547856
	140584538549264 [label=AddBackward0]
	140584538549456 -> 140584538549264
	140584538549456 [label=NativeBatchNormBackward]
	140584538549200 -> 140584538549456
	140584538549200 [label=MkldnnConvolutionBackward]
	140584538549840 -> 140584538549200
	140584538549840 [label=ReluBackward1]
	140584538549904 -> 140584538549840
	140584538549904 [label=NativeBatchNormBackward]
	140584538550160 -> 140584538549904
	140584538550160 [label=MkldnnConvolutionBackward]
	140584538549648 -> 140584538550160
	140584538549648 [label=ReluBackward1]
	140584538571088 -> 140584538549648
	140584538571088 [label=AddBackward0]
	140584538571280 -> 140584538571088
	140584538571280 [label=NativeBatchNormBackward]
	140584538571024 -> 140584538571280
	140584538571024 [label=MkldnnConvolutionBackward]
	140584538571664 -> 140584538571024
	140584538571664 [label=ReluBackward1]
	140584538571728 -> 140584538571664
	140584538571728 [label=NativeBatchNormBackward]
	140584538572048 -> 140584538571728
	140584538572048 [label=MkldnnConvolutionBackward]
	140584538571472 -> 140584538572048
	140584538571472 [label=ReluBackward1]
	140584538572368 -> 140584538571472
	140584538572368 [label=NativeBatchNormBackward]
	140584538572560 -> 140584538572368
	140584538572560 [label=MkldnnConvolutionBackward]
	140584538546896 -> 140584538572560
	140584538572752 -> 140584538572560
	140587048730288 [label="transition2.2.0.0.weight
 (192, 96, 3, 3)" fillcolor=lightblue]
	140587048730288 -> 140584538572752
	140584538572752 [label=AccumulateGrad]
	140584538572688 -> 140584538572368
	140587048730528 [label="transition2.2.0.1.weight
 (192)" fillcolor=lightblue]
	140587048730528 -> 140584538572688
	140584538572688 [label=AccumulateGrad]
	140584538572304 -> 140584538572368
	140587309072464 [label="transition2.2.0.1.bias
 (192)" fillcolor=lightblue]
	140587309072464 -> 140584538572304
	140584538572304 [label=AccumulateGrad]
	140584538572240 -> 140584538572048
	140587309265856 [label="stage3.0.branches.2.0.conv1.weight
 (192, 192, 3, 3)" fillcolor=lightblue]
	140587309265856 -> 140584538572240
	140584538572240 [label=AccumulateGrad]
	140584538572176 -> 140584538571728
	140587309266096 [label="stage3.0.branches.2.0.bn1.weight
 (192)" fillcolor=lightblue]
	140587309266096 -> 140584538572176
	140584538572176 [label=AccumulateGrad]
	140584538571920 -> 140584538571728
	140587309266176 [label="stage3.0.branches.2.0.bn1.bias
 (192)" fillcolor=lightblue]
	140587309266176 -> 140584538571920
	140584538571920 [label=AccumulateGrad]
	140584538571856 -> 140584538571024
	140587309266576 [label="stage3.0.branches.2.0.conv2.weight
 (192, 192, 3, 3)" fillcolor=lightblue]
	140587309266576 -> 140584538571856
	140584538571856 [label=AccumulateGrad]
	140584538571600 -> 140584538571280
	140587309266816 [label="stage3.0.branches.2.0.bn2.weight
 (192)" fillcolor=lightblue]
	140587309266816 -> 140584538571600
	140584538571600 [label=AccumulateGrad]
	140584538571536 -> 140584538571280
	140587309266896 [label="stage3.0.branches.2.0.bn2.bias
 (192)" fillcolor=lightblue]
	140587309266896 -> 140584538571536
	140584538571536 [label=AccumulateGrad]
	140584538571472 -> 140584538571088
	140584538570960 -> 140584538550160
	140587309267296 [label="stage3.0.branches.2.1.conv1.weight
 (192, 192, 3, 3)" fillcolor=lightblue]
	140587309267296 -> 140584538570960
	140584538570960 [label=AccumulateGrad]
	140584538550096 -> 140584538549904
	140587309267536 [label="stage3.0.branches.2.1.bn1.weight
 (192)" fillcolor=lightblue]
	140587309267536 -> 140584538550096
	140584538550096 [label=AccumulateGrad]
	140584538570896 -> 140584538549904
	140587309267616 [label="stage3.0.branches.2.1.bn1.bias
 (192)" fillcolor=lightblue]
	140587309267616 -> 140584538570896
	140584538570896 [label=AccumulateGrad]
	140584538550032 -> 140584538549200
	140587309268016 [label="stage3.0.branches.2.1.conv2.weight
 (192, 192, 3, 3)" fillcolor=lightblue]
	140587309268016 -> 140584538550032
	140584538550032 [label=AccumulateGrad]
	140584538549776 -> 140584538549456
	140587309268256 [label="stage3.0.branches.2.1.bn2.weight
 (192)" fillcolor=lightblue]
	140587309268256 -> 140584538549776
	140584538549776 [label=AccumulateGrad]
	140584538549712 -> 140584538549456
	140587309268336 [label="stage3.0.branches.2.1.bn2.bias
 (192)" fillcolor=lightblue]
	140587309268336 -> 140584538549712
	140584538549712 [label=AccumulateGrad]
	140584538549648 -> 140584538549264
	140584538549136 -> 140584538548944
	140587309268736 [label="stage3.0.branches.2.2.conv1.weight
 (192, 192, 3, 3)" fillcolor=lightblue]
	140587309268736 -> 140584538549136
	140584538549136 [label=AccumulateGrad]
	140584538549072 -> 140584538548496
	140587309326416 [label="stage3.0.branches.2.2.bn1.weight
 (192)" fillcolor=lightblue]
	140587309326416 -> 140584538549072
	140584538549072 [label=AccumulateGrad]
	140584538548816 -> 140584538548496
	140587309326496 [label="stage3.0.branches.2.2.bn1.bias
 (192)" fillcolor=lightblue]
	140587309326496 -> 140584538548816
	140584538548816 [label=AccumulateGrad]
	140584538548752 -> 140584538546576
	140587309326896 [label="stage3.0.branches.2.2.conv2.weight
 (192, 192, 3, 3)" fillcolor=lightblue]
	140587309326896 -> 140584538548752
	140584538548752 [label=AccumulateGrad]
	140584538548688 -> 140584538547216
	140587309327136 [label="stage3.0.branches.2.2.bn2.weight
 (192)" fillcolor=lightblue]
	140587309327136 -> 140584538548688
	140584538548688 [label=AccumulateGrad]
	140584538548368 -> 140584538547216
	140587309327216 [label="stage3.0.branches.2.2.bn2.bias
 (192)" fillcolor=lightblue]
	140587309327216 -> 140584538548368
	140584538548368 [label=AccumulateGrad]
	140584538547856 -> 140584538547536
	140584538546640 -> 140584538546256
	140587309327616 [label="stage3.0.branches.2.3.conv1.weight
 (192, 192, 3, 3)" fillcolor=lightblue]
	140587309327616 -> 140584538546640
	140584538546640 [label=AccumulateGrad]
	140584538546768 -> 140588144646736
	140587309327856 [label="stage3.0.branches.2.3.bn1.weight
 (192)" fillcolor=lightblue]
	140587309327856 -> 140584538546768
	140584538546768 [label=AccumulateGrad]
	140584538547408 -> 140588144646736
	140587309327936 [label="stage3.0.branches.2.3.bn1.bias
 (192)" fillcolor=lightblue]
	140587309327936 -> 140584538547408
	140584538547408 [label=AccumulateGrad]
	140588144646608 -> 140588144713680
	140587309328336 [label="stage3.0.branches.2.3.conv2.weight
 (192, 192, 3, 3)" fillcolor=lightblue]
	140587309328336 -> 140588144646608
	140588144646608 [label=AccumulateGrad]
	140588144712208 -> 140588144694288
	140587309328576 [label="stage3.0.branches.2.3.bn2.weight
 (192)" fillcolor=lightblue]
	140587309328576 -> 140588144712208
	140588144712208 [label=AccumulateGrad]
	140588144645712 -> 140588144694288
	140587309328656 [label="stage3.0.branches.2.3.bn2.bias
 (192)" fillcolor=lightblue]
	140587309328656 -> 140588144645712
	140588144645712 [label=AccumulateGrad]
	140588144694736 -> 140588144686160
	140588144712400 -> 140588144655376
	140587309330096 [label="stage3.0.fuse_layers.0.2.0.weight
 (48, 192, 1, 1)" fillcolor=lightblue]
	140587309330096 -> 140588144712400
	140588144712400 [label=AccumulateGrad]
	140588144654992 -> 140588144653392
	140587309330336 [label="stage3.0.fuse_layers.0.2.1.weight
 (48)" fillcolor=lightblue]
	140587309330336 -> 140588144654992
	140588144654992 [label=AccumulateGrad]
	140588144654096 -> 140588144653392
	140587595608144 [label="stage3.0.fuse_layers.0.2.1.bias
 (48)" fillcolor=lightblue]
	140587595608144 -> 140588144654096
	140588144654096 [label=AccumulateGrad]
	140588144644240 -> 140588144635664
	140587595667408 [label="stage3.1.branches.0.0.conv1.weight
 (48, 48, 3, 3)" fillcolor=lightblue]
	140587595667408 -> 140588144644240
	140588144644240 [label=AccumulateGrad]
	140588144635856 -> 140588144635408
	140587595667648 [label="stage3.1.branches.0.0.bn1.weight
 (48)" fillcolor=lightblue]
	140587595667648 -> 140588144635856
	140588144635856 [label=AccumulateGrad]
	140588144635600 -> 140588144635408
	140587595667728 [label="stage3.1.branches.0.0.bn1.bias
 (48)" fillcolor=lightblue]
	140587595667728 -> 140588144635600
	140588144635600 [label=AccumulateGrad]
	140588144635536 -> 140588144635024
	140587595668128 [label="stage3.1.branches.0.0.conv2.weight
 (48, 48, 3, 3)" fillcolor=lightblue]
	140587595668128 -> 140588144635536
	140588144635536 [label=AccumulateGrad]
	140588144635280 -> 140588144634832
	140587595668368 [label="stage3.1.branches.0.0.bn2.weight
 (48)" fillcolor=lightblue]
	140587595668368 -> 140588144635280
	140588144635280 [label=AccumulateGrad]
	140588144635216 -> 140588144634832
	140587595668448 [label="stage3.1.branches.0.0.bn2.bias
 (48)" fillcolor=lightblue]
	140587595668448 -> 140588144635216
	140588144635216 [label=AccumulateGrad]
	140588144635152 -> 140588144634704
	140588144634576 -> 140588144634256
	140587595668848 [label="stage3.1.branches.0.1.conv1.weight
 (48, 48, 3, 3)" fillcolor=lightblue]
	140587595668848 -> 140588144634576
	140588144634576 [label=AccumulateGrad]
	140588144634512 -> 140588144634000
	140587595669088 [label="stage3.1.branches.0.1.bn1.weight
 (48)" fillcolor=lightblue]
	140587595669088 -> 140588144634512
	140588144634512 [label=AccumulateGrad]
	140588144634448 -> 140588144634000
	140587595669168 [label="stage3.1.branches.0.1.bn1.bias
 (48)" fillcolor=lightblue]
	140587595669168 -> 140588144634448
	140588144634448 [label=AccumulateGrad]
	140588144634128 -> 140588144633616
	140587595735200 [label="stage3.1.branches.0.1.conv2.weight
 (48, 48, 3, 3)" fillcolor=lightblue]
	140587595735200 -> 140588144634128
	140588144634128 [label=AccumulateGrad]
	140588144633872 -> 140588144633424
	140587595735440 [label="stage3.1.branches.0.1.bn2.weight
 (48)" fillcolor=lightblue]
	140587595735440 -> 140588144633872
	140588144633872 [label=AccumulateGrad]
	140588144633808 -> 140588144633424
	140587595735520 [label="stage3.1.branches.0.1.bn2.bias
 (48)" fillcolor=lightblue]
	140587595735520 -> 140588144633808
	140588144633808 [label=AccumulateGrad]
	140588144633744 -> 140588144633296
	140588144633168 -> 140588144632848
	140587595735920 [label="stage3.1.branches.0.2.conv1.weight
 (48, 48, 3, 3)" fillcolor=lightblue]
	140587595735920 -> 140588144633168
	140588144633168 [label=AccumulateGrad]
	140588144633104 -> 140588144632592
	140587595736160 [label="stage3.1.branches.0.2.bn1.weight
 (48)" fillcolor=lightblue]
	140587595736160 -> 140588144633104
	140588144633104 [label=AccumulateGrad]
	140588144633040 -> 140588144632592
	140587595736240 [label="stage3.1.branches.0.2.bn1.bias
 (48)" fillcolor=lightblue]
	140587595736240 -> 140588144633040
	140588144633040 [label=AccumulateGrad]
	140588144632720 -> 140588144632208
	140587595736640 [label="stage3.1.branches.0.2.conv2.weight
 (48, 48, 3, 3)" fillcolor=lightblue]
	140587595736640 -> 140588144632720
	140588144632720 [label=AccumulateGrad]
	140588144632464 -> 140588144632016
	140587595736880 [label="stage3.1.branches.0.2.bn2.weight
 (48)" fillcolor=lightblue]
	140587595736880 -> 140588144632464
	140588144632464 [label=AccumulateGrad]
	140588144632400 -> 140588144632016
	140587595736960 [label="stage3.1.branches.0.2.bn2.bias
 (48)" fillcolor=lightblue]
	140587595736960 -> 140588144632400
	140588144632400 [label=AccumulateGrad]
	140588144632336 -> 140588144631952
	140588144619408 -> 140588144619088
	140587595737360 [label="stage3.1.branches.0.3.conv1.weight
 (48, 48, 3, 3)" fillcolor=lightblue]
	140587595737360 -> 140588144619408
	140588144619408 [label=AccumulateGrad]
	140588144619344 -> 140588144618832
	140587595737600 [label="stage3.1.branches.0.3.bn1.weight
 (48)" fillcolor=lightblue]
	140587595737600 -> 140588144619344
	140588144619344 [label=AccumulateGrad]
	140588144619280 -> 140588144618832
	140587595737680 [label="stage3.1.branches.0.3.bn1.bias
 (48)" fillcolor=lightblue]
	140587595737680 -> 140588144619280
	140588144619280 [label=AccumulateGrad]
	140588144618960 -> 140588144618448
	140587595738080 [label="stage3.1.branches.0.3.conv2.weight
 (48, 48, 3, 3)" fillcolor=lightblue]
	140587595738080 -> 140588144618960
	140588144618960 [label=AccumulateGrad]
	140588144618704 -> 140588144618256
	140587595738320 [label="stage3.1.branches.0.3.bn2.weight
 (48)" fillcolor=lightblue]
	140587595738320 -> 140588144618704
	140588144618704 [label=AccumulateGrad]
	140588144618640 -> 140588144618256
	140587595738400 [label="stage3.1.branches.0.3.bn2.bias
 (48)" fillcolor=lightblue]
	140587595738400 -> 140588144618640
	140588144618640 [label=AccumulateGrad]
	140588144618576 -> 140588144618064
	140588144618192 -> 140588144617680
	140588144618192 [label=UpsampleBilinear2DBackward1]
	140588144669968 -> 140588144618192
	140588144669968 [label=NativeBatchNormBackward]
	140588144618512 -> 140588144669968
	140588144618512 [label=MkldnnConvolutionBackward]
	140588144713232 -> 140588144618512
	140588144713232 [label=ReluBackward1]
	140588144619024 -> 140588144713232
	140588144619024 [label=AddBackward0]
	140588144631888 -> 140588144619024
	140588144631888 [label=NativeBatchNormBackward]
	140588144632272 -> 140588144631888
	140588144632272 [label=MkldnnConvolutionBackward]
	140588144633552 -> 140588144632272
	140588144633552 [label=ReluBackward1]
	140588144633232 -> 140588144633552
	140588144633232 [label=NativeBatchNormBackward]
	140588144635088 -> 140588144633232
	140588144635088 [label=MkldnnConvolutionBackward]
	140588144632976 -> 140588144635088
	140588144632976 [label=ReluBackward1]
	140588144635792 -> 140588144632976
	140588144635792 [label=AddBackward0]
	140587038865680 -> 140588144635792
	140587038865680 [label=NativeBatchNormBackward]
	140588144644432 -> 140587038865680
	140588144644432 [label=MkldnnConvolutionBackward]
	140588144647248 -> 140588144644432
	140588144647248 [label=ReluBackward1]
	140584538548560 -> 140588144647248
	140584538548560 [label=NativeBatchNormBackward]
	140584538548048 -> 140584538548560
	140584538548048 [label=MkldnnConvolutionBackward]
	140587038865808 -> 140584538548048
	140587038865808 [label=ReluBackward1]
	140584538549968 -> 140587038865808
	140584538549968 [label=AddBackward0]
	140584538549520 -> 140584538549968
	140584538549520 [label=NativeBatchNormBackward]
	140584538571408 -> 140584538549520
	140584538571408 [label=MkldnnConvolutionBackward]
	140584538571152 -> 140584538571408
	140584538571152 [label=ReluBackward1]
	140584538572112 -> 140584538571152
	140584538572112 [label=NativeBatchNormBackward]
	140584538573008 -> 140584538572112
	140584538573008 [label=MkldnnConvolutionBackward]
	140584538549008 -> 140584538573008
	140584538549008 [label=ReluBackward1]
	140584538572880 -> 140584538549008
	140584538572880 [label=AddBackward0]
	140584538573328 -> 140584538572880
	140584538573328 [label=NativeBatchNormBackward]
	140584538572944 -> 140584538573328
	140584538572944 [label=MkldnnConvolutionBackward]
	140584538573712 -> 140584538572944
	140584538573712 [label=ReluBackward1]
	140584538573776 -> 140584538573712
	140584538573776 [label=NativeBatchNormBackward]
	140584538574096 -> 140584538573776
	140584538574096 [label=MkldnnConvolutionBackward]
	140584538573520 -> 140584538574096
	140584538573520 [label=ReluBackward1]
	140584538574416 -> 140584538573520
	140584538574416 [label=AddBackward0]
	140584538574608 -> 140584538574416
	140584538574608 [label=AddBackward0]
	140584538574352 -> 140584538574608
	140584538574352 [label=NativeBatchNormBackward]
	140584538607824 -> 140584538574352
	140584538607824 [label=MkldnnConvolutionBackward]
	140588144644688 -> 140584538607824
	140584538607952 -> 140584538607824
	140587595608784 [label="stage3.0.fuse_layers.1.0.0.0.weight
 (96, 48, 3, 3)" fillcolor=lightblue]
	140587595608784 -> 140584538607952
	140584538607952 [label=AccumulateGrad]
	140584538607888 -> 140584538574352
	140587595609024 [label="stage3.0.fuse_layers.1.0.0.1.weight
 (96)" fillcolor=lightblue]
	140587595609024 -> 140584538607888
	140584538607888 [label=AccumulateGrad]
	140584538607696 -> 140584538574352
	140587595609104 [label="stage3.0.fuse_layers.1.0.0.1.bias
 (96)" fillcolor=lightblue]
	140587595609104 -> 140584538607696
	140584538607696 [label=AccumulateGrad]
	140588144646288 -> 140584538574608
	140584538574800 -> 140584538574416
	140584538574800 [label=UpsampleBilinear2DBackward1]
	140584538574544 -> 140584538574800
	140584538574544 [label=NativeBatchNormBackward]
	140584538608208 -> 140584538574544
	140584538608208 [label=MkldnnConvolutionBackward]
	140588144671952 -> 140584538608208
	140584538608144 -> 140584538608208
	140587595609744 [label="stage3.0.fuse_layers.1.2.0.weight
 (96, 192, 1, 1)" fillcolor=lightblue]
	140587595609744 -> 140584538608144
	140584538608144 [label=AccumulateGrad]
	140584538608272 -> 140584538574544
	140587595609984 [label="stage3.0.fuse_layers.1.2.1.weight
 (96)" fillcolor=lightblue]
	140587595609984 -> 140584538608272
	140584538608272 [label=AccumulateGrad]
	140584538608336 -> 140584538574544
	140587595610064 [label="stage3.0.fuse_layers.1.2.1.bias
 (96)" fillcolor=lightblue]
	140587595610064 -> 140584538608336
	140584538608336 [label=AccumulateGrad]
	140584538574288 -> 140584538574096
	140587595738960 [label="stage3.1.branches.1.0.conv1.weight
 (96, 96, 3, 3)" fillcolor=lightblue]
	140587595738960 -> 140584538574288
	140584538574288 [label=AccumulateGrad]
	140584538574224 -> 140584538573776
	140587595796640 [label="stage3.1.branches.1.0.bn1.weight
 (96)" fillcolor=lightblue]
	140587595796640 -> 140584538574224
	140584538574224 [label=AccumulateGrad]
	140584538573968 -> 140584538573776
	140587595796720 [label="stage3.1.branches.1.0.bn1.bias
 (96)" fillcolor=lightblue]
	140587595796720 -> 140584538573968
	140584538573968 [label=AccumulateGrad]
	140584538573904 -> 140584538572944
	140587595797120 [label="stage3.1.branches.1.0.conv2.weight
 (96, 96, 3, 3)" fillcolor=lightblue]
	140587595797120 -> 140584538573904
	140584538573904 [label=AccumulateGrad]
	140584538573648 -> 140584538573328
	140587595797360 [label="stage3.1.branches.1.0.bn2.weight
 (96)" fillcolor=lightblue]
	140587595797360 -> 140584538573648
	140584538573648 [label=AccumulateGrad]
	140584538573584 -> 140584538573328
	140587595797440 [label="stage3.1.branches.1.0.bn2.bias
 (96)" fillcolor=lightblue]
	140587595797440 -> 140584538573584
	140584538573584 [label=AccumulateGrad]
	140584538573520 -> 140584538572880
	140584538573072 -> 140584538573008
	140587595797840 [label="stage3.1.branches.1.1.conv1.weight
 (96, 96, 3, 3)" fillcolor=lightblue]
	140587595797840 -> 140584538573072
	140584538573072 [label=AccumulateGrad]
	140584538572816 -> 140584538572112
	140587595798080 [label="stage3.1.branches.1.1.bn1.weight
 (96)" fillcolor=lightblue]
	140587595798080 -> 140584538572816
	140584538572816 [label=AccumulateGrad]
	140584538571792 -> 140584538572112
	140587595798160 [label="stage3.1.branches.1.1.bn1.bias
 (96)" fillcolor=lightblue]
	140587595798160 -> 140584538571792
	140584538571792 [label=AccumulateGrad]
	140584538572496 -> 140584538571408
	140587595798560 [label="stage3.1.branches.1.1.conv2.weight
 (96, 96, 3, 3)" fillcolor=lightblue]
	140587595798560 -> 140584538572496
	140584538572496 [label=AccumulateGrad]
	140584538571216 -> 140584538549520
	140587595798800 [label="stage3.1.branches.1.1.bn2.weight
 (96)" fillcolor=lightblue]
	140587595798800 -> 140584538571216
	140584538571216 [label=AccumulateGrad]
	140584538571344 -> 140584538549520
	140587595798880 [label="stage3.1.branches.1.1.bn2.bias
 (96)" fillcolor=lightblue]
	140587595798880 -> 140584538571344
	140584538571344 [label=AccumulateGrad]
	140584538549008 -> 140584538549968
	140584538548880 -> 140584538548048
	140587595799280 [label="stage3.1.branches.1.2.conv1.weight
 (96, 96, 3, 3)" fillcolor=lightblue]
	140587595799280 -> 140584538548880
	140584538548880 [label=AccumulateGrad]
	140584538549328 -> 140584538548560
	140587595799520 [label="stage3.1.branches.1.2.bn1.weight
 (96)" fillcolor=lightblue]
	140587595799520 -> 140584538549328
	140584538549328 [label=AccumulateGrad]
	140584538548176 -> 140584538548560
	140587595799600 [label="stage3.1.branches.1.2.bn1.bias
 (96)" fillcolor=lightblue]
	140587595799600 -> 140584538548176
	140584538548176 [label=AccumulateGrad]
	140588144645840 -> 140588144644432
	140587595800000 [label="stage3.1.branches.1.2.conv2.weight
 (96, 96, 3, 3)" fillcolor=lightblue]
	140587595800000 -> 140588144645840
	140588144645840 [label=AccumulateGrad]
	140588144644944 -> 140587038865680
	140587595800240 [label="stage3.1.branches.1.2.bn2.weight
 (96)" fillcolor=lightblue]
	140587595800240 -> 140588144644944
	140588144644944 [label=AccumulateGrad]
	140588144644752 -> 140587038865680
	140587595800320 [label="stage3.1.branches.1.2.bn2.bias
 (96)" fillcolor=lightblue]
	140587595800320 -> 140588144644752
	140588144644752 [label=AccumulateGrad]
	140587038865808 -> 140588144635792
	140588144696784 -> 140588144635088
	140587051372784 [label="stage3.1.branches.1.3.conv1.weight
 (96, 96, 3, 3)" fillcolor=lightblue]
	140587051372784 -> 140588144696784
	140588144696784 [label=AccumulateGrad]
	140588144634640 -> 140588144633232
	140587051373024 [label="stage3.1.branches.1.3.bn1.weight
 (96)" fillcolor=lightblue]
	140587051373024 -> 140588144634640
	140588144634640 [label=AccumulateGrad]
	140588144634384 -> 140588144633232
	140587051373104 [label="stage3.1.branches.1.3.bn1.bias
 (96)" fillcolor=lightblue]
	140587051373104 -> 140588144634384
	140588144634384 [label=AccumulateGrad]
	140588144634192 -> 140588144632272
	140587051373504 [label="stage3.1.branches.1.3.conv2.weight
 (96, 96, 3, 3)" fillcolor=lightblue]
	140587051373504 -> 140588144634192
	140588144634192 [label=AccumulateGrad]
	140588144633680 -> 140588144631888
	140587051373744 [label="stage3.1.branches.1.3.bn2.weight
 (96)" fillcolor=lightblue]
	140587051373744 -> 140588144633680
	140588144633680 [label=AccumulateGrad]
	140588144633360 -> 140588144631888
	140587051373824 [label="stage3.1.branches.1.3.bn2.bias
 (96)" fillcolor=lightblue]
	140587051373824 -> 140588144633360
	140588144633360 [label=AccumulateGrad]
	140588144632976 -> 140588144619024
	140588144618896 -> 140588144618512
	140587051446016 [label="stage3.1.fuse_layers.0.1.0.weight
 (48, 96, 1, 1)" fillcolor=lightblue]
	140587051446016 -> 140588144618896
	140588144618896 [label=AccumulateGrad]
	140588144619472 -> 140588144669968
	140587051503696 [label="stage3.1.fuse_layers.0.1.1.weight
 (48)" fillcolor=lightblue]
	140587051503696 -> 140588144619472
	140588144619472 [label=AccumulateGrad]
	140588144617488 -> 140588144669968
	140587051503776 [label="stage3.1.fuse_layers.0.1.1.bias
 (48)" fillcolor=lightblue]
	140587051503776 -> 140588144617488
	140588144617488 [label=AccumulateGrad]
	140588144618000 -> 140588144617552
	140588144618000 [label=UpsampleBilinear2DBackward1]
	140588144654224 -> 140588144618000
	140588144654224 [label=NativeBatchNormBackward]
	140588144644176 -> 140588144654224
	140588144644176 [label=MkldnnConvolutionBackward]
	140588144632144 -> 140588144644176
	140588144632144 [label=ReluBackward1]
	140588144686992 -> 140588144632144
	140588144686992 [label=AddBackward0]
	140588144634768 -> 140588144686992
	140588144634768 [label=NativeBatchNormBackward]
	140584538548432 -> 140588144634768
	140584538548432 [label=MkldnnConvolutionBackward]
	140584538546832 -> 140584538548432
	140584538546832 [label=ReluBackward1]
	140584538572624 -> 140584538546832
	140584538572624 [label=NativeBatchNormBackward]
	140584538573456 -> 140584538572624
	140584538573456 [label=MkldnnConvolutionBackward]
	140588144634064 -> 140584538573456
	140588144634064 [label=ReluBackward1]
	140584538574160 -> 140588144634064
	140584538574160 [label=AddBackward0]
	140584538573840 -> 140584538574160
	140584538573840 [label=NativeBatchNormBackward]
	140584538607760 -> 140584538573840
	140584538607760 [label=MkldnnConvolutionBackward]
	140584538608656 -> 140584538607760
	140584538608656 [label=ReluBackward1]
	140584538608592 -> 140584538608656
	140584538608592 [label=NativeBatchNormBackward]
	140584538608848 -> 140584538608592
	140584538608848 [label=MkldnnConvolutionBackward]
	140584538573200 -> 140584538608848
	140584538573200 [label=ReluBackward1]
	140584538609168 -> 140584538573200
	140584538609168 [label=AddBackward0]
	140584538609360 -> 140584538609168
	140584538609360 [label=NativeBatchNormBackward]
	140584538609104 -> 140584538609360
	140584538609104 [label=MkldnnConvolutionBackward]
	140584538609744 -> 140584538609104
	140584538609744 [label=ReluBackward1]
	140584538609808 -> 140584538609744
	140584538609808 [label=NativeBatchNormBackward]
	140584538610128 -> 140584538609808
	140584538610128 [label=MkldnnConvolutionBackward]
	140584538609552 -> 140584538610128
	140584538609552 [label=ReluBackward1]
	140584538610448 -> 140584538609552
	140584538610448 [label=AddBackward0]
	140584538610640 -> 140584538610448
	140584538610640 [label=NativeBatchNormBackward]
	140584538610384 -> 140584538610640
	140584538610384 [label=MkldnnConvolutionBackward]
	140584538611024 -> 140584538610384
	140584538611024 [label=ReluBackward1]
	140584538611088 -> 140584538611024
	140584538611088 [label=NativeBatchNormBackward]
	140584538611408 -> 140584538611088
	140584538611408 [label=MkldnnConvolutionBackward]
	140584538610832 -> 140584538611408
	140584538610832 [label=ReluBackward1]
	140584538636432 -> 140584538610832
	140584538636432 [label=AddBackward0]
	140584538636560 -> 140584538636432
	140584538636560 [label=AddBackward0]
	140584538636688 -> 140584538636560
	140584538636688 [label=NativeBatchNormBackward]
	140584538636496 -> 140584538636688
	140584538636496 [label=MkldnnConvolutionBackward]
	140584538637072 -> 140584538636496
	140584538637072 [label=ReluBackward1]
	140584538637136 -> 140584538637072
	140584538637136 [label=NativeBatchNormBackward]
	140584538637456 -> 140584538637136
	140584538637456 [label=MkldnnConvolutionBackward]
	140588144644688 -> 140584538637456
	140584538637648 -> 140584538637456
	140587595610704 [label="stage3.0.fuse_layers.2.0.0.0.weight
 (48, 48, 3, 3)" fillcolor=lightblue]
	140587595610704 -> 140584538637648
	140584538637648 [label=AccumulateGrad]
	140584538637584 -> 140584538637136
	140587595610944 [label="stage3.0.fuse_layers.2.0.0.1.weight
 (48)" fillcolor=lightblue]
	140587595610944 -> 140584538637584
	140584538637584 [label=AccumulateGrad]
	140584538637328 -> 140584538637136
	140587595611024 [label="stage3.0.fuse_layers.2.0.0.1.bias
 (48)" fillcolor=lightblue]
	140587595611024 -> 140584538637328
	140584538637328 [label=AccumulateGrad]
	140584538637264 -> 140584538636496
	140587595611584 [label="stage3.0.fuse_layers.2.0.1.0.weight
 (192, 48, 3, 3)" fillcolor=lightblue]
	140587595611584 -> 140584538637264
	140584538637264 [label=AccumulateGrad]
	140584538637008 -> 140584538636688
	140587595611824 [label="stage3.0.fuse_layers.2.0.1.1.weight
 (192)" fillcolor=lightblue]
	140587595611824 -> 140584538637008
	140584538637008 [label=AccumulateGrad]
	140584538636944 -> 140584538636688
	140587595611904 [label="stage3.0.fuse_layers.2.0.1.1.bias
 (192)" fillcolor=lightblue]
	140587595611904 -> 140584538636944
	140584538636944 [label=AccumulateGrad]
	140584538636880 -> 140584538636560
	140584538636880 [label=NativeBatchNormBackward]
	140588144653264 -> 140584538636880
	140588144653264 [label=MkldnnConvolutionBackward]
	140588144646288 -> 140588144653264
	140584538637904 -> 140588144653264
	140587595665888 [label="stage3.0.fuse_layers.2.1.0.0.weight
 (192, 96, 3, 3)" fillcolor=lightblue]
	140587595665888 -> 140584538637904
	140584538637904 [label=AccumulateGrad]
	140584538638032 -> 140584538636880
	140587595666128 [label="stage3.0.fuse_layers.2.1.0.1.weight
 (192)" fillcolor=lightblue]
	140587595666128 -> 140584538638032
	140584538638032 [label=AccumulateGrad]
	140584538637392 -> 140584538636880
	140587595666208 [label="stage3.0.fuse_layers.2.1.0.1.bias
 (192)" fillcolor=lightblue]
	140587595666208 -> 140584538637392
	140584538637392 [label=AccumulateGrad]
	140588144671952 -> 140584538636432
	140584538611600 -> 140584538611408
	140587051374384 [label="stage3.1.branches.2.0.conv1.weight
 (192, 192, 3, 3)" fillcolor=lightblue]
	140587051374384 -> 140584538611600
	140584538611600 [label=AccumulateGrad]
	140584538611536 -> 140584538611088
	140587051374624 [label="stage3.1.branches.2.0.bn1.weight
 (192)" fillcolor=lightblue]
	140587051374624 -> 140584538611536
	140584538611536 [label=AccumulateGrad]
	140584538611280 -> 140584538611088
	140587051374704 [label="stage3.1.branches.2.0.bn1.bias
 (192)" fillcolor=lightblue]
	140587051374704 -> 140584538611280
	140584538611280 [label=AccumulateGrad]
	140584538611216 -> 140584538610384
	140587051375104 [label="stage3.1.branches.2.0.conv2.weight
 (192, 192, 3, 3)" fillcolor=lightblue]
	140587051375104 -> 140584538611216
	140584538611216 [label=AccumulateGrad]
	140584538610960 -> 140584538610640
	140587051375344 [label="stage3.1.branches.2.0.bn2.weight
 (192)" fillcolor=lightblue]
	140587051375344 -> 140584538610960
	140584538610960 [label=AccumulateGrad]
	140584538610896 -> 140584538610640
	140587051375424 [label="stage3.1.branches.2.0.bn2.bias
 (192)" fillcolor=lightblue]
	140587051375424 -> 140584538610896
	140584538610896 [label=AccumulateGrad]
	140584538610832 -> 140584538610448
	140584538610320 -> 140584538610128
	140587051375824 [label="stage3.1.branches.2.1.conv1.weight
 (192, 192, 3, 3)" fillcolor=lightblue]
	140587051375824 -> 140584538610320
	140584538610320 [label=AccumulateGrad]
	140584538610256 -> 140584538609808
	140587051376064 [label="stage3.1.branches.2.1.bn1.weight
 (192)" fillcolor=lightblue]
	140587051376064 -> 140584538610256
	140584538610256 [label=AccumulateGrad]
	140584538610000 -> 140584538609808
	140587051376144 [label="stage3.1.branches.2.1.bn1.bias
 (192)" fillcolor=lightblue]
	140587051376144 -> 140584538610000
	140584538610000 [label=AccumulateGrad]
	140584538609936 -> 140584538609104
	140587051376544 [label="stage3.1.branches.2.1.conv2.weight
 (192, 192, 3, 3)" fillcolor=lightblue]
	140587051376544 -> 140584538609936
	140584538609936 [label=AccumulateGrad]
	140584538609680 -> 140584538609360
	140587051442416 [label="stage3.1.branches.2.1.bn2.weight
 (192)" fillcolor=lightblue]
	140587051442416 -> 140584538609680
	140584538609680 [label=AccumulateGrad]
	140584538609616 -> 140584538609360
	140587051442496 [label="stage3.1.branches.2.1.bn2.bias
 (192)" fillcolor=lightblue]
	140587051442496 -> 140584538609616
	140584538609616 [label=AccumulateGrad]
	140584538609552 -> 140584538609168
	140584538609040 -> 140584538608848
	140587051442896 [label="stage3.1.branches.2.2.conv1.weight
 (192, 192, 3, 3)" fillcolor=lightblue]
	140587051442896 -> 140584538609040
	140584538609040 [label=AccumulateGrad]
	140584538608976 -> 140584538608592
	140587051443136 [label="stage3.1.branches.2.2.bn1.weight
 (192)" fillcolor=lightblue]
	140587051443136 -> 140584538608976
	140584538608976 [label=AccumulateGrad]
	140584538608720 -> 140584538608592
	140587051443216 [label="stage3.1.branches.2.2.bn1.bias
 (192)" fillcolor=lightblue]
	140587051443216 -> 140584538608720
	140584538608720 [label=AccumulateGrad]
	140584538608016 -> 140584538607760
	140587051443616 [label="stage3.1.branches.2.2.conv2.weight
 (192, 192, 3, 3)" fillcolor=lightblue]
	140587051443616 -> 140584538608016
	140584538608016 [label=AccumulateGrad]
	140584538608528 -> 140584538573840
	140587051443856 [label="stage3.1.branches.2.2.bn2.weight
 (192)" fillcolor=lightblue]
	140587051443856 -> 140584538608528
	140584538608528 [label=AccumulateGrad]
	140584538608080 -> 140584538573840
	140587051443936 [label="stage3.1.branches.2.2.bn2.bias
 (192)" fillcolor=lightblue]
	140587051443936 -> 140584538608080
	140584538608080 [label=AccumulateGrad]
	140584538573200 -> 140584538574160
	140584538573264 -> 140584538573456
	140587051444336 [label="stage3.1.branches.2.3.conv1.weight
 (192, 192, 3, 3)" fillcolor=lightblue]
	140587051444336 -> 140584538573264
	140584538573264 [label=AccumulateGrad]
	140584538573392 -> 140584538572624
	140587051444576 [label="stage3.1.branches.2.3.bn1.weight
 (192)" fillcolor=lightblue]
	140587051444576 -> 140584538573392
	140584538573392 [label=AccumulateGrad]
	140584538572432 -> 140584538572624
	140587051444656 [label="stage3.1.branches.2.3.bn1.bias
 (192)" fillcolor=lightblue]
	140587051444656 -> 140584538572432
	140584538572432 [label=AccumulateGrad]
	140584538570832 -> 140584538548432
	140587051445056 [label="stage3.1.branches.2.3.conv2.weight
 (192, 192, 3, 3)" fillcolor=lightblue]
	140587051445056 -> 140584538570832
	140584538570832 [label=AccumulateGrad]
	140584538547728 -> 140588144634768
	140587051445296 [label="stage3.1.branches.2.3.bn2.weight
 (192)" fillcolor=lightblue]
	140587051445296 -> 140584538547728
	140584538547728 [label=AccumulateGrad]
	140584538550224 -> 140588144634768
	140587051445376 [label="stage3.1.branches.2.3.bn2.bias
 (192)" fillcolor=lightblue]
	140587051445376 -> 140584538550224
	140584538550224 [label=AccumulateGrad]
	140588144634064 -> 140588144686992
	140588144635472 -> 140588144644176
	140587051504256 [label="stage3.1.fuse_layers.0.2.0.weight
 (48, 192, 1, 1)" fillcolor=lightblue]
	140587051504256 -> 140588144635472
	140588144635472 [label=AccumulateGrad]
	140588144644624 -> 140588144654224
	140587051504496 [label="stage3.1.fuse_layers.0.2.1.weight
 (48)" fillcolor=lightblue]
	140587051504496 -> 140588144644624
	140588144644624 [label=AccumulateGrad]
	140588144618384 -> 140588144654224
	140587051504576 [label="stage3.1.fuse_layers.0.2.1.bias
 (48)" fillcolor=lightblue]
	140587051504576 -> 140588144618384
	140588144618384 [label=AccumulateGrad]
	140588144617424 -> 140588144617104
	140587051551552 [label="stage3.2.branches.0.0.conv1.weight
 (48, 48, 3, 3)" fillcolor=lightblue]
	140587051551552 -> 140588144617424
	140588144617424 [label=AccumulateGrad]
	140588144617360 -> 140588144616848
	140587051551792 [label="stage3.2.branches.0.0.bn1.weight
 (48)" fillcolor=lightblue]
	140587051551792 -> 140588144617360
	140588144617360 [label=AccumulateGrad]
	140588144617296 -> 140588144616848
	140587051551872 [label="stage3.2.branches.0.0.bn1.bias
 (48)" fillcolor=lightblue]
	140587051551872 -> 140588144617296
	140588144617296 [label=AccumulateGrad]
	140588144616976 -> 140588144616464
	140587051552272 [label="stage3.2.branches.0.0.conv2.weight
 (48, 48, 3, 3)" fillcolor=lightblue]
	140587051552272 -> 140588144616976
	140588144616976 [label=AccumulateGrad]
	140588144616720 -> 140588144616272
	140587051552512 [label="stage3.2.branches.0.0.bn2.weight
 (48)" fillcolor=lightblue]
	140587051552512 -> 140588144616720
	140588144616720 [label=AccumulateGrad]
	140588144616656 -> 140588144616272
	140587051552592 [label="stage3.2.branches.0.0.bn2.bias
 (48)" fillcolor=lightblue]
	140587051552592 -> 140588144616656
	140588144616656 [label=AccumulateGrad]
	140588144616592 -> 140588144616144
	140588144616016 -> 140588144615696
	140587728384320 [label="stage3.2.branches.0.1.conv1.weight
 (48, 48, 3, 3)" fillcolor=lightblue]
	140587728384320 -> 140588144616016
	140588144616016 [label=AccumulateGrad]
	140588144615952 -> 140588144615504
	140587728384560 [label="stage3.2.branches.0.1.bn1.weight
 (48)" fillcolor=lightblue]
	140587728384560 -> 140588144615952
	140588144615952 [label=AccumulateGrad]
	140588144615888 -> 140588144615504
	140587728384640 [label="stage3.2.branches.0.1.bn1.bias
 (48)" fillcolor=lightblue]
	140587728384640 -> 140588144615888
	140588144615888 [label=AccumulateGrad]
	140588144611280 -> 140588144610896
	140587728385040 [label="stage3.2.branches.0.1.conv2.weight
 (48, 48, 3, 3)" fillcolor=lightblue]
	140587728385040 -> 140588144611280
	140588144611280 [label=AccumulateGrad]
	140588144611152 -> 140588144610704
	140587728385280 [label="stage3.2.branches.0.1.bn2.weight
 (48)" fillcolor=lightblue]
	140587728385280 -> 140588144611152
	140588144611152 [label=AccumulateGrad]
	140588144611088 -> 140588144610704
	140587728385360 [label="stage3.2.branches.0.1.bn2.bias
 (48)" fillcolor=lightblue]
	140587728385360 -> 140588144611088
	140588144611088 [label=AccumulateGrad]
	140588144611024 -> 140588144610576
	140588144610448 -> 140588144610128
	140587728385760 [label="stage3.2.branches.0.2.conv1.weight
 (48, 48, 3, 3)" fillcolor=lightblue]
	140587728385760 -> 140588144610448
	140588144610448 [label=AccumulateGrad]
	140588144610384 -> 140588144609872
	140587728386000 [label="stage3.2.branches.0.2.bn1.weight
 (48)" fillcolor=lightblue]
	140587728386000 -> 140588144610384
	140588144610384 [label=AccumulateGrad]
	140588144610320 -> 140588144609872
	140587728386080 [label="stage3.2.branches.0.2.bn1.bias
 (48)" fillcolor=lightblue]
	140587728386080 -> 140588144610320
	140588144610320 [label=AccumulateGrad]
	140588144610000 -> 140588144609488
	140587728386480 [label="stage3.2.branches.0.2.conv2.weight
 (48, 48, 3, 3)" fillcolor=lightblue]
	140587728386480 -> 140588144610000
	140588144610000 [label=AccumulateGrad]
	140588144609744 -> 140588144609296
	140587728386720 [label="stage3.2.branches.0.2.bn2.weight
 (48)" fillcolor=lightblue]
	140587728386720 -> 140588144609744
	140588144609744 [label=AccumulateGrad]
	140588144609680 -> 140588144609296
	140587728386800 [label="stage3.2.branches.0.2.bn2.bias
 (48)" fillcolor=lightblue]
	140587728386800 -> 140588144609680
	140588144609680 [label=AccumulateGrad]
	140588144609616 -> 140588144609168
	140588144609040 -> 140588144608720
	140587728387200 [label="stage3.2.branches.0.3.conv1.weight
 (48, 48, 3, 3)" fillcolor=lightblue]
	140587728387200 -> 140588144609040
	140588144609040 [label=AccumulateGrad]
	140588144608976 -> 140588144608464
	140587728387440 [label="stage3.2.branches.0.3.bn1.weight
 (48)" fillcolor=lightblue]
	140587728387440 -> 140588144608976
	140588144608976 [label=AccumulateGrad]
	140588144608912 -> 140588144608464
	140587728387520 [label="stage3.2.branches.0.3.bn1.bias
 (48)" fillcolor=lightblue]
	140587728387520 -> 140588144608912
	140588144608912 [label=AccumulateGrad]
	140588144608592 -> 140588144608080
	140587728387920 [label="stage3.2.branches.0.3.conv2.weight
 (48, 48, 3, 3)" fillcolor=lightblue]
	140587728387920 -> 140588144608592
	140588144608592 [label=AccumulateGrad]
	140588144608336 -> 140588144607888
	140587728445600 [label="stage3.2.branches.0.3.bn2.weight
 (48)" fillcolor=lightblue]
	140587728445600 -> 140588144608336
	140588144608336 [label=AccumulateGrad]
	140588144608272 -> 140588144607888
	140587728445680 [label="stage3.2.branches.0.3.bn2.bias
 (48)" fillcolor=lightblue]
	140587728445680 -> 140588144608272
	140588144608272 [label=AccumulateGrad]
	140588144608208 -> 140588144607696
	140588144607824 -> 140588144607376
	140588144607824 [label=UpsampleBilinear2DBackward1]
	140584538549392 -> 140588144607824
	140584538549392 [label=NativeBatchNormBackward]
	140588144608144 -> 140584538549392
	140588144608144 [label=MkldnnConvolutionBackward]
	140588144609552 -> 140588144608144
	140588144609552 [label=ReluBackward1]
	140588144608528 -> 140588144609552
	140588144608528 [label=AddBackward0]
	140588144610832 -> 140588144608528
	140588144610832 [label=NativeBatchNormBackward]
	140588144609232 -> 140588144610832
	140588144609232 [label=MkldnnConvolutionBackward]
	140588144615824 -> 140588144609232
	140588144615824 [label=ReluBackward1]
	140588144617040 -> 140588144615824
	140588144617040 [label=NativeBatchNormBackward]
	140588144616208 -> 140588144617040
	140588144616208 [label=MkldnnConvolutionBackward]
	140588144610960 -> 140588144616208
	140588144610960 [label=ReluBackward1]
	140588144617936 -> 140588144610960
	140588144617936 [label=AddBackward0]
	140588144616912 -> 140588144617936
	140588144616912 [label=NativeBatchNormBackward]
	140588144632656 -> 140588144616912
	140588144632656 [label=MkldnnConvolutionBackward]
	140584538574736 -> 140588144632656
	140584538574736 [label=ReluBackward1]
	140584538609232 -> 140584538574736
	140584538609232 [label=NativeBatchNormBackward]
	140584538608784 -> 140584538609232
	140584538608784 [label=MkldnnConvolutionBackward]
	140588144632784 -> 140584538608784
	140588144632784 [label=ReluBackward1]
	140584538609872 -> 140588144632784
	140584538609872 [label=AddBackward0]
	140584538611664 -> 140584538609872
	140584538611664 [label=NativeBatchNormBackward]
	140584538609488 -> 140584538611664
	140584538609488 [label=MkldnnConvolutionBackward]
	140584538611344 -> 140584538609488
	140584538611344 [label=ReluBackward1]
	140584538637968 -> 140584538611344
	140584538637968 [label=NativeBatchNormBackward]
	140584538636624 -> 140584538637968
	140584538636624 [label=MkldnnConvolutionBackward]
	140584538610768 -> 140584538636624
	140584538610768 [label=ReluBackward1]
	140584538637712 -> 140584538610768
	140584538637712 [label=AddBackward0]
	140584538638224 -> 140584538637712
	140584538638224 [label=NativeBatchNormBackward]
	140584538637776 -> 140584538638224
	140584538637776 [label=MkldnnConvolutionBackward]
	140584538638608 -> 140584538637776
	140584538638608 [label=ReluBackward1]
	140584538638672 -> 140584538638608
	140584538638672 [label=NativeBatchNormBackward]
	140584538638992 -> 140584538638672
	140584538638992 [label=MkldnnConvolutionBackward]
	140584538638416 -> 140584538638992
	140584538638416 [label=ReluBackward1]
	140584538639312 -> 140584538638416
	140584538639312 [label=AddBackward0]
	140584538639504 -> 140584538639312
	140584538639504 [label=AddBackward0]
	140584538639248 -> 140584538639504
	140584538639248 [label=NativeBatchNormBackward]
	140584538639824 -> 140584538639248
	140584538639824 [label=MkldnnConvolutionBackward]
	140588144617872 -> 140584538639824
	140584538640016 -> 140584538639824
	140587051505216 [label="stage3.1.fuse_layers.1.0.0.0.weight
 (96, 48, 3, 3)" fillcolor=lightblue]
	140587051505216 -> 140584538640016
	140584538640016 [label=AccumulateGrad]
	140584538639952 -> 140584538639248
	140587051505456 [label="stage3.1.fuse_layers.1.0.0.1.weight
 (96)" fillcolor=lightblue]
	140587051505456 -> 140584538639952
	140584538639952 [label=AccumulateGrad]
	140584538639440 -> 140584538639248
	140587051505536 [label="stage3.1.fuse_layers.1.0.0.1.bias
 (96)" fillcolor=lightblue]
	140587051505536 -> 140584538639440
	140584538639440 [label=AccumulateGrad]
	140588144713232 -> 140584538639504
	140584538639696 -> 140584538639312
	140584538639696 [label=UpsampleBilinear2DBackward1]
	140588144608848 -> 140584538639696
	140588144608848 [label=NativeBatchNormBackward]
	140584538639760 -> 140588144608848
	140584538639760 [label=MkldnnConvolutionBackward]
	140588144632144 -> 140584538639760
	140584538640208 -> 140584538639760
	140587051506176 [label="stage3.1.fuse_layers.1.2.0.weight
 (96, 192, 1, 1)" fillcolor=lightblue]
	140587051506176 -> 140584538640208
	140584538640208 [label=AccumulateGrad]
	140584538640080 -> 140588144608848
	140587051506416 [label="stage3.1.fuse_layers.1.2.1.weight
 (96)" fillcolor=lightblue]
	140587051506416 -> 140584538640080
	140584538640080 [label=AccumulateGrad]
	140584538640336 -> 140588144608848
	140587051506496 [label="stage3.1.fuse_layers.1.2.1.bias
 (96)" fillcolor=lightblue]
	140587051506496 -> 140584538640336
	140584538640336 [label=AccumulateGrad]
	140584538639184 -> 140584538638992
	140587728446240 [label="stage3.2.branches.1.0.conv1.weight
 (96, 96, 3, 3)" fillcolor=lightblue]
	140587728446240 -> 140584538639184
	140584538639184 [label=AccumulateGrad]
	140584538639120 -> 140584538638672
	140587728446480 [label="stage3.2.branches.1.0.bn1.weight
 (96)" fillcolor=lightblue]
	140587728446480 -> 140584538639120
	140584538639120 [label=AccumulateGrad]
	140584538638864 -> 140584538638672
	140587728446560 [label="stage3.2.branches.1.0.bn1.bias
 (96)" fillcolor=lightblue]
	140587728446560 -> 140584538638864
	140584538638864 [label=AccumulateGrad]
	140584538638800 -> 140584538637776
	140587728446960 [label="stage3.2.branches.1.0.conv2.weight
 (96, 96, 3, 3)" fillcolor=lightblue]
	140587728446960 -> 140584538638800
	140584538638800 [label=AccumulateGrad]
	140584538638544 -> 140584538638224
	140587728447200 [label="stage3.2.branches.1.0.bn2.weight
 (96)" fillcolor=lightblue]
	140587728447200 -> 140584538638544
	140584538638544 [label=AccumulateGrad]
	140584538638480 -> 140584538638224
	140587728447280 [label="stage3.2.branches.1.0.bn2.bias
 (96)" fillcolor=lightblue]
	140587728447280 -> 140584538638480
	140584538638480 [label=AccumulateGrad]
	140584538638416 -> 140584538637712
	140584538638096 -> 140584538636624
	140587728447680 [label="stage3.2.branches.1.1.conv1.weight
 (96, 96, 3, 3)" fillcolor=lightblue]
	140587728447680 -> 140584538638096
	140584538638096 [label=AccumulateGrad]
	140584538637840 -> 140584538637968
	140587728447920 [label="stage3.2.branches.1.1.bn1.weight
 (96)" fillcolor=lightblue]
	140587728447920 -> 140584538637840
	140584538637840 [label=AccumulateGrad]
	140584538636752 -> 140584538637968
	140587728448000 [label="stage3.2.branches.1.1.bn1.bias
 (96)" fillcolor=lightblue]
	140587728448000 -> 140584538636752
	140584538636752 [label=AccumulateGrad]
	140584538611472 -> 140584538609488
	140587728448400 [label="stage3.2.branches.1.1.conv2.weight
 (96, 96, 3, 3)" fillcolor=lightblue]
	140587728448400 -> 140584538611472
	140584538611472 [label=AccumulateGrad]
	140584538610704 -> 140584538611664
	140587728448640 [label="stage3.2.branches.1.1.bn2.weight
 (96)" fillcolor=lightblue]
	140587728448640 -> 140584538610704
	140584538610704 [label=AccumulateGrad]
	140584538611152 -> 140584538611664
	140587728448720 [label="stage3.2.branches.1.1.bn2.bias
 (96)" fillcolor=lightblue]
	140587728448720 -> 140584538611152
	140584538611152 [label=AccumulateGrad]
	140584538610768 -> 140584538609872
	140584538608912 -> 140584538608784
	140587728449120 [label="stage3.2.branches.1.2.conv1.weight
 (96, 96, 3, 3)" fillcolor=lightblue]
	140587728449120 -> 140584538608912
	140584538608912 [label=AccumulateGrad]
	140584538609296 -> 140584538609232
	140587728449360 [label="stage3.2.branches.1.2.bn1.weight
 (96)" fillcolor=lightblue]
	140587728449360 -> 140584538609296
	140584538609296 [label=AccumulateGrad]
	140584538608400 -> 140584538609232
	140587728449440 [label="stage3.2.branches.1.2.bn1.bias
 (96)" fillcolor=lightblue]
	140587728449440 -> 140584538608400
	140584538608400 [label=AccumulateGrad]
	140584538573136 -> 140588144632656
	140587728507280 [label="stage3.2.branches.1.2.conv2.weight
 (96, 96, 3, 3)" fillcolor=lightblue]
	140587728507280 -> 140584538573136
	140584538573136 [label=AccumulateGrad]
	140584538574480 -> 140588144616912
	140587728507520 [label="stage3.2.branches.1.2.bn2.weight
 (96)" fillcolor=lightblue]
	140587728507520 -> 140584538574480
	140584538574480 [label=AccumulateGrad]
	140584538571984 -> 140588144616912
	140587728507600 [label="stage3.2.branches.1.2.bn2.bias
 (96)" fillcolor=lightblue]
	140587728507600 -> 140584538571984
	140584538571984 [label=AccumulateGrad]
	140588144632784 -> 140588144617936
	140588144619216 -> 140588144616208
	140587728508000 [label="stage3.2.branches.1.3.conv1.weight
 (96, 96, 3, 3)" fillcolor=lightblue]
	140587728508000 -> 140588144619216
	140588144619216 [label=AccumulateGrad]
	140588144617808 -> 140588144617040
	140587728508240 [label="stage3.2.branches.1.3.bn1.weight
 (96)" fillcolor=lightblue]
	140587728508240 -> 140588144617808
	140588144617808 [label=AccumulateGrad]
	140588144616400 -> 140588144617040
	140587728508320 [label="stage3.2.branches.1.3.bn1.bias
 (96)" fillcolor=lightblue]
	140587728508320 -> 140588144616400
	140588144616400 [label=AccumulateGrad]
	140588144615568 -> 140588144609232
	140587728508720 [label="stage3.2.branches.1.3.conv2.weight
 (96, 96, 3, 3)" fillcolor=lightblue]
	140587728508720 -> 140588144615568
	140588144615568 [label=AccumulateGrad]
	140588144610640 -> 140588144610832
	140587728508960 [label="stage3.2.branches.1.3.bn2.weight
 (96)" fillcolor=lightblue]
	140587728508960 -> 140588144610640
	140588144610640 [label=AccumulateGrad]
	140588144610256 -> 140588144610832
	140587728509040 [label="stage3.2.branches.1.3.bn2.bias
 (96)" fillcolor=lightblue]
	140587728509040 -> 140588144610256
	140588144610256 [label=AccumulateGrad]
	140588144610960 -> 140588144608528
	140588144609104 -> 140588144608144
	140587728622288 [label="stage3.2.fuse_layers.0.1.0.weight
 (48, 96, 1, 1)" fillcolor=lightblue]
	140587728622288 -> 140588144609104
	140588144609104 [label=AccumulateGrad]
	140588144610064 -> 140584538549392
	140587728622528 [label="stage3.2.fuse_layers.0.1.1.weight
 (48)" fillcolor=lightblue]
	140587728622528 -> 140588144610064
	140588144610064 [label=AccumulateGrad]
	140588144608656 -> 140584538549392
	140587728622608 [label="stage3.2.fuse_layers.0.1.1.bias
 (48)" fillcolor=lightblue]
	140587728622608 -> 140588144608656
	140588144608656 [label=AccumulateGrad]
	140588144607632 -> 140588144598928
	140588144607632 [label=UpsampleBilinear2DBackward1]
	140588144634960 -> 140588144607632
	140588144634960 [label=NativeBatchNormBackward]
	140588144607312 -> 140588144634960
	140588144607312 [label=MkldnnConvolutionBackward]
	140584538574672 -> 140588144607312
	140584538574672 [label=ReluBackward1]
	140588144615632 -> 140584538574672
	140588144615632 [label=AddBackward0]
	140588144617232 -> 140588144615632
	140588144617232 [label=NativeBatchNormBackward]
	140584538608464 -> 140588144617232
	140584538608464 [label=MkldnnConvolutionBackward]
	140584538610512 -> 140584538608464
	140584538610512 [label=ReluBackward1]
	140584538637520 -> 140584538610512
	140584538637520 [label=NativeBatchNormBackward]
	140584538638160 -> 140584538637520
	140584538638160 [label=MkldnnConvolutionBackward]
	140588144618128 -> 140584538638160
	140588144618128 [label=ReluBackward1]
	140584538638928 -> 140588144618128
	140584538638928 [label=AddBackward0]
	140584538639056 -> 140584538638928
	140584538639056 [label=NativeBatchNormBackward]
	140584538638288 -> 140584538639056
	140584538638288 [label=MkldnnConvolutionBackward]
	140584538697936 -> 140584538638288
	140584538697936 [label=ReluBackward1]
	140584538698000 -> 140584538697936
	140584538698000 [label=NativeBatchNormBackward]
	140584538698320 -> 140584538698000
	140584538698320 [label=MkldnnConvolutionBackward]
	140584538639568 -> 140584538698320
	140584538639568 [label=ReluBackward1]
	140584538698640 -> 140584538639568
	140584538698640 [label=AddBackward0]
	140584538698832 -> 140584538698640
	140584538698832 [label=NativeBatchNormBackward]
	140584538698576 -> 140584538698832
	140584538698576 [label=MkldnnConvolutionBackward]
	140584538699216 -> 140584538698576
	140584538699216 [label=ReluBackward1]
	140584538699280 -> 140584538699216
	140584538699280 [label=NativeBatchNormBackward]
	140584538699600 -> 140584538699280
	140584538699600 [label=MkldnnConvolutionBackward]
	140584538699024 -> 140584538699600
	140584538699024 [label=ReluBackward1]
	140584538699920 -> 140584538699024
	140584538699920 [label=AddBackward0]
	140584538700112 -> 140584538699920
	140584538700112 [label=NativeBatchNormBackward]
	140584538699856 -> 140584538700112
	140584538699856 [label=MkldnnConvolutionBackward]
	140584538700496 -> 140584538699856
	140584538700496 [label=ReluBackward1]
	140584538700560 -> 140584538700496
	140584538700560 [label=NativeBatchNormBackward]
	140584538700880 -> 140584538700560
	140584538700880 [label=MkldnnConvolutionBackward]
	140584538700304 -> 140584538700880
	140584538700304 [label=ReluBackward1]
	140584538701200 -> 140584538700304
	140584538701200 [label=AddBackward0]
	140584538701392 -> 140584538701200
	140584538701392 [label=AddBackward0]
	140584538701520 -> 140584538701392
	140584538701520 [label=NativeBatchNormBackward]
	140584538701328 -> 140584538701520
	140584538701328 [label=MkldnnConvolutionBackward]
	140584538701968 -> 140584538701328
	140584538701968 [label=ReluBackward1]
	140584538702032 -> 140584538701968
	140584538702032 [label=NativeBatchNormBackward]
	140584538702352 -> 140584538702032
	140584538702352 [label=MkldnnConvolutionBackward]
	140588144617872 -> 140584538702352
	140584538702544 -> 140584538702352
	140587051507136 [label="stage3.1.fuse_layers.2.0.0.0.weight
 (48, 48, 3, 3)" fillcolor=lightblue]
	140587051507136 -> 140584538702544
	140584538702544 [label=AccumulateGrad]
	140584538702480 -> 140584538702032
	140587051507376 [label="stage3.1.fuse_layers.2.0.0.1.weight
 (48)" fillcolor=lightblue]
	140587051507376 -> 140584538702480
	140584538702480 [label=AccumulateGrad]
	140584538702224 -> 140584538702032
	140587051507456 [label="stage3.1.fuse_layers.2.0.0.1.bias
 (48)" fillcolor=lightblue]
	140587051507456 -> 140584538702224
	140584538702224 [label=AccumulateGrad]
	140584538702160 -> 140584538701328
	140587051549072 [label="stage3.1.fuse_layers.2.0.1.0.weight
 (192, 48, 3, 3)" fillcolor=lightblue]
	140587051549072 -> 140584538702160
	140584538702160 [label=AccumulateGrad]
	140584538701776 -> 140584538701520
	140587051549312 [label="stage3.1.fuse_layers.2.0.1.1.weight
 (192)" fillcolor=lightblue]
	140587051549312 -> 140584538701776
	140584538701776 [label=AccumulateGrad]
	140584538701136 -> 140584538701520
	140587051549392 [label="stage3.1.fuse_layers.2.0.1.1.bias
 (192)" fillcolor=lightblue]
	140587051549392 -> 140584538701136
	140584538701136 [label=AccumulateGrad]
	140584538701712 -> 140584538701392
	140584538701712 [label=NativeBatchNormBackward]
	140588144609936 -> 140584538701712
	140588144609936 [label=MkldnnConvolutionBackward]
	140588144713232 -> 140588144609936
	140584538702800 -> 140588144609936
	140587051550032 [label="stage3.1.fuse_layers.2.1.0.0.weight
 (192, 96, 3, 3)" fillcolor=lightblue]
	140587051550032 -> 140584538702800
	140584538702800 [label=AccumulateGrad]
	140584538702928 -> 140584538701712
	140587051550272 [label="stage3.1.fuse_layers.2.1.0.1.weight
 (192)" fillcolor=lightblue]
	140587051550272 -> 140584538702928
	140584538702928 [label=AccumulateGrad]
	140584538702288 -> 140584538701712
	140587051550352 [label="stage3.1.fuse_layers.2.1.0.1.bias
 (192)" fillcolor=lightblue]
	140587051550352 -> 140584538702288
	140584538702288 [label=AccumulateGrad]
	140588144632144 -> 140584538701200
	140584538701072 -> 140584538700880
	140587728509600 [label="stage3.2.branches.2.0.conv1.weight
 (192, 192, 3, 3)" fillcolor=lightblue]
	140587728509600 -> 140584538701072
	140584538701072 [label=AccumulateGrad]
	140584538701008 -> 140584538700560
	140587728509840 [label="stage3.2.branches.2.0.bn1.weight
 (192)" fillcolor=lightblue]
	140587728509840 -> 140584538701008
	140584538701008 [label=AccumulateGrad]
	140584538700752 -> 140584538700560
	140587728509920 [label="stage3.2.branches.2.0.bn1.bias
 (192)" fillcolor=lightblue]
	140587728509920 -> 140584538700752
	140584538700752 [label=AccumulateGrad]
	140584538700688 -> 140584538699856
	140587728510320 [label="stage3.2.branches.2.0.conv2.weight
 (192, 192, 3, 3)" fillcolor=lightblue]
	140587728510320 -> 140584538700688
	140584538700688 [label=AccumulateGrad]
	140584538700432 -> 140584538700112
	140587728510560 [label="stage3.2.branches.2.0.bn2.weight
 (192)" fillcolor=lightblue]
	140587728510560 -> 140584538700432
	140584538700432 [label=AccumulateGrad]
	140584538700368 -> 140584538700112
	140587728510640 [label="stage3.2.branches.2.0.bn2.bias
 (192)" fillcolor=lightblue]
	140587728510640 -> 140584538700368
	140584538700368 [label=AccumulateGrad]
	140584538700304 -> 140584538699920
	140584538699792 -> 140584538699600
	140587728564384 [label="stage3.2.branches.2.1.conv1.weight
 (192, 192, 3, 3)" fillcolor=lightblue]
	140587728564384 -> 140584538699792
	140584538699792 [label=AccumulateGrad]
	140584538699728 -> 140584538699280
	140587728564624 [label="stage3.2.branches.2.1.bn1.weight
 (192)" fillcolor=lightblue]
	140587728564624 -> 140584538699728
	140584538699728 [label=AccumulateGrad]
	140584538699472 -> 140584538699280
	140587728564704 [label="stage3.2.branches.2.1.bn1.bias
 (192)" fillcolor=lightblue]
	140587728564704 -> 140584538699472
	140584538699472 [label=AccumulateGrad]
	140584538699408 -> 140584538698576
	140587728565104 [label="stage3.2.branches.2.1.conv2.weight
 (192, 192, 3, 3)" fillcolor=lightblue]
	140587728565104 -> 140584538699408
	140584538699408 [label=AccumulateGrad]
	140584538699152 -> 140584538698832
	140587728565344 [label="stage3.2.branches.2.1.bn2.weight
 (192)" fillcolor=lightblue]
	140587728565344 -> 140584538699152
	140584538699152 [label=AccumulateGrad]
	140584538699088 -> 140584538698832
	140587728565424 [label="stage3.2.branches.2.1.bn2.bias
 (192)" fillcolor=lightblue]
	140587728565424 -> 140584538699088
	140584538699088 [label=AccumulateGrad]
	140584538699024 -> 140584538698640
	140584538698512 -> 140584538698320
	140587728565824 [label="stage3.2.branches.2.2.conv1.weight
 (192, 192, 3, 3)" fillcolor=lightblue]
	140587728565824 -> 140584538698512
	140584538698512 [label=AccumulateGrad]
	140584538698448 -> 140584538698000
	140587728566064 [label="stage3.2.branches.2.2.bn1.weight
 (192)" fillcolor=lightblue]
	140587728566064 -> 140584538698448
	140584538698448 [label=AccumulateGrad]
	140584538698192 -> 140584538698000
	140587728566144 [label="stage3.2.branches.2.2.bn1.bias
 (192)" fillcolor=lightblue]
	140587728566144 -> 140584538698192
	140584538698192 [label=AccumulateGrad]
	140584538698128 -> 140584538638288
	140587728566544 [label="stage3.2.branches.2.2.conv2.weight
 (192, 192, 3, 3)" fillcolor=lightblue]
	140587728566544 -> 140584538698128
	140584538698128 [label=AccumulateGrad]
	140584538640272 -> 140584538639056
	140587728566784 [label="stage3.2.branches.2.2.bn2.weight
 (192)" fillcolor=lightblue]
	140587728566784 -> 140584538640272
	140584538640272 [label=AccumulateGrad]
	140584538697872 -> 140584538639056
	140587728566864 [label="stage3.2.branches.2.2.bn2.bias
 (192)" fillcolor=lightblue]
	140587728566864 -> 140584538697872
	140584538697872 [label=AccumulateGrad]
	140584538639568 -> 140584538638928
	140584538638736 -> 140584538638160
	140587728567264 [label="stage3.2.branches.2.3.conv1.weight
 (192, 192, 3, 3)" fillcolor=lightblue]
	140587728567264 -> 140584538638736
	140584538638736 [label=AccumulateGrad]
	140584538638352 -> 140584538637520
	140587728567504 [label="stage3.2.branches.2.3.bn1.weight
 (192)" fillcolor=lightblue]
	140587728567504 -> 140584538638352
	140584538638352 [label=AccumulateGrad]
	140584538636368 -> 140584538637520
	140587728567584 [label="stage3.2.branches.2.3.bn1.bias
 (192)" fillcolor=lightblue]
	140587728567584 -> 140584538636368
	140584538636368 [label=AccumulateGrad]
	140584538610192 -> 140584538608464
	140587728567984 [label="stage3.2.branches.2.3.conv2.weight
 (192, 192, 3, 3)" fillcolor=lightblue]
	140587728567984 -> 140584538610192
	140584538610192 [label=AccumulateGrad]
	140584538610064 -> 140588144617232
	140587728568224 [label="stage3.2.branches.2.3.bn2.weight
 (192)" fillcolor=lightblue]
	140587728568224 -> 140584538610064
	140584538610064 [label=AccumulateGrad]
	140584538610576 -> 140588144617232
	140587728621648 [label="stage3.2.branches.2.3.bn2.bias
 (192)" fillcolor=lightblue]
	140587728621648 -> 140584538610576
	140584538610576 [label=AccumulateGrad]
	140588144618128 -> 140588144615632
	140584538574032 -> 140588144607312
	140587728623088 [label="stage3.2.fuse_layers.0.2.0.weight
 (48, 192, 1, 1)" fillcolor=lightblue]
	140587728623088 -> 140584538574032
	140584538574032 [label=AccumulateGrad]
	140588144610512 -> 140588144634960
	140587728623328 [label="stage3.2.fuse_layers.0.2.1.weight
 (48)" fillcolor=lightblue]
	140587728623328 -> 140588144610512
	140588144610512 [label=AccumulateGrad]
	140588144609424 -> 140588144634960
	140587728623408 [label="stage3.2.fuse_layers.0.2.1.bias
 (48)" fillcolor=lightblue]
	140587728623408 -> 140588144609424
	140588144609424 [label=AccumulateGrad]
	140588144598800 -> 140588144598480
	140587618320048 [label="stage3.3.branches.0.0.conv1.weight
 (48, 48, 3, 3)" fillcolor=lightblue]
	140587618320048 -> 140588144598800
	140588144598800 [label=AccumulateGrad]
	140588144598736 -> 140588144598224
	140587618320288 [label="stage3.3.branches.0.0.bn1.weight
 (48)" fillcolor=lightblue]
	140587618320288 -> 140588144598736
	140588144598736 [label=AccumulateGrad]
	140588144598672 -> 140588144598224
	140587618390096 [label="stage3.3.branches.0.0.bn1.bias
 (48)" fillcolor=lightblue]
	140587618390096 -> 140588144598672
	140588144598672 [label=AccumulateGrad]
	140588144598352 -> 140588144597840
	140587618390496 [label="stage3.3.branches.0.0.conv2.weight
 (48, 48, 3, 3)" fillcolor=lightblue]
	140587618390496 -> 140588144598352
	140588144598352 [label=AccumulateGrad]
	140588144598096 -> 140588144597648
	140587618390736 [label="stage3.3.branches.0.0.bn2.weight
 (48)" fillcolor=lightblue]
	140587618390736 -> 140588144598096
	140588144598096 [label=AccumulateGrad]
	140588144598032 -> 140588144597648
	140587618390816 [label="stage3.3.branches.0.0.bn2.bias
 (48)" fillcolor=lightblue]
	140587618390816 -> 140588144598032
	140588144598032 [label=AccumulateGrad]
	140588144597968 -> 140588144597520
	140588144597392 -> 140588144597072
	140587618391216 [label="stage3.3.branches.0.1.conv1.weight
 (48, 48, 3, 3)" fillcolor=lightblue]
	140587618391216 -> 140588144597392
	140588144597392 [label=AccumulateGrad]
	140588144597328 -> 140588144596816
	140587618391456 [label="stage3.3.branches.0.1.bn1.weight
 (48)" fillcolor=lightblue]
	140587618391456 -> 140588144597328
	140588144597328 [label=AccumulateGrad]
	140588144597264 -> 140588144596816
	140587618391536 [label="stage3.3.branches.0.1.bn1.bias
 (48)" fillcolor=lightblue]
	140587618391536 -> 140588144597264
	140588144597264 [label=AccumulateGrad]
	140588144596944 -> 140588144596112
	140587618391936 [label="stage3.3.branches.0.1.conv2.weight
 (48, 48, 3, 3)" fillcolor=lightblue]
	140587618391936 -> 140588144596944
	140588144596944 [label=AccumulateGrad]
	140588144596688 -> 140588144596368
	140587618392176 [label="stage3.3.branches.0.1.bn2.weight
 (48)" fillcolor=lightblue]
	140587618392176 -> 140588144596688
	140588144596688 [label=AccumulateGrad]
	140588144596624 -> 140588144596368
	140587618392256 [label="stage3.3.branches.0.1.bn2.bias
 (48)" fillcolor=lightblue]
	140587618392256 -> 140588144596624
	140588144596624 [label=AccumulateGrad]
	140588144596560 -> 140588144596176
	140587850540560 -> 140588144595728
	140587618392656 [label="stage3.3.branches.0.2.conv1.weight
 (48, 48, 3, 3)" fillcolor=lightblue]
	140587618392656 -> 140587850540560
	140587850540560 [label=AccumulateGrad]
	140588144595984 -> 140588144595472
	140587618392896 [label="stage3.3.branches.0.2.bn1.weight
 (48)" fillcolor=lightblue]
	140587618392896 -> 140588144595984
	140588144595984 [label=AccumulateGrad]
	140588144595920 -> 140588144595472
	140587618392976 [label="stage3.3.branches.0.2.bn1.bias
 (48)" fillcolor=lightblue]
	140587618392976 -> 140588144595920
	140588144595920 [label=AccumulateGrad]
	140588144595600 -> 140588144595152
	140587618393376 [label="stage3.3.branches.0.2.conv2.weight
 (48, 48, 3, 3)" fillcolor=lightblue]
	140587618393376 -> 140588144595600
	140588144595600 [label=AccumulateGrad]
	140588144595344 -> 140588144582352
	140587618393616 [label="stage3.3.branches.0.2.bn2.weight
 (48)" fillcolor=lightblue]
	140587618393616 -> 140588144595344
	140588144595344 [label=AccumulateGrad]
	140588144595280 -> 140588144582352
	140587618393696 [label="stage3.3.branches.0.2.bn2.bias
 (48)" fillcolor=lightblue]
	140587618393696 -> 140588144595280
	140588144595280 [label=AccumulateGrad]
	140588144595024 -> 140588144582416
	140588144582288 -> 140588144581968
	140587618447440 [label="stage3.3.branches.0.3.conv1.weight
 (48, 48, 3, 3)" fillcolor=lightblue]
	140587618447440 -> 140588144582288
	140588144582288 [label=AccumulateGrad]
	140588144582224 -> 140588144581712
	140587618447680 [label="stage3.3.branches.0.3.bn1.weight
 (48)" fillcolor=lightblue]
	140587618447680 -> 140588144582224
	140588144582224 [label=AccumulateGrad]
	140588144582160 -> 140588144581712
	140587618447760 [label="stage3.3.branches.0.3.bn1.bias
 (48)" fillcolor=lightblue]
	140587618447760 -> 140588144582160
	140588144582160 [label=AccumulateGrad]
	140588144581840 -> 140588144581328
	140587618448160 [label="stage3.3.branches.0.3.conv2.weight
 (48, 48, 3, 3)" fillcolor=lightblue]
	140587618448160 -> 140588144581840
	140588144581840 [label=AccumulateGrad]
	140588144581584 -> 140588144581136
	140587618448400 [label="stage3.3.branches.0.3.bn2.weight
 (48)" fillcolor=lightblue]
	140587618448400 -> 140588144581584
	140588144581584 [label=AccumulateGrad]
	140588144581520 -> 140588144581136
	140587618448480 [label="stage3.3.branches.0.3.bn2.bias
 (48)" fillcolor=lightblue]
	140587618448480 -> 140588144581520
	140588144581520 [label=AccumulateGrad]
	140588144581456 -> 140588144580944
	140588144581072 -> 140588144580560
	140588144581072 [label=UpsampleBilinear2DBackward1]
	140584538609424 -> 140588144581072
	140584538609424 [label=NativeBatchNormBackward]
	140588144607760 -> 140584538609424
	140588144607760 [label=MkldnnConvolutionBackward]
	140588144581776 -> 140588144607760
	140588144581776 [label=ReluBackward1]
	140588144581904 -> 140588144581776
	140588144581904 [label=AddBackward0]
	140588144595536 -> 140588144581904
	140588144595536 [label=NativeBatchNormBackward]
	140588144595088 -> 140588144595536
	140588144595088 [label=MkldnnConvolutionBackward]
	140588144596432 -> 140588144595088
	140588144596432 [label=ReluBackward1]
	140588144596304 -> 140588144596432
	140588144596304 [label=NativeBatchNormBackward]
	140588144597904 -> 140588144596304
	140588144597904 [label=MkldnnConvolutionBackward]
	140588144595856 -> 140588144597904
	140588144595856 [label=ReluBackward1]
	140588144598608 -> 140588144595856
	140588144598608 [label=AddBackward0]
	140588144598416 -> 140588144598608
	140588144598416 [label=NativeBatchNormBackward]
	140588144616528 -> 140588144598416
	140588144616528 [label=MkldnnConvolutionBackward]
	140584538639632 -> 140588144616528
	140584538639632 [label=ReluBackward1]
	140584538698704 -> 140584538639632
	140584538698704 [label=NativeBatchNormBackward]
	140584538698256 -> 140584538698704
	140584538698256 [label=MkldnnConvolutionBackward]
	140588144617616 -> 140584538698256
	140588144617616 [label=ReluBackward1]
	140584538699344 -> 140588144617616
	140584538699344 [label=AddBackward0]
	140584538701264 -> 140584538699344
	140584538701264 [label=NativeBatchNormBackward]
	140584538698960 -> 140584538701264
	140584538698960 [label=MkldnnConvolutionBackward]
	140584538700816 -> 140584538698960
	140584538700816 [label=ReluBackward1]
	140584538701456 -> 140584538700816
	140584538701456 [label=NativeBatchNormBackward]
	140584538702096 -> 140584538701456
	140584538702096 [label=MkldnnConvolutionBackward]
	140584538700240 -> 140584538702096
	140584538700240 [label=ReluBackward1]
	140584538702608 -> 140584538700240
	140584538702608 [label=AddBackward0]
	140584538703120 -> 140584538702608
	140584538703120 [label=NativeBatchNormBackward]
	140584538702672 -> 140584538703120
	140584538702672 [label=MkldnnConvolutionBackward]
	140584538703504 -> 140584538702672
	140584538703504 [label=ReluBackward1]
	140584538703568 -> 140584538703504
	140584538703568 [label=NativeBatchNormBackward]
	140584538703888 -> 140584538703568
	140584538703888 [label=MkldnnConvolutionBackward]
	140584538703312 -> 140584538703888
	140584538703312 [label=ReluBackward1]
	140584538704208 -> 140584538703312
	140584538704208 [label=AddBackward0]
	140584538704400 -> 140584538704208
	140584538704400 [label=AddBackward0]
	140584538704144 -> 140584538704400
	140584538704144 [label=NativeBatchNormBackward]
	140584538704720 -> 140584538704144
	140584538704720 [label=MkldnnConvolutionBackward]
	140588144607504 -> 140584538704720
	140584538704912 -> 140584538704720
	140587728624048 [label="stage3.2.fuse_layers.1.0.0.0.weight
 (96, 48, 3, 3)" fillcolor=lightblue]
	140587728624048 -> 140584538704912
	140584538704912 [label=AccumulateGrad]
	140584538704848 -> 140584538704144
	140587728624288 [label="stage3.2.fuse_layers.1.0.0.1.weight
 (96)" fillcolor=lightblue]
	140587728624288 -> 140584538704848
	140584538704848 [label=AccumulateGrad]
	140584538704336 -> 140584538704144
	140587728624368 [label="stage3.2.fuse_layers.1.0.0.1.bias
 (96)" fillcolor=lightblue]
	140587728624368 -> 140584538704336
	140584538704336 [label=AccumulateGrad]
	140588144609552 -> 140584538704400
	140584538704592 -> 140584538704208
	140584538704592 [label=UpsampleBilinear2DBackward1]
	140588144607568 -> 140584538704592
	140588144607568 [label=NativeBatchNormBackward]
	140584538704656 -> 140588144607568
	140584538704656 [label=MkldnnConvolutionBackward]
	140584538574672 -> 140584538704656
	140584538705104 -> 140584538704656
	140587728625008 [label="stage3.2.fuse_layers.1.2.0.weight
 (96, 192, 1, 1)" fillcolor=lightblue]
	140587728625008 -> 140584538705104
	140584538705104 [label=AccumulateGrad]
	140584538705232 -> 140588144607568
	140587728625248 [label="stage3.2.fuse_layers.1.2.1.weight
 (96)" fillcolor=lightblue]
	140587728625248 -> 140584538705232
	140584538705232 [label=AccumulateGrad]
	140584538705296 -> 140588144607568
	140587728625328 [label="stage3.2.fuse_layers.1.2.1.bias
 (96)" fillcolor=lightblue]
	140587728625328 -> 140584538705296
	140584538705296 [label=AccumulateGrad]
	140584538704080 -> 140584538703888
	140587618449040 [label="stage3.3.branches.1.0.conv1.weight
 (96, 96, 3, 3)" fillcolor=lightblue]
	140587618449040 -> 140584538704080
	140584538704080 [label=AccumulateGrad]
	140584538704016 -> 140584538703568
	140587618449280 [label="stage3.3.branches.1.0.bn1.weight
 (96)" fillcolor=lightblue]
	140587618449280 -> 140584538704016
	140584538704016 [label=AccumulateGrad]
	140584538703760 -> 140584538703568
	140587618449360 [label="stage3.3.branches.1.0.bn1.bias
 (96)" fillcolor=lightblue]
	140587618449360 -> 140584538703760
	140584538703760 [label=AccumulateGrad]
	140584538703696 -> 140584538702672
	140587618449760 [label="stage3.3.branches.1.0.conv2.weight
 (96, 96, 3, 3)" fillcolor=lightblue]
	140587618449760 -> 140584538703696
	140584538703696 [label=AccumulateGrad]
	140584538703440 -> 140584538703120
	140587618450000 [label="stage3.3.branches.1.0.bn2.weight
 (96)" fillcolor=lightblue]
	140587618450000 -> 140584538703440
	140584538703440 [label=AccumulateGrad]
	140584538703376 -> 140584538703120
	140587618450080 [label="stage3.3.branches.1.0.bn2.bias
 (96)" fillcolor=lightblue]
	140587618450080 -> 140584538703376
	140584538703376 [label=AccumulateGrad]
	140584538703312 -> 140584538702608
	140584538702992 -> 140584538702096
	140587618450480 [label="stage3.3.branches.1.1.conv1.weight
 (96, 96, 3, 3)" fillcolor=lightblue]
	140587618450480 -> 140584538702992
	140584538702992 [label=AccumulateGrad]
	140584538702736 -> 140584538701456
	140587618450720 [label="stage3.3.branches.1.1.bn1.weight
 (96)" fillcolor=lightblue]
	140587618450720 -> 140584538702736
	140584538702736 [label=AccumulateGrad]
	140584538702864 -> 140584538701456
	140587618450800 [label="stage3.3.branches.1.1.bn1.bias
 (96)" fillcolor=lightblue]
	140587618450800 -> 140584538702864
	140584538702864 [label=AccumulateGrad]
	140584538700944 -> 140584538698960
	140587618451200 [label="stage3.3.branches.1.1.conv2.weight
 (96, 96, 3, 3)" fillcolor=lightblue]
	140587618451200 -> 140584538700944
	140584538700944 [label=AccumulateGrad]
	140584538700176 -> 140584538701264
	140587618504784 [label="stage3.3.branches.1.1.bn2.weight
 (96)" fillcolor=lightblue]
	140587618504784 -> 140584538700176
	140584538700176 [label=AccumulateGrad]
	140584538700624 -> 140584538701264
	140587618504864 [label="stage3.3.branches.1.1.bn2.bias
 (96)" fillcolor=lightblue]
	140587618504864 -> 140584538700624
	140584538700624 [label=AccumulateGrad]
	140584538700240 -> 140584538699344
	140584538698384 -> 140584538698256
	140587618505264 [label="stage3.3.branches.1.2.conv1.weight
 (96, 96, 3, 3)" fillcolor=lightblue]
	140587618505264 -> 140584538698384
	140584538698384 [label=AccumulateGrad]
	140584538698768 -> 140584538698704
	140587618505504 [label="stage3.3.branches.1.2.bn1.weight
 (96)" fillcolor=lightblue]
	140587618505504 -> 140584538698768
	140584538698768 [label=AccumulateGrad]
	140584538698064 -> 140584538698704
	140587618505584 [label="stage3.3.branches.1.2.bn1.bias
 (96)" fillcolor=lightblue]
	140587618505584 -> 140584538698064
	140584538698064 [label=AccumulateGrad]
	140584538637200 -> 140588144616528
	140587618505984 [label="stage3.3.branches.1.2.conv2.weight
 (96, 96, 3, 3)" fillcolor=lightblue]
	140587618505984 -> 140584538637200
	140584538637200 [label=AccumulateGrad]
	140584538639888 -> 140588144598416
	140587618506224 [label="stage3.3.branches.1.2.bn2.weight
 (96)" fillcolor=lightblue]
	140587618506224 -> 140584538639888
	140584538639888 [label=AccumulateGrad]
	140584538636816 -> 140588144598416
	140587618506304 [label="stage3.3.branches.1.2.bn2.bias
 (96)" fillcolor=lightblue]
	140587618506304 -> 140584538636816
	140584538636816 [label=AccumulateGrad]
	140588144617616 -> 140588144598608
	140588144598288 -> 140588144597904
	140587618506704 [label="stage3.3.branches.1.3.conv1.weight
 (96, 96, 3, 3)" fillcolor=lightblue]
	140587618506704 -> 140588144598288
	140588144598288 [label=AccumulateGrad]
	140588144597456 -> 140588144596304
	140587618506944 [label="stage3.3.branches.1.3.bn1.weight
 (96)" fillcolor=lightblue]
	140587618506944 -> 140588144597456
	140588144597456 [label=AccumulateGrad]
	140588144597200 -> 140588144596304
	140587618507024 [label="stage3.3.branches.1.3.bn1.bias
 (96)" fillcolor=lightblue]
	140587618507024 -> 140588144597200
	140588144597200 [label=AccumulateGrad]
	140588144597008 -> 140588144595088
	140587618507424 [label="stage3.3.branches.1.3.conv2.weight
 (96, 96, 3, 3)" fillcolor=lightblue]
	140587618507424 -> 140588144597008
	140588144597008 [label=AccumulateGrad]
	140588144596496 -> 140588144595536
	140587618507664 [label="stage3.3.branches.1.3.bn2.weight
 (96)" fillcolor=lightblue]
	140587618507664 -> 140588144596496
	140588144596496 [label=AccumulateGrad]
	140588144596240 -> 140588144595536
	140587618507744 [label="stage3.3.branches.1.3.bn2.bias
 (96)" fillcolor=lightblue]
	140587618507744 -> 140588144596240
	140588144596240 [label=AccumulateGrad]
	140588144595856 -> 140588144581904
	140588144582480 -> 140588144607760
	140587317290560 [label="stage3.3.fuse_layers.0.1.0.weight
 (48, 96, 1, 1)" fillcolor=lightblue]
	140587317290560 -> 140588144582480
	140588144582480 [label=AccumulateGrad]
	140588144580368 -> 140584538609424
	140587317290800 [label="stage3.3.fuse_layers.0.1.1.weight
 (48)" fillcolor=lightblue]
	140587317290800 -> 140588144580368
	140588144580368 [label=AccumulateGrad]
	140588144581392 -> 140584538609424
	140587317290880 [label="stage3.3.fuse_layers.0.1.1.bias
 (48)" fillcolor=lightblue]
	140587317290880 -> 140588144581392
	140588144581392 [label=AccumulateGrad]
	140588144580880 -> 140588144580432
	140588144580880 [label=UpsampleBilinear2DBackward1]
	140588144608016 -> 140588144580880
	140588144608016 [label=NativeBatchNormBackward]
	140588144616080 -> 140588144608016
	140588144616080 [label=MkldnnConvolutionBackward]
	140588144582608 -> 140588144616080
	140588144582608 [label=ReluBackward1]
	140588144595216 -> 140588144582608
	140588144595216 [label=AddBackward0]
	140588144597776 -> 140588144595216
	140588144597776 [label=NativeBatchNormBackward]
	140584538697808 -> 140588144597776
	140584538697808 [label=MkldnnConvolutionBackward]
	140584538699984 -> 140584538697808
	140584538699984 [label=ReluBackward1]
	140584538699664 -> 140584538699984
	140584538699664 [label=NativeBatchNormBackward]
	140584538704272 -> 140584538699664
	140584538704272 [label=MkldnnConvolutionBackward]
	140588144598992 -> 140584538704272
	140588144598992 [label=ReluBackward1]
	140584538703824 -> 140588144598992
	140584538703824 [label=AddBackward0]
	140584538703952 -> 140584538703824
	140584538703952 [label=NativeBatchNormBackward]
	140584538703184 -> 140584538703952
	140584538703184 [label=MkldnnConvolutionBackward]
	140584538705616 -> 140584538703184
	140584538705616 [label=ReluBackward1]
	140584538705552 -> 140584538705616
	140584538705552 [label=NativeBatchNormBackward]
	140584538705808 -> 140584538705552
	140584538705808 [label=MkldnnConvolutionBackward]
	140584538704464 -> 140584538705808
	140584538704464 [label=ReluBackward1]
	140584538763536 -> 140584538704464
	140584538763536 [label=AddBackward0]
	140584538763728 -> 140584538763536
	140584538763728 [label=NativeBatchNormBackward]
	140584538763472 -> 140584538763728
	140584538763472 [label=MkldnnConvolutionBackward]
	140584538764112 -> 140584538763472
	140584538764112 [label=ReluBackward1]
	140584538764176 -> 140584538764112
	140584538764176 [label=NativeBatchNormBackward]
	140584538764496 -> 140584538764176
	140584538764496 [label=MkldnnConvolutionBackward]
	140584538763920 -> 140584538764496
	140584538763920 [label=ReluBackward1]
	140584538764816 -> 140584538763920
	140584538764816 [label=AddBackward0]
	140584538765008 -> 140584538764816
	140584538765008 [label=NativeBatchNormBackward]
	140584538764752 -> 140584538765008
	140584538764752 [label=MkldnnConvolutionBackward]
	140584538765392 -> 140584538764752
	140584538765392 [label=ReluBackward1]
	140584538765456 -> 140584538765392
	140584538765456 [label=NativeBatchNormBackward]
	140584538765776 -> 140584538765456
	140584538765776 [label=MkldnnConvolutionBackward]
	140584538765200 -> 140584538765776
	140584538765200 [label=ReluBackward1]
	140584538766096 -> 140584538765200
	140584538766096 [label=AddBackward0]
	140584538766288 -> 140584538766096
	140584538766288 [label=AddBackward0]
	140584538766416 -> 140584538766288
	140584538766416 [label=NativeBatchNormBackward]
	140584538766224 -> 140584538766416
	140584538766224 [label=MkldnnConvolutionBackward]
	140584538766800 -> 140584538766224
	140584538766800 [label=ReluBackward1]
	140584538766864 -> 140584538766800
	140584538766864 [label=NativeBatchNormBackward]
	140584538767184 -> 140584538766864
	140584538767184 [label=MkldnnConvolutionBackward]
	140588144607504 -> 140584538767184
	140584538775632 -> 140584538767184
	140587618316688 [label="stage3.2.fuse_layers.2.0.0.0.weight
 (48, 48, 3, 3)" fillcolor=lightblue]
	140587618316688 -> 140584538775632
	140584538775632 [label=AccumulateGrad]
	140584538767312 -> 140584538766864
	140587618316928 [label="stage3.2.fuse_layers.2.0.0.1.weight
 (48)" fillcolor=lightblue]
	140587618316928 -> 140584538767312
	140584538767312 [label=AccumulateGrad]
	140584538767056 -> 140584538766864
	140587618317008 [label="stage3.2.fuse_layers.2.0.0.1.bias
 (48)" fillcolor=lightblue]
	140587618317008 -> 140584538767056
	140584538767056 [label=AccumulateGrad]
	140584538766992 -> 140584538766224
	140587618317568 [label="stage3.2.fuse_layers.2.0.1.0.weight
 (192, 48, 3, 3)" fillcolor=lightblue]
	140587618317568 -> 140584538766992
	140584538766992 [label=AccumulateGrad]
	140584538766736 -> 140584538766416
	140587618317808 [label="stage3.2.fuse_layers.2.0.1.1.weight
 (192)" fillcolor=lightblue]
	140587618317808 -> 140584538766736
	140584538766736 [label=AccumulateGrad]
	140584538766672 -> 140584538766416
	140587618317888 [label="stage3.2.fuse_layers.2.0.1.1.bias
 (192)" fillcolor=lightblue]
	140587618317888 -> 140584538766672
	140584538766672 [label=AccumulateGrad]
	140584538766608 -> 140584538766288
	140584538766608 [label=NativeBatchNormBackward]
	140584538639376 -> 140584538766608
	140584538639376 [label=MkldnnConvolutionBackward]
	140588144609552 -> 140584538639376
	140584538776016 -> 140584538639376
	140587618318528 [label="stage3.2.fuse_layers.2.1.0.0.weight
 (192, 96, 3, 3)" fillcolor=lightblue]
	140587618318528 -> 140584538776016
	140584538776016 [label=AccumulateGrad]
	140584538767248 -> 140584538766608
	140587618318768 [label="stage3.2.fuse_layers.2.1.0.1.weight
 (192)" fillcolor=lightblue]
	140587618318768 -> 140584538767248
	140584538767248 [label=AccumulateGrad]
	140584538766928 -> 140584538766608
	140587618318848 [label="stage3.2.fuse_layers.2.1.0.1.bias
 (192)" fillcolor=lightblue]
	140587618318848 -> 140584538766928
	140584538766928 [label=AccumulateGrad]
	140584538574672 -> 140584538766096
	140584538765968 -> 140584538765776
	140587618508304 [label="stage3.3.branches.2.0.conv1.weight
 (192, 192, 3, 3)" fillcolor=lightblue]
	140587618508304 -> 140584538765968
	140584538765968 [label=AccumulateGrad]
	140584538765904 -> 140584538765456
	140587618508544 [label="stage3.3.branches.2.0.bn1.weight
 (192)" fillcolor=lightblue]
	140587618508544 -> 140584538765904
	140584538765904 [label=AccumulateGrad]
	140584538765648 -> 140584538765456
	140587618508624 [label="stage3.3.branches.2.0.bn1.bias
 (192)" fillcolor=lightblue]
	140587618508624 -> 140584538765648
	140584538765648 [label=AccumulateGrad]
	140584538765584 -> 140584538764752
	140587317231936 [label="stage3.3.branches.2.0.conv2.weight
 (192, 192, 3, 3)" fillcolor=lightblue]
	140587317231936 -> 140584538765584
	140584538765584 [label=AccumulateGrad]
	140584538765328 -> 140584538765008
	140587317232176 [label="stage3.3.branches.2.0.bn2.weight
 (192)" fillcolor=lightblue]
	140587317232176 -> 140584538765328
	140584538765328 [label=AccumulateGrad]
	140584538765264 -> 140584538765008
	140587317232256 [label="stage3.3.branches.2.0.bn2.bias
 (192)" fillcolor=lightblue]
	140587317232256 -> 140584538765264
	140584538765264 [label=AccumulateGrad]
	140584538765200 -> 140584538764816
	140584538764688 -> 140584538764496
	140587317232656 [label="stage3.3.branches.2.1.conv1.weight
 (192, 192, 3, 3)" fillcolor=lightblue]
	140587317232656 -> 140584538764688
	140584538764688 [label=AccumulateGrad]
	140584538764624 -> 140584538764176
	140587317232896 [label="stage3.3.branches.2.1.bn1.weight
 (192)" fillcolor=lightblue]
	140587317232896 -> 140584538764624
	140584538764624 [label=AccumulateGrad]
	140584538764368 -> 140584538764176
	140587317232976 [label="stage3.3.branches.2.1.bn1.bias
 (192)" fillcolor=lightblue]
	140587317232976 -> 140584538764368
	140584538764368 [label=AccumulateGrad]
	140584538764304 -> 140584538763472
	140587317233376 [label="stage3.3.branches.2.1.conv2.weight
 (192, 192, 3, 3)" fillcolor=lightblue]
	140587317233376 -> 140584538764304
	140584538764304 [label=AccumulateGrad]
	140584538764048 -> 140584538763728
	140587317233616 [label="stage3.3.branches.2.1.bn2.weight
 (192)" fillcolor=lightblue]
	140587317233616 -> 140584538764048
	140584538764048 [label=AccumulateGrad]
	140584538763984 -> 140584538763728
	140587317233696 [label="stage3.3.branches.2.1.bn2.bias
 (192)" fillcolor=lightblue]
	140587317233696 -> 140584538763984
	140584538763984 [label=AccumulateGrad]
	140584538763920 -> 140584538763536
	140584538763408 -> 140584538705808
	140587317234096 [label="stage3.3.branches.2.2.conv1.weight
 (192, 192, 3, 3)" fillcolor=lightblue]
	140587317234096 -> 140584538763408
	140584538763408 [label=AccumulateGrad]
	140584538705680 -> 140584538705552
	140587317234336 [label="stage3.3.branches.2.2.bn1.weight
 (192)" fillcolor=lightblue]
	140587317234336 -> 140584538705680
	140584538705680 [label=AccumulateGrad]
	140584538705744 -> 140584538705552
	140587317234416 [label="stage3.3.branches.2.2.bn1.bias
 (192)" fillcolor=lightblue]
	140587317234416 -> 140584538705744
	140584538705744 [label=AccumulateGrad]
	140584538704976 -> 140584538703184
	140587317234816 [label="stage3.3.branches.2.2.conv2.weight
 (192, 192, 3, 3)" fillcolor=lightblue]
	140587317234816 -> 140584538704976
	140584538704976 [label=AccumulateGrad]
	140584538705488 -> 140584538703952
	140587317235056 [label="stage3.3.branches.2.2.bn2.weight
 (192)" fillcolor=lightblue]
	140587317235056 -> 140584538705488
	140584538705488 [label=AccumulateGrad]
	140584538705040 -> 140584538703952
	140587317235136 [label="stage3.3.branches.2.2.bn2.bias
 (192)" fillcolor=lightblue]
	140587317235136 -> 140584538705040
	140584538705040 [label=AccumulateGrad]
	140584538704464 -> 140584538703824
	140584538703632 -> 140584538704272
	140587317235536 [label="stage3.3.branches.2.3.conv1.weight
 (192, 192, 3, 3)" fillcolor=lightblue]
	140587317235536 -> 140584538703632
	140584538703632 [label=AccumulateGrad]
	140584538703248 -> 140584538699664
	140587317289120 [label="stage3.3.branches.2.3.bn1.weight
 (192)" fillcolor=lightblue]
	140587317289120 -> 140584538703248
	140584538703248 [label=AccumulateGrad]
	140584538701904 -> 140584538699664
	140587317289200 [label="stage3.3.branches.2.3.bn1.bias
 (192)" fillcolor=lightblue]
	140587317289200 -> 140584538701904
	140584538701904 [label=AccumulateGrad]
	140584538700048 -> 140584538697808
	140587317289600 [label="stage3.3.branches.2.3.conv2.weight
 (192, 192, 3, 3)" fillcolor=lightblue]
	140587317289600 -> 140584538700048
	140584538700048 [label=AccumulateGrad]
	140584538699536 -> 140588144597776
	140587317289840 [label="stage3.3.branches.2.3.bn2.weight
 (192)" fillcolor=lightblue]
	140587317289840 -> 140584538699536
	140584538699536 [label=AccumulateGrad]
	140584538701584 -> 140588144597776
	140587317289920 [label="stage3.3.branches.2.3.bn2.bias
 (192)" fillcolor=lightblue]
	140587317289920 -> 140584538701584
	140584538701584 [label=AccumulateGrad]
	140588144598992 -> 140588144595216
	140588144596880 -> 140588144616080
	140587317291360 [label="stage3.3.fuse_layers.0.2.0.weight
 (48, 192, 1, 1)" fillcolor=lightblue]
	140587317291360 -> 140588144596880
	140588144596880 [label=AccumulateGrad]
	140588144582096 -> 140588144608016
	140587317291600 [label="stage3.3.fuse_layers.0.2.1.weight
 (48)" fillcolor=lightblue]
	140587317291600 -> 140588144582096
	140588144582096 [label=AccumulateGrad]
	140588144581264 -> 140588144608016
	140587317291680 [label="stage3.3.fuse_layers.0.2.1.bias
 (48)" fillcolor=lightblue]
	140587317291680 -> 140588144581264
	140588144581264 [label=AccumulateGrad]
	140588144580304 -> 140588144579984
	140587317409744 [label="stage4.0.branches.0.0.conv1.weight
 (48, 48, 3, 3)" fillcolor=lightblue]
	140587317409744 -> 140588144580304
	140588144580304 [label=AccumulateGrad]
	140588144580240 -> 140588144579728
	140587317409984 [label="stage4.0.branches.0.0.bn1.weight
 (48)" fillcolor=lightblue]
	140587317409984 -> 140588144580240
	140588144580240 [label=AccumulateGrad]
	140588144580176 -> 140588144579728
	140587317410064 [label="stage4.0.branches.0.0.bn1.bias
 (48)" fillcolor=lightblue]
	140587317410064 -> 140588144580176
	140588144580176 [label=AccumulateGrad]
	140588144579856 -> 140588144579344
	140587317410464 [label="stage4.0.branches.0.0.conv2.weight
 (48, 48, 3, 3)" fillcolor=lightblue]
	140587317410464 -> 140588144579856
	140588144579856 [label=AccumulateGrad]
	140588144579600 -> 140588144579152
	140587317410704 [label="stage4.0.branches.0.0.bn2.weight
 (48)" fillcolor=lightblue]
	140587317410704 -> 140588144579600
	140588144579600 [label=AccumulateGrad]
	140588144579536 -> 140588144579152
	140587317410784 [label="stage4.0.branches.0.0.bn2.bias
 (48)" fillcolor=lightblue]
	140587317410784 -> 140588144579536
	140588144579536 [label=AccumulateGrad]
	140588144579472 -> 140588144579024
	140588144578896 -> 140588144578704
	140587317411184 [label="stage4.0.branches.0.1.conv1.weight
 (48, 48, 3, 3)" fillcolor=lightblue]
	140587317411184 -> 140588144578896
	140588144578896 [label=AccumulateGrad]
	140588144578832 -> 140588144565968
	140587317411424 [label="stage4.0.branches.0.1.bn1.weight
 (48)" fillcolor=lightblue]
	140587317411424 -> 140588144578832
	140588144578832 [label=AccumulateGrad]
	140588144578768 -> 140588144565968
	140587317411504 [label="stage4.0.branches.0.1.bn1.bias
 (48)" fillcolor=lightblue]
	140587317411504 -> 140588144578768
	140588144578768 [label=AccumulateGrad]
	140588144566096 -> 140588144565584
	140587057303712 [label="stage4.0.branches.0.1.conv2.weight
 (48, 48, 3, 3)" fillcolor=lightblue]
	140587057303712 -> 140588144566096
	140588144566096 [label=AccumulateGrad]
	140588144565840 -> 140588144565392
	140587057303952 [label="stage4.0.branches.0.1.bn2.weight
 (48)" fillcolor=lightblue]
	140587057303952 -> 140588144565840
	140588144565840 [label=AccumulateGrad]
	140588144565776 -> 140588144565392
	140587057304032 [label="stage4.0.branches.0.1.bn2.bias
 (48)" fillcolor=lightblue]
	140587057304032 -> 140588144565776
	140588144565776 [label=AccumulateGrad]
	140588144565712 -> 140588144565264
	140588144565136 -> 140588144564816
	140587057304432 [label="stage4.0.branches.0.2.conv1.weight
 (48, 48, 3, 3)" fillcolor=lightblue]
	140587057304432 -> 140588144565136
	140588144565136 [label=AccumulateGrad]
	140588144565072 -> 140588144564560
	140587057304672 [label="stage4.0.branches.0.2.bn1.weight
 (48)" fillcolor=lightblue]
	140587057304672 -> 140588144565072
	140588144565072 [label=AccumulateGrad]
	140588144565008 -> 140588144564560
	140587057304752 [label="stage4.0.branches.0.2.bn1.bias
 (48)" fillcolor=lightblue]
	140587057304752 -> 140588144565008
	140588144565008 [label=AccumulateGrad]
	140588144564688 -> 140588144564176
	140587057305152 [label="stage4.0.branches.0.2.conv2.weight
 (48, 48, 3, 3)" fillcolor=lightblue]
	140587057305152 -> 140588144564688
	140588144564688 [label=AccumulateGrad]
	140588144564432 -> 140588144563984
	140587057305392 [label="stage4.0.branches.0.2.bn2.weight
 (48)" fillcolor=lightblue]
	140587057305392 -> 140588144564432
	140588144564432 [label=AccumulateGrad]
	140588144564368 -> 140588144563984
	140587057305472 [label="stage4.0.branches.0.2.bn2.bias
 (48)" fillcolor=lightblue]
	140587057305472 -> 140588144564368
	140588144564368 [label=AccumulateGrad]
	140588144564304 -> 140588144563856
	140588144563728 -> 140588144563408
	140587057305872 [label="stage4.0.branches.0.3.conv1.weight
 (48, 48, 3, 3)" fillcolor=lightblue]
	140587057305872 -> 140588144563728
	140588144563728 [label=AccumulateGrad]
	140588144563664 -> 140588144563152
	140587057306112 [label="stage4.0.branches.0.3.bn1.weight
 (48)" fillcolor=lightblue]
	140587057306112 -> 140588144563664
	140588144563664 [label=AccumulateGrad]
	140588144563600 -> 140588144563152
	140587057306192 [label="stage4.0.branches.0.3.bn1.bias
 (48)" fillcolor=lightblue]
	140587057306192 -> 140588144563600
	140588144563600 [label=AccumulateGrad]
	140588144563280 -> 140588144562768
	140587057306592 [label="stage4.0.branches.0.3.conv2.weight
 (48, 48, 3, 3)" fillcolor=lightblue]
	140587057306592 -> 140588144563280
	140588144563280 [label=AccumulateGrad]
	140588144563024 -> 140588144562576
	140587057306832 [label="stage4.0.branches.0.3.bn2.weight
 (48)" fillcolor=lightblue]
	140587057306832 -> 140588144563024
	140588144563024 [label=AccumulateGrad]
	140588144562960 -> 140588144562576
	140587057306912 [label="stage4.0.branches.0.3.bn2.bias
 (48)" fillcolor=lightblue]
	140587057306912 -> 140588144562960
	140588144562960 [label=AccumulateGrad]
	140588144562896 -> 140588144562384
	140588144562512 -> 140588144557840
	140588144562512 [label=UpsampleBilinear2DBackward1]
	140584538640144 -> 140588144562512
	140584538640144 [label=NativeBatchNormBackward]
	140588144562832 -> 140584538640144
	140588144562832 [label=MkldnnConvolutionBackward]
	140588144563920 -> 140588144562832
	140588144563920 [label=ReluBackward1]
	140588144563344 -> 140588144563920
	140588144563344 [label=AddBackward0]
	140588144564624 -> 140588144563344
	140588144564624 [label=NativeBatchNormBackward]
	140588144564240 -> 140588144564624
	140588144564240 [label=MkldnnConvolutionBackward]
	140588144565520 -> 140588144564240
	140588144565520 [label=ReluBackward1]
	140588144579088 -> 140588144565520
	140588144579088 [label=NativeBatchNormBackward]
	140588144579408 -> 140588144579088
	140588144579408 [label=MkldnnConvolutionBackward]
	140588144564944 -> 140588144579408
	140588144564944 [label=ReluBackward1]
	140588144580112 -> 140588144564944
	140588144580112 [label=AddBackward0]
	140588144598864 -> 140588144580112
	140588144598864 [label=NativeBatchNormBackward]
	140588144581008 -> 140588144598864
	140588144581008 [label=MkldnnConvolutionBackward]
	140584538705872 -> 140588144581008
	140584538705872 [label=ReluBackward1]
	140584538705168 -> 140584538705872
	140584538705168 [label=NativeBatchNormBackward]
	140584538705360 -> 140584538705168
	140584538705360 [label=MkldnnConvolutionBackward]
	140588144595664 -> 140584538705360
	140588144595664 [label=ReluBackward1]
	140584538764240 -> 140588144595664
	140584538764240 [label=AddBackward0]
	140584538766160 -> 140584538764240
	140584538766160 [label=NativeBatchNormBackward]
	140584538763856 -> 140584538766160
	140584538763856 [label=MkldnnConvolutionBackward]
	140584538765712 -> 140584538763856
	140584538765712 [label=ReluBackward1]
	140584538767120 -> 140584538765712
	140584538767120 [label=NativeBatchNormBackward]
	140584538766032 -> 140584538767120
	140584538766032 [label=MkldnnConvolutionBackward]
	140584538765136 -> 140584538766032
	140584538765136 [label=ReluBackward1]
	140584538775952 -> 140584538765136
	140584538775952 [label=AddBackward0]
	140584538776208 -> 140584538775952
	140584538776208 [label=NativeBatchNormBackward]
	140584538775760 -> 140584538776208
	140584538775760 [label=MkldnnConvolutionBackward]
	140584538776592 -> 140584538775760
	140584538776592 [label=ReluBackward1]
	140584538776656 -> 140584538776592
	140584538776656 [label=NativeBatchNormBackward]
	140584538776976 -> 140584538776656
	140584538776976 [label=MkldnnConvolutionBackward]
	140584538776400 -> 140584538776976
	140584538776400 [label=ReluBackward1]
	140584538777296 -> 140584538776400
	140584538777296 [label=AddBackward0]
	140584538777488 -> 140584538777296
	140584538777488 [label=AddBackward0]
	140584538777232 -> 140584538777488
	140584538777232 [label=NativeBatchNormBackward]
	140584538777808 -> 140584538777232
	140584538777808 [label=MkldnnConvolutionBackward]
	140588144580752 -> 140584538777808
	140584538778000 -> 140584538777808
	140587317292320 [label="stage3.3.fuse_layers.1.0.0.0.weight
 (96, 48, 3, 3)" fillcolor=lightblue]
	140587317292320 -> 140584538778000
	140584538778000 [label=AccumulateGrad]
	140584538777936 -> 140584538777232
	140587317292560 [label="stage3.3.fuse_layers.1.0.0.1.weight
 (96)" fillcolor=lightblue]
	140587317292560 -> 140584538777936
	140584538777936 [label=AccumulateGrad]
	140584538777424 -> 140584538777232
	140587317292640 [label="stage3.3.fuse_layers.1.0.0.1.bias
 (96)" fillcolor=lightblue]
	140587317292640 -> 140584538777424
	140584538777424 [label=AccumulateGrad]
	140588144581776 -> 140584538777488
	140584538777680 -> 140584538777296
	140584538777680 [label=UpsampleBilinear2DBackward1]
	140584538701648 -> 140584538777680
	140584538701648 [label=NativeBatchNormBackward]
	140584538777744 -> 140584538701648
	140584538777744 [label=MkldnnConvolutionBackward]
	140588144582608 -> 140584538777744
	140584538778192 -> 140584538777744
	140587317346624 [label="stage3.3.fuse_layers.1.2.0.weight
 (96, 192, 1, 1)" fillcolor=lightblue]
	140587317346624 -> 140584538778192
	140584538778192 [label=AccumulateGrad]
	140584538778320 -> 140584538701648
	140587317346864 [label="stage3.3.fuse_layers.1.2.1.weight
 (96)" fillcolor=lightblue]
	140587317346864 -> 140584538778320
	140584538778320 [label=AccumulateGrad]
	140584538778384 -> 140584538701648
	140587317346944 [label="stage3.3.fuse_layers.1.2.1.bias
 (96)" fillcolor=lightblue]
	140587317346944 -> 140584538778384
	140584538778384 [label=AccumulateGrad]
	140584538777168 -> 140584538776976
	140587057307472 [label="stage4.0.branches.1.0.conv1.weight
 (96, 96, 3, 3)" fillcolor=lightblue]
	140587057307472 -> 140584538777168
	140584538777168 [label=AccumulateGrad]
	140584538777104 -> 140584538776656
	140587057365152 [label="stage4.0.branches.1.0.bn1.weight
 (96)" fillcolor=lightblue]
	140587057365152 -> 140584538777104
	140584538777104 [label=AccumulateGrad]
	140584538776848 -> 140584538776656
	140587057365232 [label="stage4.0.branches.1.0.bn1.bias
 (96)" fillcolor=lightblue]
	140587057365232 -> 140584538776848
	140584538776848 [label=AccumulateGrad]
	140584538776784 -> 140584538775760
	140587057365632 [label="stage4.0.branches.1.0.conv2.weight
 (96, 96, 3, 3)" fillcolor=lightblue]
	140587057365632 -> 140584538776784
	140584538776784 [label=AccumulateGrad]
	140584538776528 -> 140584538776208
	140587057365872 [label="stage4.0.branches.1.0.bn2.weight
 (96)" fillcolor=lightblue]
	140587057365872 -> 140584538776528
	140584538776528 [label=AccumulateGrad]
	140584538776464 -> 140584538776208
	140587057365952 [label="stage4.0.branches.1.0.bn2.bias
 (96)" fillcolor=lightblue]
	140587057365952 -> 140584538776464
	140584538776464 [label=AccumulateGrad]
	140584538776400 -> 140584538775952
	140584538776080 -> 140584538766032
	140587057366352 [label="stage4.0.branches.1.1.conv1.weight
 (96, 96, 3, 3)" fillcolor=lightblue]
	140587057366352 -> 140584538776080
	140584538776080 [label=AccumulateGrad]
	140584538766480 -> 140584538767120
	140587057366592 [label="stage4.0.branches.1.1.bn1.weight
 (96)" fillcolor=lightblue]
	140587057366592 -> 140584538766480
	140584538766480 [label=AccumulateGrad]
	140584538775824 -> 140584538767120
	140587057366672 [label="stage4.0.branches.1.1.bn1.bias
 (96)" fillcolor=lightblue]
	140587057366672 -> 140584538775824
	140584538775824 [label=AccumulateGrad]
	140584538765840 -> 140584538763856
	140587057367072 [label="stage4.0.branches.1.1.conv2.weight
 (96, 96, 3, 3)" fillcolor=lightblue]
	140587057367072 -> 140584538765840
	140584538765840 [label=AccumulateGrad]
	140584538765072 -> 140584538766160
	140587057367312 [label="stage4.0.branches.1.1.bn2.weight
 (96)" fillcolor=lightblue]
	140587057367312 -> 140584538765072
	140584538765072 [label=AccumulateGrad]
	140584538765520 -> 140584538766160
	140587057367392 [label="stage4.0.branches.1.1.bn2.bias
 (96)" fillcolor=lightblue]
	140587057367392 -> 140584538765520
	140584538765520 [label=AccumulateGrad]
	140584538765136 -> 140584538764240
	140584538763664 -> 140584538705360
	140587057367792 [label="stage4.0.branches.1.2.conv1.weight
 (96, 96, 3, 3)" fillcolor=lightblue]
	140587057367792 -> 140584538763664
	140584538763664 [label=AccumulateGrad]
	140584538764880 -> 140584538705168
	140587057368032 [label="stage4.0.branches.1.2.bn1.weight
 (96)" fillcolor=lightblue]
	140587057368032 -> 140584538764880
	140584538764880 [label=AccumulateGrad]
	140584538763344 -> 140584538705168
	140587057368112 [label="stage4.0.branches.1.2.bn1.bias
 (96)" fillcolor=lightblue]
	140587057368112 -> 140584538763344
	140584538763344 [label=AccumulateGrad]
	140584538704784 -> 140588144581008
	140587057368512 [label="stage4.0.branches.1.2.conv2.weight
 (96, 96, 3, 3)" fillcolor=lightblue]
	140587057368512 -> 140584538704784
	140584538704784 [label=AccumulateGrad]
	140588144579920 -> 140588144598864
	140587057368752 [label="stage4.0.branches.1.2.bn2.weight
 (96)" fillcolor=lightblue]
	140587057368752 -> 140588144579920
	140588144579920 [label=AccumulateGrad]
	140584538702416 -> 140588144598864
	140587057368832 [label="stage4.0.branches.1.2.bn2.bias
 (96)" fillcolor=lightblue]
	140587057368832 -> 140584538702416
	140584538702416 [label=AccumulateGrad]
	140588144595664 -> 140588144580112
	140588144580816 -> 140588144579408
	140587057422576 [label="stage4.0.branches.1.3.conv1.weight
 (96, 96, 3, 3)" fillcolor=lightblue]
	140587057422576 -> 140588144580816
	140588144580816 [label=AccumulateGrad]
	140588144579792 -> 140588144579088
	140587057422816 [label="stage4.0.branches.1.3.bn1.weight
 (96)" fillcolor=lightblue]
	140587057422816 -> 140588144579792
	140588144579792 [label=AccumulateGrad]
	140588144578640 -> 140588144579088
	140587057422896 [label="stage4.0.branches.1.3.bn1.bias
 (96)" fillcolor=lightblue]
	140587057422896 -> 140588144578640
	140588144578640 [label=AccumulateGrad]
	140588144566224 -> 140588144564240
	140587057423296 [label="stage4.0.branches.1.3.conv2.weight
 (96, 96, 3, 3)" fillcolor=lightblue]
	140587057423296 -> 140588144566224
	140588144566224 [label=AccumulateGrad]
	140588144565648 -> 140588144564624
	140587057423536 [label="stage4.0.branches.1.3.bn2.weight
 (96)" fillcolor=lightblue]
	140587057423536 -> 140588144565648
	140588144565648 [label=AccumulateGrad]
	140588144565200 -> 140588144564624
	140587057423616 [label="stage4.0.branches.1.3.bn2.bias
 (96)" fillcolor=lightblue]
	140587057423616 -> 140588144565200
	140588144565200 [label=AccumulateGrad]
	140588144564944 -> 140588144563344
	140588144563792 -> 140588144562832
	140587304904416 [label="stage4.0.fuse_layers.0.1.0.weight
 (48, 96, 1, 1)" fillcolor=lightblue]
	140587304904416 -> 140588144563792
	140588144563792 [label=AccumulateGrad]
	140588144563216 -> 140584538640144
	140587304904656 [label="stage4.0.fuse_layers.0.1.1.weight
 (48)" fillcolor=lightblue]
	140587304904656 -> 140588144563216
	140588144563216 [label=AccumulateGrad]
	140588144562256 -> 140584538640144
	140587304904736 [label="stage4.0.fuse_layers.0.1.1.bias
 (48)" fillcolor=lightblue]
	140587304904736 -> 140588144562256
	140588144562256 [label=AccumulateGrad]
	140588144558032 -> 140588144557648
	140588144558032 [label=UpsampleBilinear2DBackward1]
	140584538698896 -> 140588144558032
	140584538698896 [label=NativeBatchNormBackward]
	140588144562704 -> 140584538698896
	140588144562704 [label=MkldnnConvolutionBackward]
	140588144565328 -> 140588144562704
	140588144565328 [label=ReluBackward1]
	140584538703056 -> 140588144565328
	140584538703056 [label=AddBackward0]
	140588144580496 -> 140584538703056
	140588144580496 [label=NativeBatchNormBackward]
	140588144579280 -> 140588144580496
	140588144579280 [label=MkldnnConvolutionBackward]
	140584538764432 -> 140588144579280
	140584538764432 [label=ReluBackward1]
	140584538764560 -> 140584538764432
	140584538764560 [label=NativeBatchNormBackward]
	140584538777360 -> 140584538764560
	140584538777360 [label=MkldnnConvolutionBackward]
	140588144580688 -> 140584538777360
	140588144580688 [label=ReluBackward1]
	140584538776912 -> 140588144580688
	140584538776912 [label=AddBackward0]
	140584538777040 -> 140584538776912
	140584538777040 [label=NativeBatchNormBackward]
	140584538776272 -> 140584538777040
	140584538776272 [label=MkldnnConvolutionBackward]
	140584538778704 -> 140584538776272
	140584538778704 [label=ReluBackward1]
	140584538778640 -> 140584538778704
	140584538778640 [label=NativeBatchNormBackward]
	140584538778896 -> 140584538778640
	140584538778896 [label=MkldnnConvolutionBackward]
	140584538777552 -> 140584538778896
	140584538777552 [label=ReluBackward1]
	140584538779216 -> 140584538777552
	140584538779216 [label=AddBackward0]
	140584538779408 -> 140584538779216
	140584538779408 [label=NativeBatchNormBackward]
	140584538779152 -> 140584538779408
	140584538779152 [label=MkldnnConvolutionBackward]
	140584557351120 -> 140584538779152
	140584557351120 [label=ReluBackward1]
	140584557351184 -> 140584557351120
	140584557351184 [label=NativeBatchNormBackward]
	140584557351504 -> 140584557351184
	140584557351504 [label=MkldnnConvolutionBackward]
	140584538779600 -> 140584557351504
	140584538779600 [label=ReluBackward1]
	140584557351824 -> 140584538779600
	140584557351824 [label=AddBackward0]
	140584557352016 -> 140584557351824
	140584557352016 [label=NativeBatchNormBackward]
	140584557351760 -> 140584557352016
	140584557351760 [label=MkldnnConvolutionBackward]
	140584557352400 -> 140584557351760
	140584557352400 [label=ReluBackward1]
	140584557352464 -> 140584557352400
	140584557352464 [label=NativeBatchNormBackward]
	140584557352784 -> 140584557352464
	140584557352784 [label=MkldnnConvolutionBackward]
	140584557352208 -> 140584557352784
	140584557352208 [label=ReluBackward1]
	140584557353104 -> 140584557352208
	140584557353104 [label=AddBackward0]
	140584557353296 -> 140584557353104
	140584557353296 [label=AddBackward0]
	140584557353424 -> 140584557353296
	140584557353424 [label=NativeBatchNormBackward]
	140584557353232 -> 140584557353424
	140584557353232 [label=MkldnnConvolutionBackward]
	140584557353808 -> 140584557353232
	140584557353808 [label=ReluBackward1]
	140584557353872 -> 140584557353808
	140584557353872 [label=NativeBatchNormBackward]
	140584557354192 -> 140584557353872
	140584557354192 [label=MkldnnConvolutionBackward]
	140588144580752 -> 140584557354192
	140584557354384 -> 140584557354192
	140587317347584 [label="stage3.3.fuse_layers.2.0.0.0.weight
 (48, 48, 3, 3)" fillcolor=lightblue]
	140587317347584 -> 140584557354384
	140584557354384 [label=AccumulateGrad]
	140584557354320 -> 140584557353872
	140587317347824 [label="stage3.3.fuse_layers.2.0.0.1.weight
 (48)" fillcolor=lightblue]
	140587317347824 -> 140584557354320
	140584557354320 [label=AccumulateGrad]
	140584557354064 -> 140584557353872
	140587317347904 [label="stage3.3.fuse_layers.2.0.0.1.bias
 (48)" fillcolor=lightblue]
	140587317347904 -> 140584557354064
	140584557354064 [label=AccumulateGrad]
	140584557354000 -> 140584557353232
	140587317348464 [label="stage3.3.fuse_layers.2.0.1.0.weight
 (192, 48, 3, 3)" fillcolor=lightblue]
	140587317348464 -> 140584557354000
	140584557354000 [label=AccumulateGrad]
	140584557353744 -> 140584557353424
	140587317348704 [label="stage3.3.fuse_layers.2.0.1.1.weight
 (192)" fillcolor=lightblue]
	140587317348704 -> 140584557353744
	140584557353744 [label=AccumulateGrad]
	140584557353680 -> 140584557353424
	140587317348784 [label="stage3.3.fuse_layers.2.0.1.1.bias
 (192)" fillcolor=lightblue]
	140587317348784 -> 140584557353680
	140584557353680 [label=AccumulateGrad]
	140584557353616 -> 140584557353296
	140584557353616 [label=NativeBatchNormBackward]
	140588144564752 -> 140584557353616
	140588144564752 [label=MkldnnConvolutionBackward]
	140588144581776 -> 140588144564752
	140584557354640 -> 140588144564752
	140587317349424 [label="stage3.3.fuse_layers.2.1.0.0.weight
 (192, 96, 3, 3)" fillcolor=lightblue]
	140587317349424 -> 140584557354640
	140584557354640 [label=AccumulateGrad]
	140584557354768 -> 140584557353616
	140587317349664 [label="stage3.3.fuse_layers.2.1.0.1.weight
 (192)" fillcolor=lightblue]
	140587317349664 -> 140584557354768
	140584557354768 [label=AccumulateGrad]
	140584557354128 -> 140584557353616
	140587317349744 [label="stage3.3.fuse_layers.2.1.0.1.bias
 (192)" fillcolor=lightblue]
	140587317349744 -> 140584557354128
	140584557354128 [label=AccumulateGrad]
	140588144582608 -> 140584557353104
	140584557352976 -> 140584557352784
	140587057424176 [label="stage4.0.branches.2.0.conv1.weight
 (192, 192, 3, 3)" fillcolor=lightblue]
	140587057424176 -> 140584557352976
	140584557352976 [label=AccumulateGrad]
	140584557352912 -> 140584557352464
	140587057424416 [label="stage4.0.branches.2.0.bn1.weight
 (192)" fillcolor=lightblue]
	140587057424416 -> 140584557352912
	140584557352912 [label=AccumulateGrad]
	140584557352656 -> 140584557352464
	140587057424496 [label="stage4.0.branches.2.0.bn1.bias
 (192)" fillcolor=lightblue]
	140587057424496 -> 140584557352656
	140584557352656 [label=AccumulateGrad]
	140584557352592 -> 140584557351760
	140587057424896 [label="stage4.0.branches.2.0.conv2.weight
 (192, 192, 3, 3)" fillcolor=lightblue]
	140587057424896 -> 140584557352592
	140584557352592 [label=AccumulateGrad]
	140584557352336 -> 140584557352016
	140587057425136 [label="stage4.0.branches.2.0.bn2.weight
 (192)" fillcolor=lightblue]
	140587057425136 -> 140584557352336
	140584557352336 [label=AccumulateGrad]
	140584557352272 -> 140584557352016
	140587057425216 [label="stage4.0.branches.2.0.bn2.bias
 (192)" fillcolor=lightblue]
	140587057425216 -> 140584557352272
	140584557352272 [label=AccumulateGrad]
	140584557352208 -> 140584557351824
	140584557351696 -> 140584557351504
	140587057425616 [label="stage4.0.branches.2.1.conv1.weight
 (192, 192, 3, 3)" fillcolor=lightblue]
	140587057425616 -> 140584557351696
	140584557351696 [label=AccumulateGrad]
	140584557351632 -> 140584557351184
	140587057425856 [label="stage4.0.branches.2.1.bn1.weight
 (192)" fillcolor=lightblue]
	140587057425856 -> 140584557351632
	140584557351632 [label=AccumulateGrad]
	140584557351376 -> 140584557351184
	140587057425936 [label="stage4.0.branches.2.1.bn1.bias
 (192)" fillcolor=lightblue]
	140587057425936 -> 140584557351376
	140584557351376 [label=AccumulateGrad]
	140584557351312 -> 140584538779152
	140587057426336 [label="stage4.0.branches.2.1.conv2.weight
 (192, 192, 3, 3)" fillcolor=lightblue]
	140587057426336 -> 140584557351312
	140584557351312 [label=AccumulateGrad]
	140584538779344 -> 140584538779408
	140587057488112 [label="stage4.0.branches.2.1.bn2.weight
 (192)" fillcolor=lightblue]
	140587057488112 -> 140584538779344
	140584538779344 [label=AccumulateGrad]
	140584557351056 -> 140584538779408
	140587057488192 [label="stage4.0.branches.2.1.bn2.bias
 (192)" fillcolor=lightblue]
	140587057488192 -> 140584557351056
	140584557351056 [label=AccumulateGrad]
	140584538779600 -> 140584538779216
	140584538779088 -> 140584538778896
	140587057488592 [label="stage4.0.branches.2.2.conv1.weight
 (192, 192, 3, 3)" fillcolor=lightblue]
	140587057488592 -> 140584538779088
	140584538779088 [label=AccumulateGrad]
	140584538779024 -> 140584538778640
	140587057488832 [label="stage4.0.branches.2.2.bn1.weight
 (192)" fillcolor=lightblue]
	140587057488832 -> 140584538779024
	140584538779024 [label=AccumulateGrad]
	140584538778768 -> 140584538778640
	140587057488912 [label="stage4.0.branches.2.2.bn1.bias
 (192)" fillcolor=lightblue]
	140587057488912 -> 140584538778768
	140584538778768 [label=AccumulateGrad]
	140584538778064 -> 140584538776272
	140587057489312 [label="stage4.0.branches.2.2.conv2.weight
 (192, 192, 3, 3)" fillcolor=lightblue]
	140587057489312 -> 140584538778064
	140584538778064 [label=AccumulateGrad]
	140584538778576 -> 140584538777040
	140587057489552 [label="stage4.0.branches.2.2.bn2.weight
 (192)" fillcolor=lightblue]
	140587057489552 -> 140584538778576
	140584538778576 [label=AccumulateGrad]
	140584538778128 -> 140584538777040
	140587057489632 [label="stage4.0.branches.2.2.bn2.bias
 (192)" fillcolor=lightblue]
	140587057489632 -> 140584538778128
	140584538778128 [label=AccumulateGrad]
	140584538777552 -> 140584538776912
	140584538776720 -> 140584538777360
	140587057490032 [label="stage4.0.branches.2.3.conv1.weight
 (192, 192, 3, 3)" fillcolor=lightblue]
	140587057490032 -> 140584538776720
	140584538776720 [label=AccumulateGrad]
	140584538776336 -> 140584538764560
	140587057490272 [label="stage4.0.branches.2.3.bn1.weight
 (192)" fillcolor=lightblue]
	140587057490272 -> 140584538776336
	140584538776336 [label=AccumulateGrad]
	140584538775888 -> 140584538764560
	140587057490352 [label="stage4.0.branches.2.3.bn1.bias
 (192)" fillcolor=lightblue]
	140587057490352 -> 140584538775888
	140584538775888 [label=AccumulateGrad]
	140584538764944 -> 140588144579280
	140587057490752 [label="stage4.0.branches.2.3.conv2.weight
 (192, 192, 3, 3)" fillcolor=lightblue]
	140587057490752 -> 140584538764944
	140584538764944 [label=AccumulateGrad]
	140584538766352 -> 140588144580496
	140587057490992 [label="stage4.0.branches.2.3.bn2.weight
 (192)" fillcolor=lightblue]
	140587057490992 -> 140584538766352
	140584538766352 [label=AccumulateGrad]
	140584538763792 -> 140588144580496
	140587057491072 [label="stage4.0.branches.2.3.bn2.bias
 (192)" fillcolor=lightblue]
	140587057491072 -> 140584538763792
	140584538763792 [label=AccumulateGrad]
	140588144580688 -> 140584538703056
	140584538704528 -> 140588144562704
	140587304905216 [label="stage4.0.fuse_layers.0.2.0.weight
 (48, 192, 1, 1)" fillcolor=lightblue]
	140587304905216 -> 140584538704528
	140584538704528 [label=AccumulateGrad]
	140588144566032 -> 140584538698896
	140587304905456 [label="stage4.0.fuse_layers.0.2.1.weight
 (48)" fillcolor=lightblue]
	140587304905456 -> 140588144566032
	140588144566032 [label=AccumulateGrad]
	140588144564112 -> 140584538698896
	140587304905536 [label="stage4.0.fuse_layers.0.2.1.bias
 (48)" fillcolor=lightblue]
	140587304905536 -> 140588144564112
	140588144564112 [label=AccumulateGrad]
	140588144557968 -> 140588144557520
	140588144557968 [label=UpsampleBilinear2DBackward1]
	140588144557456 -> 140588144557968
	140588144557456 [label=NativeBatchNormBackward]
	140584538766544 -> 140588144557456
	140584538766544 [label=MkldnnConvolutionBackward]
	140588144597584 -> 140584538766544
	140588144597584 [label=ReluBackward1]
	140584538778512 -> 140588144597584
	140584538778512 [label=AddBackward0]
	140584538777872 -> 140584538778512
	140584538777872 [label=NativeBatchNormBackward]
	140584538778256 -> 140584538777872
	140584538778256 [label=MkldnnConvolutionBackward]
	140584538778960 -> 140584538778256
	140584538778960 [label=ReluBackward1]
	140584557351888 -> 140584538778960
	140584557351888 [label=NativeBatchNormBackward]
	140584557351952 -> 140584557351888
	140584557351952 [label=MkldnnConvolutionBackward]
	140584538779536 -> 140584557351952
	140584538779536 [label=ReluBackward1]
	140584557352720 -> 140584538779536
	140584557352720 [label=AddBackward0]
	140584557352848 -> 140584557352720
	140584557352848 [label=NativeBatchNormBackward]
	140584557352080 -> 140584557352848
	140584557352080 [label=MkldnnConvolutionBackward]
	140584557354832 -> 140584557352080
	140584557354832 [label=ReluBackward1]
	140584557354512 -> 140584557354832
	140584557354512 [label=NativeBatchNormBackward]
	140584557354896 -> 140584557354512
	140584557354896 [label=MkldnnConvolutionBackward]
	140584557353360 -> 140584557354896
	140584557353360 [label=ReluBackward1]
	140584557375888 -> 140584557353360
	140584557375888 [label=AddBackward0]
	140584557376080 -> 140584557375888
	140584557376080 [label=NativeBatchNormBackward]
	140584557375824 -> 140584557376080
	140584557375824 [label=MkldnnConvolutionBackward]
	140584557376464 -> 140584557375824
	140584557376464 [label=ReluBackward1]
	140584557376528 -> 140584557376464
	140584557376528 [label=NativeBatchNormBackward]
	140584557376848 -> 140584557376528
	140584557376848 [label=MkldnnConvolutionBackward]
	140584557376272 -> 140584557376848
	140584557376272 [label=ReluBackward1]
	140584557377168 -> 140584557376272
	140584557377168 [label=AddBackward0]
	140584557377360 -> 140584557377168
	140584557377360 [label=NativeBatchNormBackward]
	140584557377104 -> 140584557377360
	140584557377104 [label=MkldnnConvolutionBackward]
	140584557377744 -> 140584557377104
	140584557377744 [label=ReluBackward1]
	140584557377808 -> 140584557377744
	140584557377808 [label=NativeBatchNormBackward]
	140584557378128 -> 140584557377808
	140584557378128 [label=MkldnnConvolutionBackward]
	140584557377552 -> 140584557378128
	140584557377552 [label=ReluBackward1]
	140584557378448 -> 140584557377552
	140584557378448 [label=NativeBatchNormBackward]
	140584557378640 -> 140584557378448
	140584557378640 [label=MkldnnConvolutionBackward]
	140584557352208 -> 140584557378640
	140584557378832 -> 140584557378640
	140587317408384 [label="transition3.3.0.0.weight
 (384, 192, 3, 3)" fillcolor=lightblue]
	140587317408384 -> 140584557378832
	140584557378832 [label=AccumulateGrad]
	140584557378768 -> 140584557378448
	140587317408624 [label="transition3.3.0.1.weight
 (384)" fillcolor=lightblue]
	140587317408624 -> 140584557378768
	140584557378768 [label=AccumulateGrad]
	140584557378384 -> 140584557378448
	140587317408704 [label="transition3.3.0.1.bias
 (384)" fillcolor=lightblue]
	140587317408704 -> 140584557378384
	140584557378384 [label=AccumulateGrad]
	140584557378320 -> 140584557378128
	140587057491632 [label="stage4.0.branches.3.0.conv1.weight
 (384, 384, 3, 3)" fillcolor=lightblue]
	140587057491632 -> 140584557378320
	140584557378320 [label=AccumulateGrad]
	140584557378256 -> 140584557377808
	140587057491872 [label="stage4.0.branches.3.0.bn1.weight
 (384)" fillcolor=lightblue]
	140587057491872 -> 140584557378256
	140584557378256 [label=AccumulateGrad]
	140584557378000 -> 140584557377808
	140587304845392 [label="stage4.0.branches.3.0.bn1.bias
 (384)" fillcolor=lightblue]
	140587304845392 -> 140584557378000
	140584557378000 [label=AccumulateGrad]
	140584557377936 -> 140584557377104
	140587304845792 [label="stage4.0.branches.3.0.conv2.weight
 (384, 384, 3, 3)" fillcolor=lightblue]
	140587304845792 -> 140584557377936
	140584557377936 [label=AccumulateGrad]
	140584557377680 -> 140584557377360
	140587304846032 [label="stage4.0.branches.3.0.bn2.weight
 (384)" fillcolor=lightblue]
	140587304846032 -> 140584557377680
	140584557377680 [label=AccumulateGrad]
	140584557377616 -> 140584557377360
	140587304846112 [label="stage4.0.branches.3.0.bn2.bias
 (384)" fillcolor=lightblue]
	140587304846112 -> 140584557377616
	140584557377616 [label=AccumulateGrad]
	140584557377552 -> 140584557377168
	140584557377040 -> 140584557376848
	140587304846512 [label="stage4.0.branches.3.1.conv1.weight
 (384, 384, 3, 3)" fillcolor=lightblue]
	140587304846512 -> 140584557377040
	140584557377040 [label=AccumulateGrad]
	140584557376976 -> 140584557376528
	140587304846752 [label="stage4.0.branches.3.1.bn1.weight
 (384)" fillcolor=lightblue]
	140587304846752 -> 140584557376976
	140584557376976 [label=AccumulateGrad]
	140584557376720 -> 140584557376528
	140587304846832 [label="stage4.0.branches.3.1.bn1.bias
 (384)" fillcolor=lightblue]
	140587304846832 -> 140584557376720
	140584557376720 [label=AccumulateGrad]
	140584557376656 -> 140584557375824
	140587304847232 [label="stage4.0.branches.3.1.conv2.weight
 (384, 384, 3, 3)" fillcolor=lightblue]
	140587304847232 -> 140584557376656
	140584557376656 [label=AccumulateGrad]
	140584557376400 -> 140584557376080
	140587304847472 [label="stage4.0.branches.3.1.bn2.weight
 (384)" fillcolor=lightblue]
	140587304847472 -> 140584557376400
	140584557376400 [label=AccumulateGrad]
	140584557376336 -> 140584557376080
	140587304847552 [label="stage4.0.branches.3.1.bn2.bias
 (384)" fillcolor=lightblue]
	140587304847552 -> 140584557376336
	140584557376336 [label=AccumulateGrad]
	140584557376272 -> 140584557375888
	140584557375760 -> 140584557354896
	140587304847952 [label="stage4.0.branches.3.2.conv1.weight
 (384, 384, 3, 3)" fillcolor=lightblue]
	140587304847952 -> 140584557375760
	140584557375760 [label=AccumulateGrad]
	140584557375696 -> 140584557354512
	140587304848192 [label="stage4.0.branches.3.2.bn1.weight
 (384)" fillcolor=lightblue]
	140587304848192 -> 140584557375696
	140584557375696 [label=AccumulateGrad]
	140584557375568 -> 140584557354512
	140587304848272 [label="stage4.0.branches.3.2.bn1.bias
 (384)" fillcolor=lightblue]
	140587304848272 -> 140584557375568
	140584557375568 [label=AccumulateGrad]
	140584557354256 -> 140584557352080
	140587304848672 [label="stage4.0.branches.3.2.conv2.weight
 (384, 384, 3, 3)" fillcolor=lightblue]
	140587304848672 -> 140584557354256
	140584557354256 [label=AccumulateGrad]
	140584557354576 -> 140584557352848
	140587304848912 [label="stage4.0.branches.3.2.bn2.weight
 (384)" fillcolor=lightblue]
	140587304848912 -> 140584557354576
	140584557354576 [label=AccumulateGrad]
	140584557353936 -> 140584557352848
	140587304848992 [label="stage4.0.branches.3.2.bn2.bias
 (384)" fillcolor=lightblue]
	140587304848992 -> 140584557353936
	140584557353936 [label=AccumulateGrad]
	140584557353360 -> 140584557352720
	140584557352528 -> 140584557351952
	140587304902736 [label="stage4.0.branches.3.3.conv1.weight
 (384, 384, 3, 3)" fillcolor=lightblue]
	140587304902736 -> 140584557352528
	140584557352528 [label=AccumulateGrad]
	140584557352144 -> 140584557351888
	140587304902976 [label="stage4.0.branches.3.3.bn1.weight
 (384)" fillcolor=lightblue]
	140587304902976 -> 140584557352144
	140584557352144 [label=AccumulateGrad]
	140584557351440 -> 140584557351888
	140587304903056 [label="stage4.0.branches.3.3.bn1.bias
 (384)" fillcolor=lightblue]
	140587304903056 -> 140584557351440
	140584557351440 [label=AccumulateGrad]
	140584557350992 -> 140584538778256
	140587304903456 [label="stage4.0.branches.3.3.conv2.weight
 (384, 384, 3, 3)" fillcolor=lightblue]
	140587304903456 -> 140584557350992
	140584557350992 [label=AccumulateGrad]
	140584538778832 -> 140584538777872
	140587304903696 [label="stage4.0.branches.3.3.bn2.weight
 (384)" fillcolor=lightblue]
	140587304903696 -> 140584538778832
	140584538778832 [label=AccumulateGrad]
	140584538779472 -> 140584538777872
	140587304903776 [label="stage4.0.branches.3.3.bn2.bias
 (384)" fillcolor=lightblue]
	140587304903776 -> 140584538779472
	140584538779472 [label=AccumulateGrad]
	140584538779536 -> 140584538778512
	140584538779280 -> 140584538766544
	140587304906016 [label="stage4.0.fuse_layers.0.3.0.weight
 (48, 384, 1, 1)" fillcolor=lightblue]
	140587304906016 -> 140584538779280
	140584538779280 [label=AccumulateGrad]
	140588144578960 -> 140588144557456
	140587304906256 [label="stage4.0.fuse_layers.0.3.1.weight
 (48)" fillcolor=lightblue]
	140587304906256 -> 140588144578960
	140588144578960 [label=AccumulateGrad]
	140588144562448 -> 140588144557456
	140587304906336 [label="stage4.0.fuse_layers.0.3.1.bias
 (48)" fillcolor=lightblue]
	140587304906336 -> 140588144562448
	140588144562448 [label=AccumulateGrad]
	140588144557392 -> 140588144557072
	140587637404528 [label="stage4.1.branches.0.0.conv1.weight
 (48, 48, 3, 3)" fillcolor=lightblue]
	140587637404528 -> 140588144557392
	140588144557392 [label=AccumulateGrad]
	140588144557328 -> 140588144556816
	140587637404768 [label="stage4.1.branches.0.0.bn1.weight
 (48)" fillcolor=lightblue]
	140587637404768 -> 140588144557328
	140588144557328 [label=AccumulateGrad]
	140588144557264 -> 140588144556816
	140587637404848 [label="stage4.1.branches.0.0.bn1.bias
 (48)" fillcolor=lightblue]
	140587637404848 -> 140588144557264
	140588144557264 [label=AccumulateGrad]
	140588144556944 -> 140588144556432
	140587637405248 [label="stage4.1.branches.0.0.conv2.weight
 (48, 48, 3, 3)" fillcolor=lightblue]
	140587637405248 -> 140588144556944
	140588144556944 [label=AccumulateGrad]
	140588144556688 -> 140588144556240
	140587637405488 [label="stage4.1.branches.0.0.bn2.weight
 (48)" fillcolor=lightblue]
	140587637405488 -> 140588144556688
	140588144556688 [label=AccumulateGrad]
	140588144556624 -> 140588144556240
	140587637405568 [label="stage4.1.branches.0.0.bn2.bias
 (48)" fillcolor=lightblue]
	140587637405568 -> 140588144556624
	140588144556624 [label=AccumulateGrad]
	140588144556560 -> 140588144556112
	140588144555984 -> 140588144555664
	140587637405968 [label="stage4.1.branches.0.1.conv1.weight
 (48, 48, 3, 3)" fillcolor=lightblue]
	140587637405968 -> 140588144555984
	140588144555984 [label=AccumulateGrad]
	140588144555920 -> 140588144555408
	140587637406208 [label="stage4.1.branches.0.1.bn1.weight
 (48)" fillcolor=lightblue]
	140587637406208 -> 140588144555920
	140588144555920 [label=AccumulateGrad]
	140588144555856 -> 140588144555408
	140587637406288 [label="stage4.1.branches.0.1.bn1.bias
 (48)" fillcolor=lightblue]
	140587637406288 -> 140588144555856
	140588144555856 [label=AccumulateGrad]
	140588144555536 -> 140588144555024
	140587637406688 [label="stage4.1.branches.0.1.conv2.weight
 (48, 48, 3, 3)" fillcolor=lightblue]
	140587637406688 -> 140588144555536
	140588144555536 [label=AccumulateGrad]
	140588144555280 -> 140588144554832
	140587637406928 [label="stage4.1.branches.0.1.bn2.weight
 (48)" fillcolor=lightblue]
	140587637406928 -> 140588144555280
	140588144555280 [label=AccumulateGrad]
	140588144555216 -> 140588144554832
	140587637407008 [label="stage4.1.branches.0.1.bn2.bias
 (48)" fillcolor=lightblue]
	140587637407008 -> 140588144555216
	140588144555216 [label=AccumulateGrad]
	140588144555152 -> 140588144554704
	140588144554576 -> 140588144554256
	140587637407408 [label="stage4.1.branches.0.2.conv1.weight
 (48, 48, 3, 3)" fillcolor=lightblue]
	140587637407408 -> 140588144554576
	140588144554576 [label=AccumulateGrad]
	140588144554512 -> 140588144554064
	140587637407648 [label="stage4.1.branches.0.2.bn1.weight
 (48)" fillcolor=lightblue]
	140587637407648 -> 140588144554512
	140588144554512 [label=AccumulateGrad]
	140588144554448 -> 140588144554064
	140587637473360 [label="stage4.1.branches.0.2.bn1.bias
 (48)" fillcolor=lightblue]
	140587637473360 -> 140588144554448
	140588144554448 [label=AccumulateGrad]
	140588144541648 -> 140588144541264
	140587637473760 [label="stage4.1.branches.0.2.conv2.weight
 (48, 48, 3, 3)" fillcolor=lightblue]
	140587637473760 -> 140588144541648
	140588144541648 [label=AccumulateGrad]
	140588144541520 -> 140588144541072
	140587637474000 [label="stage4.1.branches.0.2.bn2.weight
 (48)" fillcolor=lightblue]
	140587637474000 -> 140588144541520
	140588144541520 [label=AccumulateGrad]
	140588144541456 -> 140588144541072
	140587637474080 [label="stage4.1.branches.0.2.bn2.bias
 (48)" fillcolor=lightblue]
	140587637474080 -> 140588144541456
	140588144541456 [label=AccumulateGrad]
	140588144541392 -> 140588144540944
	140588144540816 -> 140588144540624
	140587637474480 [label="stage4.1.branches.0.3.conv1.weight
 (48, 48, 3, 3)" fillcolor=lightblue]
	140587637474480 -> 140588144540816
	140588144540816 [label=AccumulateGrad]
	140588144540752 -> 140588144540304
	140587637474720 [label="stage4.1.branches.0.3.bn1.weight
 (48)" fillcolor=lightblue]
	140587637474720 -> 140588144540752
	140588144540752 [label=AccumulateGrad]
	140588144540496 -> 140588144540304
	140587637474800 [label="stage4.1.branches.0.3.bn1.bias
 (48)" fillcolor=lightblue]
	140587637474800 -> 140588144540496
	140588144540496 [label=AccumulateGrad]
	140588144540432 -> 140587842056464
	140587637475200 [label="stage4.1.branches.0.3.conv2.weight
 (48, 48, 3, 3)" fillcolor=lightblue]
	140587637475200 -> 140588144540432
	140588144540432 [label=AccumulateGrad]
	140587842059344 -> 140587720590480
	140587637475440 [label="stage4.1.branches.0.3.bn2.weight
 (48)" fillcolor=lightblue]
	140587637475440 -> 140587842059344
	140587842059344 [label=AccumulateGrad]
	140588144540176 -> 140587720590480
	140587637475520 [label="stage4.1.branches.0.3.bn2.bias
 (48)" fillcolor=lightblue]
	140587637475520 -> 140588144540176
	140588144540176 [label=AccumulateGrad]
	140587720545744 -> 140587720593040
	140587842058128 -> 140587037651024
	140587842058128 [label=UpsampleBilinear2DBackward1]
	140587720590544 -> 140587842058128
	140587720590544 [label=NativeBatchNormBackward]
	140587840180688 -> 140587720590544
	140587840180688 [label=MkldnnConvolutionBackward]
	140588144540880 -> 140587840180688
	140588144540880 [label=ReluBackward1]
	140588144540368 -> 140588144540880
	140588144540368 [label=AddBackward0]
	140588144554384 -> 140588144540368
	140588144554384 [label=NativeBatchNormBackward]
	140588144555088 -> 140588144554384
	140588144555088 [label=MkldnnConvolutionBackward]
	140588144554960 -> 140588144555088
	140588144554960 [label=ReluBackward1]
	140588144555600 -> 140588144554960
	140588144555600 [label=NativeBatchNormBackward]
	140588144557584 -> 140588144555600
	140588144557584 [label=MkldnnConvolutionBackward]
	140588144554640 -> 140588144557584
	140588144554640 [label=ReluBackward1]
	140588144563536 -> 140588144554640
	140588144563536 [label=AddBackward0]
	140588144556880 -> 140588144563536
	140588144556880 [label=NativeBatchNormBackward]
	140584538777616 -> 140588144556880
	140584538777616 [label=MkldnnConvolutionBackward]
	140584557354960 -> 140584538777616
	140584557354960 [label=ReluBackward1]
	140584557353552 -> 140584557354960
	140584557353552 [label=NativeBatchNormBackward]
	140584557354704 -> 140584557353552
	140584557354704 [label=MkldnnConvolutionBackward]
	140584538775696 -> 140584557354704
	140584538775696 [label=ReluBackward1]
	140584557376016 -> 140584538775696
	140584557376016 [label=AddBackward0]
	140584557378064 -> 140584557376016
	140584557378064 [label=NativeBatchNormBackward]
	140584557376144 -> 140584557378064
	140584557376144 [label=MkldnnConvolutionBackward]
	140584557378192 -> 140584557376144
	140584557378192 [label=ReluBackward1]
	140584557379216 -> 140584557378192
	140584557379216 [label=NativeBatchNormBackward]
	140584557378512 -> 140584557379216
	140584557378512 [label=MkldnnConvolutionBackward]
	140584557377424 -> 140584557378512
	140584557377424 [label=ReluBackward1]
	140584557379280 -> 140584557377424
	140584557379280 [label=AddBackward0]
	140584557379472 -> 140584557379280
	140584557379472 [label=NativeBatchNormBackward]
	140584557416592 -> 140584557379472
	140584557416592 [label=MkldnnConvolutionBackward]
	140584557416784 -> 140584557416592
	140584557416784 [label=ReluBackward1]
	140584557416848 -> 140584557416784
	140584557416848 [label=NativeBatchNormBackward]
	140584557417168 -> 140584557416848
	140584557417168 [label=MkldnnConvolutionBackward]
	140584557378960 -> 140584557417168
	140584557378960 [label=ReluBackward1]
	140584557417488 -> 140584557378960
	140584557417488 [label=AddBackward0]
	140584557417680 -> 140584557417488
	140584557417680 [label=AddBackward0]
	140584557417424 -> 140584557417680
	140584557417424 [label=AddBackward0]
	140584557417936 -> 140584557417424
	140584557417936 [label=NativeBatchNormBackward]
	140584557418192 -> 140584557417936
	140584557418192 [label=MkldnnConvolutionBackward]
	140588144562320 -> 140584557418192
	140584557418384 -> 140584557418192
	140587304960320 [label="stage4.0.fuse_layers.1.0.0.0.weight
 (96, 48, 3, 3)" fillcolor=lightblue]
	140587304960320 -> 140584557418384
	140584557418384 [label=AccumulateGrad]
	140584557418320 -> 140584557417936
	140587304960560 [label="stage4.0.fuse_layers.1.0.0.1.weight
 (96)" fillcolor=lightblue]
	140587304960560 -> 140584557418320
	140584557418320 [label=AccumulateGrad]
	140584557417616 -> 140584557417936
	140587304960640 [label="stage4.0.fuse_layers.1.0.0.1.bias
 (96)" fillcolor=lightblue]
	140587304960640 -> 140584557417616
	140584557417616 [label=AccumulateGrad]
	140588144563920 -> 140584557417424
	140584557418064 -> 140584557417680
	140584557418064 [label=UpsampleBilinear2DBackward1]
	140588144540688 -> 140584557418064
	140588144540688 [label=NativeBatchNormBackward]
	140584557418128 -> 140588144540688
	140584557418128 [label=MkldnnConvolutionBackward]
	140588144565328 -> 140584557418128
	140584557418576 -> 140584557418128
	140587304961280 [label="stage4.0.fuse_layers.1.2.0.weight
 (96, 192, 1, 1)" fillcolor=lightblue]
	140587304961280 -> 140584557418576
	140584557418576 [label=AccumulateGrad]
	140584557418704 -> 140588144540688
	140587304961520 [label="stage4.0.fuse_layers.1.2.1.weight
 (96)" fillcolor=lightblue]
	140587304961520 -> 140584557418704
	140584557418704 [label=AccumulateGrad]
	140584557418768 -> 140588144540688
	140587304961600 [label="stage4.0.fuse_layers.1.2.1.bias
 (96)" fillcolor=lightblue]
	140587304961600 -> 140584557418768
	140584557418768 [label=AccumulateGrad]
	140584557417872 -> 140584557417488
	140584557417872 [label=UpsampleBilinear2DBackward1]
	140584557418000 -> 140584557417872
	140584557418000 [label=NativeBatchNormBackward]
	140584557418256 -> 140584557418000
	140584557418256 [label=MkldnnConvolutionBackward]
	140588144597584 -> 140584557418256
	140584557419024 -> 140584557418256
	140587304962080 [label="stage4.0.fuse_layers.1.3.0.weight
 (96, 384, 1, 1)" fillcolor=lightblue]
	140587304962080 -> 140584557419024
	140584557419024 [label=AccumulateGrad]
	140584557419088 -> 140584557418000
	140587304962320 [label="stage4.0.fuse_layers.1.3.1.weight
 (96)" fillcolor=lightblue]
	140587304962320 -> 140584557419088
	140584557419088 [label=AccumulateGrad]
	140584557418960 -> 140584557418000
	140587304962400 [label="stage4.0.fuse_layers.1.3.1.bias
 (96)" fillcolor=lightblue]
	140587304962400 -> 140584557418960
	140584557418960 [label=AccumulateGrad]
	140584557417360 -> 140584557417168
	140587637476080 [label="stage4.1.branches.1.0.conv1.weight
 (96, 96, 3, 3)" fillcolor=lightblue]
	140587637476080 -> 140584557417360
	140584557417360 [label=AccumulateGrad]
	140584557417296 -> 140584557416848
	140587637476320 [label="stage4.1.branches.1.0.bn1.weight
 (96)" fillcolor=lightblue]
	140587637476320 -> 140584557417296
	140584557417296 [label=AccumulateGrad]
	140584557417040 -> 140584557416848
	140587637476400 [label="stage4.1.branches.1.0.bn1.bias
 (96)" fillcolor=lightblue]
	140587637476400 -> 140584557417040
	140584557417040 [label=AccumulateGrad]
	140584557416976 -> 140584557416592
	140587637476800 [label="stage4.1.branches.1.0.conv2.weight
 (96, 96, 3, 3)" fillcolor=lightblue]
	140587637476800 -> 140584557416976
	140584557416976 [label=AccumulateGrad]
	140584557416720 -> 140584557379472
	140587637477040 [label="stage4.1.branches.1.0.bn2.weight
 (96)" fillcolor=lightblue]
	140587637477040 -> 140584557416720
	140584557416720 [label=AccumulateGrad]
	140584557416656 -> 140584557379472
	140587637477120 [label="stage4.1.branches.1.0.bn2.bias
 (96)" fillcolor=lightblue]
	140587637477120 -> 140584557416656
	140584557416656 [label=AccumulateGrad]
	140584557378960 -> 140584557379280
	140584557379024 -> 140584557378512
	140587637534960 [label="stage4.1.branches.1.1.conv1.weight
 (96, 96, 3, 3)" fillcolor=lightblue]
	140587637534960 -> 140584557379024
	140584557379024 [label=AccumulateGrad]
	140584557379152 -> 140584557379216
	140587637535200 [label="stage4.1.branches.1.1.bn1.weight
 (96)" fillcolor=lightblue]
	140587637535200 -> 140584557379152
	140584557379152 [label=AccumulateGrad]
	140584557378704 -> 140584557379216
	140587637535280 [label="stage4.1.branches.1.1.bn1.bias
 (96)" fillcolor=lightblue]
	140587637535280 -> 140584557378704
	140584557378704 [label=AccumulateGrad]
	140584557377872 -> 140584557376144
	140587637535680 [label="stage4.1.branches.1.1.conv2.weight
 (96, 96, 3, 3)" fillcolor=lightblue]
	140587637535680 -> 140584557377872
	140584557377872 [label=AccumulateGrad]
	140584557377232 -> 140584557378064
	140587637535920 [label="stage4.1.branches.1.1.bn2.weight
 (96)" fillcolor=lightblue]
	140587637535920 -> 140584557377232
	140584557377232 [label=AccumulateGrad]
	140584557377296 -> 140584557378064
	140587637536000 [label="stage4.1.branches.1.1.bn2.bias
 (96)" fillcolor=lightblue]
	140587637536000 -> 140584557377296
	140584557377296 [label=AccumulateGrad]
	140584557377424 -> 140584557376016
	140584557375632 -> 140584557354704
	140587637536400 [label="stage4.1.branches.1.2.conv1.weight
 (96, 96, 3, 3)" fillcolor=lightblue]
	140587637536400 -> 140584557375632
	140584557375632 [label=AccumulateGrad]
	140584557354448 -> 140584557353552
	140587637536640 [label="stage4.1.branches.1.2.bn1.weight
 (96)" fillcolor=lightblue]
	140587637536640 -> 140584557354448
	140584557354448 [label=AccumulateGrad]
	140584557376208 -> 140584557353552
	140587637536720 [label="stage4.1.branches.1.2.bn1.bias
 (96)" fillcolor=lightblue]
	140587637536720 -> 140584557376208
	140584557376208 [label=AccumulateGrad]
	140584557353040 -> 140584538777616
	140587637537120 [label="stage4.1.branches.1.2.conv2.weight
 (96, 96, 3, 3)" fillcolor=lightblue]
	140587637537120 -> 140584557353040
	140584557353040 [label=AccumulateGrad]
	140584538776144 -> 140588144556880
	140587637537360 [label="stage4.1.branches.1.2.bn2.weight
 (96)" fillcolor=lightblue]
	140587637537360 -> 140584538776144
	140584538776144 [label=AccumulateGrad]
	140584557351568 -> 140588144556880
	140587637537440 [label="stage4.1.branches.1.2.bn2.bias
 (96)" fillcolor=lightblue]
	140587637537440 -> 140584557351568
	140584557351568 [label=AccumulateGrad]
	140584538775696 -> 140588144563536
	140588144557200 -> 140588144557584
	140587637537840 [label="stage4.1.branches.1.3.conv1.weight
 (96, 96, 3, 3)" fillcolor=lightblue]
	140587637537840 -> 140588144557200
	140588144557200 [label=AccumulateGrad]
	140588144557008 -> 140588144555600
	140587637538080 [label="stage4.1.branches.1.3.bn1.weight
 (96)" fillcolor=lightblue]
	140587637538080 -> 140588144557008
	140588144557008 [label=AccumulateGrad]
	140588144556048 -> 140588144555600
	140587637538160 [label="stage4.1.branches.1.3.bn1.bias
 (96)" fillcolor=lightblue]
	140587637538160 -> 140588144556048
	140588144556048 [label=AccumulateGrad]
	140588144555472 -> 140588144555088
	140587637538560 [label="stage4.1.branches.1.3.conv2.weight
 (96, 96, 3, 3)" fillcolor=lightblue]
	140587637538560 -> 140588144555472
	140588144555472 [label=AccumulateGrad]
	140588144556496 -> 140588144554384
	140587637596240 [label="stage4.1.branches.1.3.bn2.weight
 (96)" fillcolor=lightblue]
	140587637596240 -> 140588144556496
	140588144556496 [label=AccumulateGrad]
	140588144554768 -> 140588144554384
	140587637596320 [label="stage4.1.branches.1.3.bn2.bias
 (96)" fillcolor=lightblue]
	140587637596320 -> 140588144554768
	140588144554768 [label=AccumulateGrad]
	140588144554640 -> 140588144540368
	140588144541328 -> 140587840180688
	140587069193344 [label="stage4.1.fuse_layers.0.1.0.weight
 (48, 96, 1, 1)" fillcolor=lightblue]
	140587069193344 -> 140588144541328
	140588144541328 [label=AccumulateGrad]
	140588144541008 -> 140587720590544
	140587069193584 [label="stage4.1.fuse_layers.0.1.1.weight
 (48)" fillcolor=lightblue]
	140587069193584 -> 140588144541008
	140588144541008 [label=AccumulateGrad]
	140588144540560 -> 140587720590544
	140587069193664 [label="stage4.1.fuse_layers.0.1.1.bias
 (48)" fillcolor=lightblue]
	140587069193664 -> 140588144540560
	140588144540560 [label=AccumulateGrad]
	140587037651536 -> 140587038820048
	140587037651536 [label=UpsampleBilinear2DBackward1]
	140587842056400 -> 140587037651536
	140587842056400 [label=NativeBatchNormBackward]
	140588144539984 -> 140587842056400
	140588144539984 [label=MkldnnConvolutionBackward]
	140584557351248 -> 140588144539984
	140584557351248 [label=ReluBackward1]
	140588144554128 -> 140584557351248
	140588144554128 [label=AddBackward0]
	140588144556368 -> 140588144554128
	140588144556368 [label=NativeBatchNormBackward]
	140588144556176 -> 140588144556368
	140588144556176 [label=MkldnnConvolutionBackward]
	140584557376912 -> 140588144556176
	140584557376912 [label=ReluBackward1]
	140584557376592 -> 140584557376912
	140584557376592 [label=NativeBatchNormBackward]
	140584557379088 -> 140584557376592
	140584557379088 [label=MkldnnConvolutionBackward]
	140588144557904 -> 140584557379088
	140588144557904 [label=ReluBackward1]
	140584557417104 -> 140588144557904
	140584557417104 [label=AddBackward0]
	140584557417232 -> 140584557417104
	140584557417232 [label=NativeBatchNormBackward]
	140584557416528 -> 140584557417232
	140584557416528 [label=MkldnnConvolutionBackward]
	140584557419344 -> 140584557416528
	140584557419344 [label=ReluBackward1]
	140584557419280 -> 140584557419344
	140584557419280 [label=NativeBatchNormBackward]
	140584557419536 -> 140584557419280
	140584557419536 [label=MkldnnConvolutionBackward]
	140584557417744 -> 140584557419536
	140584557417744 [label=ReluBackward1]
	140584557419856 -> 140584557417744
	140584557419856 [label=AddBackward0]
	140584557420048 -> 140584557419856
	140584557420048 [label=NativeBatchNormBackward]
	140584557419792 -> 140584557420048
	140584557419792 [label=MkldnnConvolutionBackward]
	140584557420432 -> 140584557419792
	140584557420432 [label=ReluBackward1]
	140584557449296 -> 140584557420432
	140584557449296 [label=NativeBatchNormBackward]
	140584557449552 -> 140584557449296
	140584557449552 [label=MkldnnConvolutionBackward]
	140584557420240 -> 140584557449552
	140584557420240 [label=ReluBackward1]
	140584557449872 -> 140584557420240
	140584557449872 [label=AddBackward0]
	140584557450064 -> 140584557449872
	140584557450064 [label=NativeBatchNormBackward]
	140584557449808 -> 140584557450064
	140584557449808 [label=MkldnnConvolutionBackward]
	140584557450448 -> 140584557449808
	140584557450448 [label=ReluBackward1]
	140584557450512 -> 140584557450448
	140584557450512 [label=NativeBatchNormBackward]
	140584557450832 -> 140584557450512
	140584557450832 [label=MkldnnConvolutionBackward]
	140584557450256 -> 140584557450832
	140584557450256 [label=ReluBackward1]
	140584557451152 -> 140584557450256
	140584557451152 [label=AddBackward0]
	140584557451344 -> 140584557451152
	140584557451344 [label=AddBackward0]
	140584557451088 -> 140584557451344
	140584557451088 [label=AddBackward0]
	140584557451664 -> 140584557451088
	140584557451664 [label=NativeBatchNormBackward]
	140584557451280 -> 140584557451664
	140584557451280 [label=MkldnnConvolutionBackward]
	140584557452048 -> 140584557451280
	140584557452048 [label=ReluBackward1]
	140584557452112 -> 140584557452048
	140584557452112 [label=NativeBatchNormBackward]
	140584557452432 -> 140584557452112
	140584557452432 [label=MkldnnConvolutionBackward]
	140588144562320 -> 140584557452432
	140584557452624 -> 140584557452432
	140587304963040 [label="stage4.0.fuse_layers.2.0.0.0.weight
 (48, 48, 3, 3)" fillcolor=lightblue]
	140587304963040 -> 140584557452624
	140584557452624 [label=AccumulateGrad]
	140584557452560 -> 140584557452112
	140587304963280 [label="stage4.0.fuse_layers.2.0.0.1.weight
 (48)" fillcolor=lightblue]
	140587304963280 -> 140584557452560
	140584557452560 [label=AccumulateGrad]
	140584557452304 -> 140584557452112
	140587304963360 [label="stage4.0.fuse_layers.2.0.0.1.bias
 (48)" fillcolor=lightblue]
	140587304963360 -> 140584557452304
	140584557452304 [label=AccumulateGrad]
	140584557452240 -> 140584557451280
	140587304963920 [label="stage4.0.fuse_layers.2.0.1.0.weight
 (192, 48, 3, 3)" fillcolor=lightblue]
	140587304963920 -> 140584557452240
	140584557452240 [label=AccumulateGrad]
	140584557451984 -> 140584557451664
	140587305017504 [label="stage4.0.fuse_layers.2.0.1.1.weight
 (192)" fillcolor=lightblue]
	140587305017504 -> 140584557451984
	140584557451984 [label=AccumulateGrad]
	140584557451920 -> 140584557451664
	140587305017584 [label="stage4.0.fuse_layers.2.0.1.1.bias
 (192)" fillcolor=lightblue]
	140587305017584 -> 140584557451920
	140584557451920 [label=AccumulateGrad]
	140584557451856 -> 140584557451088
	140584557451856 [label=NativeBatchNormBackward]
	140584557353168 -> 140584557451856
	140584557353168 [label=MkldnnConvolutionBackward]
	140588144563920 -> 140584557353168
	140584557452880 -> 140584557353168
	140587305018224 [label="stage4.0.fuse_layers.2.1.0.0.weight
 (192, 96, 3, 3)" fillcolor=lightblue]
	140587305018224 -> 140584557452880
	140584557452880 [label=AccumulateGrad]
	140584557453008 -> 140584557451856
	140587305018464 [label="stage4.0.fuse_layers.2.1.0.1.weight
 (192)" fillcolor=lightblue]
	140587305018464 -> 140584557453008
	140584557453008 [label=AccumulateGrad]
	140584557452368 -> 140584557451856
	140587305018544 [label="stage4.0.fuse_layers.2.1.0.1.bias
 (192)" fillcolor=lightblue]
	140587305018544 -> 140584557452368
	140584557452368 [label=AccumulateGrad]
	140588144565328 -> 140584557451344
	140584557451536 -> 140584557451152
	140584557451536 [label=UpsampleBilinear2DBackward1]
	140584557451728 -> 140584557451536
	140584557451728 [label=NativeBatchNormBackward]
	140584557451600 -> 140584557451728
	140584557451600 [label=MkldnnConvolutionBackward]
	140588144597584 -> 140584557451600
	140584557452752 -> 140584557451600
	140587305019184 [label="stage4.0.fuse_layers.2.3.0.weight
 (192, 384, 1, 1)" fillcolor=lightblue]
	140587305019184 -> 140584557452752
	140584557452752 [label=AccumulateGrad]
	140584557453072 -> 140584557451728
	140587305019424 [label="stage4.0.fuse_layers.2.3.1.weight
 (192)" fillcolor=lightblue]
	140587305019424 -> 140584557453072
	140584557453072 [label=AccumulateGrad]
	140584557452816 -> 140584557451728
	140587305019504 [label="stage4.0.fuse_layers.2.3.1.bias
 (192)" fillcolor=lightblue]
	140587305019504 -> 140584557452816
	140584557452816 [label=AccumulateGrad]
	140584557451024 -> 140584557450832
	140587637596880 [label="stage4.1.branches.2.0.conv1.weight
 (192, 192, 3, 3)" fillcolor=lightblue]
	140587637596880 -> 140584557451024
	140584557451024 [label=AccumulateGrad]
	140584557450960 -> 140584557450512
	140587637597120 [label="stage4.1.branches.2.0.bn1.weight
 (192)" fillcolor=lightblue]
	140587637597120 -> 140584557450960
	140584557450960 [label=AccumulateGrad]
	140584557450704 -> 140584557450512
	140587637597200 [label="stage4.1.branches.2.0.bn1.bias
 (192)" fillcolor=lightblue]
	140587637597200 -> 140584557450704
	140584557450704 [label=AccumulateGrad]
	140584557450640 -> 140584557449808
	140587637597600 [label="stage4.1.branches.2.0.conv2.weight
 (192, 192, 3, 3)" fillcolor=lightblue]
	140587637597600 -> 140584557450640
	140584557450640 [label=AccumulateGrad]
	140584557450384 -> 140584557450064
	140587637597840 [label="stage4.1.branches.2.0.bn2.weight
 (192)" fillcolor=lightblue]
	140587637597840 -> 140584557450384
	140584557450384 [label=AccumulateGrad]
	140584557450320 -> 140584557450064
	140587637597920 [label="stage4.1.branches.2.0.bn2.bias
 (192)" fillcolor=lightblue]
	140587637597920 -> 140584557450320
	140584557450320 [label=AccumulateGrad]
	140584557450256 -> 140584557449872
	140584557449744 -> 140584557449552
	140587637598320 [label="stage4.1.branches.2.1.conv1.weight
 (192, 192, 3, 3)" fillcolor=lightblue]
	140587637598320 -> 140584557449744
	140584557449744 [label=AccumulateGrad]
	140584557449680 -> 140584557449296
	140587637598560 [label="stage4.1.branches.2.1.bn1.weight
 (192)" fillcolor=lightblue]
	140587637598560 -> 140584557449680
	140584557449680 [label=AccumulateGrad]
	140584557449424 -> 140584557449296
	140587637598640 [label="stage4.1.branches.2.1.bn1.bias
 (192)" fillcolor=lightblue]
	140587637598640 -> 140584557449424
	140584557449424 [label=AccumulateGrad]
	140584557420496 -> 140584557419792
	140587637599040 [label="stage4.1.branches.2.1.conv2.weight
 (192, 192, 3, 3)" fillcolor=lightblue]
	140587637599040 -> 140584557420496
	140584557420496 [label=AccumulateGrad]
	140584557420368 -> 140584557420048
	140587637599280 [label="stage4.1.branches.2.1.bn2.weight
 (192)" fillcolor=lightblue]
	140587637599280 -> 140584557420368
	140584557420368 [label=AccumulateGrad]
	140584557420304 -> 140584557420048
	140587637599360 [label="stage4.1.branches.2.1.bn2.bias
 (192)" fillcolor=lightblue]
	140587637599360 -> 140584557420304
	140584557420304 [label=AccumulateGrad]
	140584557420240 -> 140584557419856
	140584557419728 -> 140584557419536
	140587637599760 [label="stage4.1.branches.2.2.conv1.weight
 (192, 192, 3, 3)" fillcolor=lightblue]
	140587637599760 -> 140584557419728
	140584557419728 [label=AccumulateGrad]
	140584557419664 -> 140584557419280
	140587637600000 [label="stage4.1.branches.2.2.bn1.weight
 (192)" fillcolor=lightblue]
	140587637600000 -> 140584557419664
	140584557419664 [label=AccumulateGrad]
	140584557419408 -> 140584557419280
	140587637600080 [label="stage4.1.branches.2.2.bn1.bias
 (192)" fillcolor=lightblue]
	140587637600080 -> 140584557419408
	140584557419408 [label=AccumulateGrad]
	140584557418512 -> 140584557416528
	140587069096256 [label="stage4.1.branches.2.2.conv2.weight
 (192, 192, 3, 3)" fillcolor=lightblue]
	140587069096256 -> 140584557418512
	140584557418512 [label=AccumulateGrad]
	140584557419216 -> 140584557417232
	140587069096496 [label="stage4.1.branches.2.2.bn2.weight
 (192)" fillcolor=lightblue]
	140587069096496 -> 140584557419216
	140584557419216 [label=AccumulateGrad]
	140584557418832 -> 140584557417232
	140587069096576 [label="stage4.1.branches.2.2.bn2.bias
 (192)" fillcolor=lightblue]
	140587069096576 -> 140584557418832
	140584557418832 [label=AccumulateGrad]
	140584557417744 -> 140584557417104
	140584557416912 -> 140584557379088
	140587069096976 [label="stage4.1.branches.2.3.conv1.weight
 (192, 192, 3, 3)" fillcolor=lightblue]
	140587069096976 -> 140584557416912
	140584557416912 [label=AccumulateGrad]
	140584557379408 -> 140584557376592
	140587069097216 [label="stage4.1.branches.2.3.bn1.weight
 (192)" fillcolor=lightblue]
	140587069097216 -> 140584557379408
	140584557379408 [label=AccumulateGrad]
	140584557379536 -> 140584557376592
	140587069097296 [label="stage4.1.branches.2.3.bn1.bias
 (192)" fillcolor=lightblue]
	140587069097296 -> 140584557379536
	140584557379536 [label=AccumulateGrad]
	140584557377488 -> 140588144556176
	140587069097696 [label="stage4.1.branches.2.3.conv2.weight
 (192, 192, 3, 3)" fillcolor=lightblue]
	140587069097696 -> 140584557377488
	140584557377488 [label=AccumulateGrad]
	140584557379344 -> 140588144556368
	140587069097936 [label="stage4.1.branches.2.3.bn2.weight
 (192)" fillcolor=lightblue]
	140587069097936 -> 140584557379344
	140584557379344 [label=AccumulateGrad]
	140584557375952 -> 140588144556368
	140587069098016 [label="stage4.1.branches.2.3.bn2.bias
 (192)" fillcolor=lightblue]
	140587069098016 -> 140584557375952
	140584557375952 [label=AccumulateGrad]
	140588144557904 -> 140588144554128
	140584538763600 -> 140588144539984
	140587069235600 [label="stage4.1.fuse_layers.0.2.0.weight
 (48, 192, 1, 1)" fillcolor=lightblue]
	140587069235600 -> 140584538763600
	140584538763600 [label=AccumulateGrad]
	140584538778448 -> 140587842056400
	140587069235840 [label="stage4.1.fuse_layers.0.2.1.weight
 (48)" fillcolor=lightblue]
	140587069235840 -> 140584538778448
	140584538778448 [label=AccumulateGrad]
	140584538705424 -> 140587842056400
	140587069235920 [label="stage4.1.fuse_layers.0.2.1.bias
 (48)" fillcolor=lightblue]
	140587069235920 -> 140584538705424
	140584538705424 [label=AccumulateGrad]
	140587038865232 -> 140587304754960
	140587038865232 [label=UpsampleBilinear2DBackward1]
	140587588026832 -> 140587038865232
	140587588026832 [label=NativeBatchNormBackward]
	140588144541200 -> 140587588026832
	140588144541200 [label=MkldnnConvolutionBackward]
	140584557376784 -> 140588144541200
	140584557376784 [label=ReluBackward1]
	140584557417552 -> 140584557376784
	140584557417552 [label=AddBackward0]
	140584557418640 -> 140584557417552
	140584557418640 [label=NativeBatchNormBackward]
	140584557419920 -> 140584557418640
	140584557419920 [label=MkldnnConvolutionBackward]
	140584557419600 -> 140584557419920
	140584557419600 [label=ReluBackward1]
	140584557449936 -> 140584557419600
	140584557449936 [label=NativeBatchNormBackward]
	140584557450000 -> 140584557449936
	140584557450000 [label=MkldnnConvolutionBackward]
	140584557419472 -> 140584557450000
	140584557419472 [label=ReluBackward1]
	140584557450768 -> 140584557419472
	140584557450768 [label=AddBackward0]
	140584557450896 -> 140584557450768
	140584557450896 [label=NativeBatchNormBackward]
	140584557450128 -> 140584557450896
	140584557450128 [label=MkldnnConvolutionBackward]
	140584557453264 -> 140584557450128
	140584557453264 [label=ReluBackward1]
	140584557477968 -> 140584557453264
	140584557477968 [label=NativeBatchNormBackward]
	140584557478160 -> 140584557477968
	140584557478160 [label=MkldnnConvolutionBackward]
	140584557451408 -> 140584557478160
	140584557451408 [label=ReluBackward1]
	140584557478480 -> 140584557451408
	140584557478480 [label=AddBackward0]
	140584557478672 -> 140584557478480
	140584557478672 [label=NativeBatchNormBackward]
	140584557478416 -> 140584557478672
	140584557478416 [label=MkldnnConvolutionBackward]
	140584557479056 -> 140584557478416
	140584557479056 [label=ReluBackward1]
	140584557479120 -> 140584557479056
	140584557479120 [label=NativeBatchNormBackward]
	140584557479440 -> 140584557479120
	140584557479440 [label=MkldnnConvolutionBackward]
	140584557478864 -> 140584557479440
	140584557478864 [label=ReluBackward1]
	140584557479760 -> 140584557478864
	140584557479760 [label=AddBackward0]
	140584557479952 -> 140584557479760
	140584557479952 [label=NativeBatchNormBackward]
	140584557479696 -> 140584557479952
	140584557479696 [label=MkldnnConvolutionBackward]
	140584557480336 -> 140584557479696
	140584557480336 [label=ReluBackward1]
	140584557480400 -> 140584557480336
	140584557480400 [label=NativeBatchNormBackward]
	140584557480720 -> 140584557480400
	140584557480720 [label=MkldnnConvolutionBackward]
	140584557480144 -> 140584557480720
	140584557480144 [label=ReluBackward1]
	140584557481040 -> 140584557480144
	140584557481040 [label=AddBackward0]
	140584557481232 -> 140584557481040
	140584557481232 [label=AddBackward0]
	140584557481360 -> 140584557481232
	140584557481360 [label=AddBackward0]
	140584557481168 -> 140584557481360
	140584557481168 [label=NativeBatchNormBackward]
	140584557481616 -> 140584557481168
	140584557481616 [label=MkldnnConvolutionBackward]
	140584557481936 -> 140584557481616
	140584557481936 [label=ReluBackward1]
	140584557486160 -> 140584557481936
	140584557486160 [label=NativeBatchNormBackward]
	140584557486480 -> 140584557486160
	140584557486480 [label=MkldnnConvolutionBackward]
	140584557486672 -> 140584557486480
	140584557486672 [label=ReluBackward1]
	140584557486736 -> 140584557486672
	140584557486736 [label=NativeBatchNormBackward]
	140584557487056 -> 140584557486736
	140584557487056 [label=MkldnnConvolutionBackward]
	140588144562320 -> 140584557487056
	140584557487248 -> 140584557487056
	140587305020144 [label="stage4.0.fuse_layers.3.0.0.0.weight
 (48, 48, 3, 3)" fillcolor=lightblue]
	140587305020144 -> 140584557487248
	140584557487248 [label=AccumulateGrad]
	140584557487184 -> 140584557486736
	140587305020384 [label="stage4.0.fuse_layers.3.0.0.1.weight
 (48)" fillcolor=lightblue]
	140587305020384 -> 140584557487184
	140584557487184 [label=AccumulateGrad]
	140584557486928 -> 140584557486736
	140587305020464 [label="stage4.0.fuse_layers.3.0.0.1.bias
 (48)" fillcolor=lightblue]
	140587305020464 -> 140584557486928
	140584557486928 [label=AccumulateGrad]
	140584557486864 -> 140584557486480
	140587305021024 [label="stage4.0.fuse_layers.3.0.1.0.weight
 (48, 48, 3, 3)" fillcolor=lightblue]
	140587305021024 -> 140584557486864
	140584557486864 [label=AccumulateGrad]
	140584557486608 -> 140584557486160
	140587305021264 [label="stage4.0.fuse_layers.3.0.1.1.weight
 (48)" fillcolor=lightblue]
	140587305021264 -> 140584557486608
	140584557486608 [label=AccumulateGrad]
	140584557486352 -> 140584557486160
	140587305021344 [label="stage4.0.fuse_layers.3.0.1.1.bias
 (48)" fillcolor=lightblue]
	140587305021344 -> 140584557486352
	140584557486352 [label=AccumulateGrad]
	140584557486288 -> 140584557481616
	140587305079344 [label="stage4.0.fuse_layers.3.0.2.0.weight
 (384, 48, 3, 3)" fillcolor=lightblue]
	140587305079344 -> 140584557486288
	140584557486288 [label=AccumulateGrad]
	140584557481872 -> 140584557481168
	140587305079584 [label="stage4.0.fuse_layers.3.0.2.1.weight
 (384)" fillcolor=lightblue]
	140587305079584 -> 140584557481872
	140584557481872 [label=AccumulateGrad]
	140584557481808 -> 140584557481168
	140587305079664 [label="stage4.0.fuse_layers.3.0.2.1.bias
 (384)" fillcolor=lightblue]
	140587305079664 -> 140584557481808
	140584557481808 [label=AccumulateGrad]
	140584557481744 -> 140584557481360
	140584557481744 [label=NativeBatchNormBackward]
	140584557480976 -> 140584557481744
	140584557480976 [label=MkldnnConvolutionBackward]
	140584557486992 -> 140584557480976
	140584557486992 [label=ReluBackward1]
	140584557486224 -> 140584557486992
	140584557486224 [label=NativeBatchNormBackward]
	140584557487568 -> 140584557486224
	140584557487568 [label=MkldnnConvolutionBackward]
	140588144563920 -> 140584557487568
	140584557487696 -> 140584557487568
	140587305080304 [label="stage4.0.fuse_layers.3.1.0.0.weight
 (96, 96, 3, 3)" fillcolor=lightblue]
	140587305080304 -> 140584557487696
	140584557487696 [label=AccumulateGrad]
	140584557487376 -> 140584557486224
	140587305080544 [label="stage4.0.fuse_layers.3.1.0.1.weight
 (96)" fillcolor=lightblue]
	140587305080544 -> 140584557487376
	140584557487376 [label=AccumulateGrad]
	140584557486800 -> 140584557486224
	140587305080624 [label="stage4.0.fuse_layers.3.1.0.1.bias
 (96)" fillcolor=lightblue]
	140587305080624 -> 140584557486800
	140584557486800 [label=AccumulateGrad]
	140584557487504 -> 140584557480976
	140587305081184 [label="stage4.0.fuse_layers.3.1.1.0.weight
 (384, 96, 3, 3)" fillcolor=lightblue]
	140587305081184 -> 140584557487504
	140584557487504 [label=AccumulateGrad]
	140584557378576 -> 140584557481744
	140587305081424 [label="stage4.0.fuse_layers.3.1.1.1.weight
 (384)" fillcolor=lightblue]
	140587305081424 -> 140584557378576
	140584557378576 [label=AccumulateGrad]
	140584557487632 -> 140584557481744
	140587305081504 [label="stage4.0.fuse_layers.3.1.1.1.bias
 (384)" fillcolor=lightblue]
	140587305081504 -> 140584557487632
	140584557487632 [label=AccumulateGrad]
	140584557481552 -> 140584557481232
	140584557481552 [label=NativeBatchNormBackward]
	140584557481680 -> 140584557481552
	140584557481680 [label=MkldnnConvolutionBackward]
	140588144565328 -> 140584557481680
	140584557488080 -> 140584557481680
	140587305082144 [label="stage4.0.fuse_layers.3.2.0.0.weight
 (384, 192, 3, 3)" fillcolor=lightblue]
	140587305082144 -> 140584557488080
	140584557488080 [label=AccumulateGrad]
	140584557487888 -> 140584557481552
	140587305082384 [label="stage4.0.fuse_layers.3.2.0.1.weight
 (384)" fillcolor=lightblue]
	140587305082384 -> 140584557487888
	140584557487888 [label=AccumulateGrad]
	140584557487120 -> 140584557481552
	140587305082464 [label="stage4.0.fuse_layers.3.2.0.1.bias
 (384)" fillcolor=lightblue]
	140587305082464 -> 140584557487120
	140584557487120 [label=AccumulateGrad]
	140588144597584 -> 140584557481040
	140584557480912 -> 140584557480720
	140587069098576 [label="stage4.1.branches.3.0.conv1.weight
 (384, 384, 3, 3)" fillcolor=lightblue]
	140587069098576 -> 140584557480912
	140584557480912 [label=AccumulateGrad]
	140584557480848 -> 140584557480400
	140587069098816 [label="stage4.1.branches.3.0.bn1.weight
 (384)" fillcolor=lightblue]
	140587069098816 -> 140584557480848
	140584557480848 [label=AccumulateGrad]
	140584557480592 -> 140584557480400
	140587069098896 [label="stage4.1.branches.3.0.bn1.bias
 (384)" fillcolor=lightblue]
	140587069098896 -> 140584557480592
	140584557480592 [label=AccumulateGrad]
	140584557480528 -> 140584557479696
	140587069099296 [label="stage4.1.branches.3.0.conv2.weight
 (384, 384, 3, 3)" fillcolor=lightblue]
	140587069099296 -> 140584557480528
	140584557480528 [label=AccumulateGrad]
	140584557480272 -> 140584557479952
	140587069099536 [label="stage4.1.branches.3.0.bn2.weight
 (384)" fillcolor=lightblue]
	140587069099536 -> 140584557480272
	140584557480272 [label=AccumulateGrad]
	140584557480208 -> 140584557479952
	140587069099616 [label="stage4.1.branches.3.0.bn2.bias
 (384)" fillcolor=lightblue]
	140587069099616 -> 140584557480208
	140584557480208 [label=AccumulateGrad]
	140584557480144 -> 140584557479760
	140584557479632 -> 140584557479440
	140587069153600 [label="stage4.1.branches.3.1.conv1.weight
 (384, 384, 3, 3)" fillcolor=lightblue]
	140587069153600 -> 140584557479632
	140584557479632 [label=AccumulateGrad]
	140584557479568 -> 140584557479120
	140587069153840 [label="stage4.1.branches.3.1.bn1.weight
 (384)" fillcolor=lightblue]
	140587069153840 -> 140584557479568
	140584557479568 [label=AccumulateGrad]
	140584557479312 -> 140584557479120
	140587069153920 [label="stage4.1.branches.3.1.bn1.bias
 (384)" fillcolor=lightblue]
	140587069153920 -> 140584557479312
	140584557479312 [label=AccumulateGrad]
	140584557479248 -> 140584557478416
	140587069154720 [label="stage4.1.branches.3.1.conv2.weight
 (384, 384, 3, 3)" fillcolor=lightblue]
	140587069154720 -> 140584557479248
	140584557479248 [label=AccumulateGrad]
	140584557478992 -> 140584557478672
	140587069154960 [label="stage4.1.branches.3.1.bn2.weight
 (384)" fillcolor=lightblue]
	140587069154960 -> 140584557478992
	140584557478992 [label=AccumulateGrad]
	140584557478928 -> 140584557478672
	140587069155040 [label="stage4.1.branches.3.1.bn2.bias
 (384)" fillcolor=lightblue]
	140587069155040 -> 140584557478928
	140584557478928 [label=AccumulateGrad]
	140584557478864 -> 140584557478480
	140584557478352 -> 140584557478160
	140587069155840 [label="stage4.1.branches.3.2.conv1.weight
 (384, 384, 3, 3)" fillcolor=lightblue]
	140587069155840 -> 140584557478352
	140584557478352 [label=AccumulateGrad]
	140584557478288 -> 140584557477968
	140587069156080 [label="stage4.1.branches.3.2.bn1.weight
 (384)" fillcolor=lightblue]
	140587069156080 -> 140584557478288
	140584557478288 [label=AccumulateGrad]
	140584557478032 -> 140584557477968
	140587069156160 [label="stage4.1.branches.3.2.bn1.bias
 (384)" fillcolor=lightblue]
	140587069156160 -> 140584557478032
	140584557478032 [label=AccumulateGrad]
	140584557452496 -> 140584557450128
	140587069156960 [label="stage4.1.branches.3.2.conv2.weight
 (384, 384, 3, 3)" fillcolor=lightblue]
	140587069156960 -> 140584557452496
	140584557452496 [label=AccumulateGrad]
	140584557453200 -> 140584557450896
	140587069157200 [label="stage4.1.branches.3.2.bn2.weight
 (384)" fillcolor=lightblue]
	140587069157200 -> 140584557453200
	140584557453200 [label=AccumulateGrad]
	140584557452176 -> 140584557450896
	140587069157280 [label="stage4.1.branches.3.2.bn2.bias
 (384)" fillcolor=lightblue]
	140587069157280 -> 140584557452176
	140584557452176 [label=AccumulateGrad]
	140584557451408 -> 140584557450768
	140584557450576 -> 140584557450000
	140587069190944 [label="stage4.1.branches.3.3.conv1.weight
 (384, 384, 3, 3)" fillcolor=lightblue]
	140587069190944 -> 140584557450576
	140584557450576 [label=AccumulateGrad]
	140584557450192 -> 140584557449936
	140587069191184 [label="stage4.1.branches.3.3.bn1.weight
 (384)" fillcolor=lightblue]
	140587069191184 -> 140584557450192
	140584557450192 [label=AccumulateGrad]
	140584557449488 -> 140584557449936
	140587069191264 [label="stage4.1.branches.3.3.bn1.bias
 (384)" fillcolor=lightblue]
	140587069191264 -> 140584557449488
	140584557449488 [label=AccumulateGrad]
	140584557420176 -> 140584557419920
	140587069192064 [label="stage4.1.branches.3.3.conv2.weight
 (384, 384, 3, 3)" fillcolor=lightblue]
	140587069192064 -> 140584557420176
	140584557420176 [label=AccumulateGrad]
	140584557419984 -> 140584557418640
	140587069192304 [label="stage4.1.branches.3.3.bn2.weight
 (384)" fillcolor=lightblue]
	140587069192304 -> 140584557419984
	140584557419984 [label=AccumulateGrad]
	140584557420112 -> 140584557418640
	140587069192384 [label="stage4.1.branches.3.3.bn2.bias
 (384)" fillcolor=lightblue]
	140587069192384 -> 140584557420112
	140584557420112 [label=AccumulateGrad]
	140584557419472 -> 140584557417552
	140584557378896 -> 140588144541200
	140587069236800 [label="stage4.1.fuse_layers.0.3.0.weight
 (48, 384, 1, 1)" fillcolor=lightblue]
	140587069236800 -> 140584557378896
	140584557378896 [label=AccumulateGrad]
	140588144557776 -> 140587588026832
	140587069237040 [label="stage4.1.fuse_layers.0.3.1.weight
 (48)" fillcolor=lightblue]
	140587069237040 -> 140588144557776
	140588144557776 [label=AccumulateGrad]
	140588144554192 -> 140587588026832
	140587069237120 [label="stage4.1.fuse_layers.0.3.1.bias
 (48)" fillcolor=lightblue]
	140587069237120 -> 140588144554192
	140588144554192 [label=AccumulateGrad]
	140587304689104 -> 140587039160208
	140587305878224 [label="stage4.2.branches.0.0.conv1.weight
 (48, 48, 3, 3)" fillcolor=lightblue]
	140587305878224 -> 140587304689104
	140587304689104 [label=AccumulateGrad]
	140587589188752 -> 140587841931024
	140587305878464 [label="stage4.2.branches.0.0.bn1.weight
 (48)" fillcolor=lightblue]
	140587305878464 -> 140587589188752
	140587589188752 [label=AccumulateGrad]
	140587589187472 -> 140587841931024
	140587305878544 [label="stage4.2.branches.0.0.bn1.bias
 (48)" fillcolor=lightblue]
	140587305878544 -> 140587589187472
	140587589187472 [label=AccumulateGrad]
	140588144540112 -> 140588144539600
	140587305879344 [label="stage4.2.branches.0.0.conv2.weight
 (48, 48, 3, 3)" fillcolor=lightblue]
	140587305879344 -> 140588144540112
	140588144540112 [label=AccumulateGrad]
	140588144539856 -> 140588144539408
	140587305879584 [label="stage4.2.branches.0.0.bn2.weight
 (48)" fillcolor=lightblue]
	140587305879584 -> 140588144539856
	140588144539856 [label=AccumulateGrad]
	140588144539792 -> 140588144539408
	140587305879664 [label="stage4.2.branches.0.0.bn2.bias
 (48)" fillcolor=lightblue]
	140587305879664 -> 140588144539792
	140588144539792 [label=AccumulateGrad]
	140588144539728 -> 140588144539280
	140588144539152 -> 140588144538832
	140587305880544 [label="stage4.2.branches.0.1.conv1.weight
 (48, 48, 3, 3)" fillcolor=lightblue]
	140587305880544 -> 140588144539152
	140588144539152 [label=AccumulateGrad]
	140588144539088 -> 140588144538576
	140587305880784 [label="stage4.2.branches.0.1.bn1.weight
 (48)" fillcolor=lightblue]
	140587305880784 -> 140588144539088
	140588144539088 [label=AccumulateGrad]
	140588144539024 -> 140588144538576
	140587305880864 [label="stage4.2.branches.0.1.bn1.bias
 (48)" fillcolor=lightblue]
	140587305880864 -> 140588144539024
	140588144539024 [label=AccumulateGrad]
	140588144538704 -> 140588144537872
	140587305922720 [label="stage4.2.branches.0.1.conv2.weight
 (48, 48, 3, 3)" fillcolor=lightblue]
	140587305922720 -> 140588144538704
	140588144538704 [label=AccumulateGrad]
	140588144538448 -> 140588144538128
	140587305922960 [label="stage4.2.branches.0.1.bn2.weight
 (48)" fillcolor=lightblue]
	140587305922960 -> 140588144538448
	140588144538448 [label=AccumulateGrad]
	140588144538384 -> 140588144538128
	140587305923040 [label="stage4.2.branches.0.1.bn2.bias
 (48)" fillcolor=lightblue]
	140587305923040 -> 140588144538384
	140588144538384 [label=AccumulateGrad]
	140588144538320 -> 140588144537936
	140588144537808 -> 140588144529296
	140587305923840 [label="stage4.2.branches.0.2.conv1.weight
 (48, 48, 3, 3)" fillcolor=lightblue]
	140587305923840 -> 140588144537808
	140588144537808 [label=AccumulateGrad]
	140588144529232 -> 140588144529040
	140587305924080 [label="stage4.2.branches.0.2.bn1.weight
 (48)" fillcolor=lightblue]
	140587305924080 -> 140588144529232
	140588144529232 [label=AccumulateGrad]
	140588144537744 -> 140588144529040
	140587305924160 [label="stage4.2.branches.0.2.bn1.bias
 (48)" fillcolor=lightblue]
	140587305924160 -> 140588144537744
	140588144537744 [label=AccumulateGrad]
	140588144529168 -> 140588144528336
	140587305924960 [label="stage4.2.branches.0.2.conv2.weight
 (48, 48, 3, 3)" fillcolor=lightblue]
	140587305924960 -> 140588144529168
	140588144529168 [label=AccumulateGrad]
	140588144528912 -> 140588144528592
	140587305925200 [label="stage4.2.branches.0.2.bn2.weight
 (48)" fillcolor=lightblue]
	140587305925200 -> 140588144528912
	140588144528912 [label=AccumulateGrad]
	140588144528848 -> 140588144528592
	140587305925280 [label="stage4.2.branches.0.2.bn2.bias
 (48)" fillcolor=lightblue]
	140587305925280 -> 140588144528848
	140588144528848 [label=AccumulateGrad]
	140588144528784 -> 140588144528400
	140588144528272 -> 140588144528080
	140587305926080 [label="stage4.2.branches.0.3.conv1.weight
 (48, 48, 3, 3)" fillcolor=lightblue]
	140587305926080 -> 140588144528272
	140588144528272 [label=AccumulateGrad]
	140588144528208 -> 140588144527760
	140587305926320 [label="stage4.2.branches.0.3.bn1.weight
 (48)" fillcolor=lightblue]
	140587305926320 -> 140588144528208
	140588144528208 [label=AccumulateGrad]
	140588144527952 -> 140588144527760
	140587305926400 [label="stage4.2.branches.0.3.bn1.bias
 (48)" fillcolor=lightblue]
	140587305926400 -> 140588144527952
	140588144527952 [label=AccumulateGrad]
	140588144527888 -> 140588144526544
	140587305960064 [label="stage4.2.branches.0.3.conv2.weight
 (48, 48, 3, 3)" fillcolor=lightblue]
	140587305960064 -> 140588144527888
	140588144527888 [label=AccumulateGrad]
	140588144527632 -> 140588144527312
	140587305960304 [label="stage4.2.branches.0.3.bn2.weight
 (48)" fillcolor=lightblue]
	140587305960304 -> 140588144527632
	140588144527632 [label=AccumulateGrad]
	140588144527568 -> 140588144527312
	140587305960384 [label="stage4.2.branches.0.3.bn2.bias
 (48)" fillcolor=lightblue]
	140587305960384 -> 140588144527568
	140588144527568 [label=AccumulateGrad]
	140588144527504 -> 140588144527056
	140588144527184 -> 140588144526480
	140588144527184 [label=UpsampleBilinear2DBackward1]
	140587304755152 -> 140588144527184
	140587304755152 [label=NativeBatchNormBackward]
	140588144527440 -> 140587304755152
	140588144527440 [label=MkldnnConvolutionBackward]
	140588144528144 -> 140588144527440
	140588144528144 [label=ReluBackward1]
	140588144527824 -> 140588144528144
	140588144527824 [label=AddBackward0]
	140588144528656 -> 140588144527824
	140588144528656 [label=NativeBatchNormBackward]
	140588144537680 -> 140588144528656
	140588144537680 [label=MkldnnConvolutionBackward]
	140588144538192 -> 140588144537680
	140588144538192 [label=ReluBackward1]
	140588144538768 -> 140588144538192
	140588144538768 [label=NativeBatchNormBackward]
	140588144555792 -> 140588144538768
	140588144555792 [label=MkldnnConvolutionBackward]
	140588144529104 -> 140588144555792
	140588144529104 [label=ReluBackward1]
	140587589188240 -> 140588144529104
	140587589188240 [label=AddBackward0]
	140584557418896 -> 140587589188240
	140584557418896 [label=NativeBatchNormBackward]
	140584557417808 -> 140584557418896
	140584557417808 [label=MkldnnConvolutionBackward]
	140584557452688 -> 140584557417808
	140584557452688 [label=ReluBackward1]
	140584557452944 -> 140584557452688
	140584557452944 [label=NativeBatchNormBackward]
	140584557479824 -> 140584557452944
	140584557479824 [label=MkldnnConvolutionBackward]
	140584557418448 -> 140584557479824
	140584557418448 [label=ReluBackward1]
	140584557479184 -> 140584557418448
	140584557479184 [label=AddBackward0]
	140584557481104 -> 140584557479184
	140584557481104 [label=NativeBatchNormBackward]
	140584557478800 -> 140584557481104
	140584557478800 [label=MkldnnConvolutionBackward]
	140584557480656 -> 140584557478800
	140584557480656 [label=ReluBackward1]
	140584557481296 -> 140584557480656
	140584557481296 [label=NativeBatchNormBackward]
	140584557487952 -> 140584557481296
	140584557487952 [label=MkldnnConvolutionBackward]
	140584557480080 -> 140584557487952
	140584557480080 [label=ReluBackward1]
	140584557487440 -> 140584557480080
	140584557487440 [label=AddBackward0]
	140584557488208 -> 140584557487440
	140584557488208 [label=NativeBatchNormBackward]
	140584557487824 -> 140584557488208
	140584557487824 [label=MkldnnConvolutionBackward]
	140584557488592 -> 140584557487824
	140584557488592 [label=ReluBackward1]
	140584557488656 -> 140584557488592
	140584557488656 [label=NativeBatchNormBackward]
	140584557488976 -> 140584557488656
	140584557488976 [label=MkldnnConvolutionBackward]
	140584557488400 -> 140584557488976
	140584557488400 [label=ReluBackward1]
	140584557489296 -> 140584557488400
	140584557489296 [label=AddBackward0]
	140584557489488 -> 140584557489296
	140584557489488 [label=AddBackward0]
	140584557489232 -> 140584557489488
	140584557489232 [label=AddBackward0]
	140584557489744 -> 140584557489232
	140584557489744 [label=NativeBatchNormBackward]
	140584557490000 -> 140584557489744
	140584557490000 [label=MkldnnConvolutionBackward]
	140587588026704 -> 140584557490000
	140584557535312 -> 140584557490000
	140587069238160 [label="stage4.1.fuse_layers.1.0.0.0.weight
 (96, 48, 3, 3)" fillcolor=lightblue]
	140587069238160 -> 140584557535312
	140584557535312 [label=AccumulateGrad]
	140584557490128 -> 140584557489744
	140587069238400 [label="stage4.1.fuse_layers.1.0.0.1.weight
 (96)" fillcolor=lightblue]
	140587069238400 -> 140584557490128
	140584557490128 [label=AccumulateGrad]
	140584557489424 -> 140584557489744
	140587069238480 [label="stage4.1.fuse_layers.1.0.0.1.bias
 (96)" fillcolor=lightblue]
	140587069238480 -> 140584557489424
	140584557489424 [label=AccumulateGrad]
	140588144540880 -> 140584557489232
	140584557489872 -> 140584557489488
	140584557489872 [label=UpsampleBilinear2DBackward1]
	140584557353488 -> 140584557489872
	140584557353488 [label=NativeBatchNormBackward]
	140584557489936 -> 140584557353488
	140584557489936 [label=MkldnnConvolutionBackward]
	140584557351248 -> 140584557489936
	140584557535504 -> 140584557489936
	140587069276480 [label="stage4.1.fuse_layers.1.2.0.weight
 (96, 192, 1, 1)" fillcolor=lightblue]
	140587069276480 -> 140584557535504
	140584557535504 [label=AccumulateGrad]
	140584557535632 -> 140584557353488
	140587069276720 [label="stage4.1.fuse_layers.1.2.1.weight
 (96)" fillcolor=lightblue]
	140587069276720 -> 140584557535632
	140584557535632 [label=AccumulateGrad]
	140584557535696 -> 140584557353488
	140587069276800 [label="stage4.1.fuse_layers.1.2.1.bias
 (96)" fillcolor=lightblue]
	140587069276800 -> 140584557535696
	140584557535696 [label=AccumulateGrad]
	140584557489680 -> 140584557489296
	140584557489680 [label=UpsampleBilinear2DBackward1]
	140584557489808 -> 140584557489680
	140584557489808 [label=NativeBatchNormBackward]
	140584557535824 -> 140584557489808
	140584557535824 [label=MkldnnConvolutionBackward]
	140584557376784 -> 140584557535824
	140584557535952 -> 140584557535824
	140587069277680 [label="stage4.1.fuse_layers.1.3.0.weight
 (96, 384, 1, 1)" fillcolor=lightblue]
	140587069277680 -> 140584557535952
	140584557535952 [label=AccumulateGrad]
	140584557536016 -> 140584557489808
	140587069277920 [label="stage4.1.fuse_layers.1.3.1.weight
 (96)" fillcolor=lightblue]
	140587069277920 -> 140584557536016
	140584557536016 [label=AccumulateGrad]
	140584557535888 -> 140584557489808
	140587069278000 [label="stage4.1.fuse_layers.1.3.1.bias
 (96)" fillcolor=lightblue]
	140587069278000 -> 140584557535888
	140584557535888 [label=AccumulateGrad]
	140584557489168 -> 140584557488976
	140587305961264 [label="stage4.2.branches.1.0.conv1.weight
 (96, 96, 3, 3)" fillcolor=lightblue]
	140587305961264 -> 140584557489168
	140584557489168 [label=AccumulateGrad]
	140584557489104 -> 140584557488656
	140587305961504 [label="stage4.2.branches.1.0.bn1.weight
 (96)" fillcolor=lightblue]
	140587305961504 -> 140584557489104
	140584557489104 [label=AccumulateGrad]
	140584557488848 -> 140584557488656
	140587305961584 [label="stage4.2.branches.1.0.bn1.bias
 (96)" fillcolor=lightblue]
	140587305961584 -> 140584557488848
	140584557488848 [label=AccumulateGrad]
	140584557488784 -> 140584557487824
	140587305962384 [label="stage4.2.branches.1.0.conv2.weight
 (96, 96, 3, 3)" fillcolor=lightblue]
	140587305962384 -> 140584557488784
	140584557488784 [label=AccumulateGrad]
	140584557488528 -> 140584557488208
	140587305962624 [label="stage4.2.branches.1.0.bn2.weight
 (96)" fillcolor=lightblue]
	140587305962624 -> 140584557488528
	140584557488528 [label=AccumulateGrad]
	140584557488464 -> 140584557488208
	140587305962704 [label="stage4.2.branches.1.0.bn2.bias
 (96)" fillcolor=lightblue]
	140587305962704 -> 140584557488464
	140584557488464 [label=AccumulateGrad]
	140584557488400 -> 140584557487440
	140584557486544 -> 140584557487952
	140587306004640 [label="stage4.2.branches.1.1.conv1.weight
 (96, 96, 3, 3)" fillcolor=lightblue]
	140587306004640 -> 140584557486544
	140584557486544 [label=AccumulateGrad]
	140584557488016 -> 140584557481296
	140587306004880 [label="stage4.2.branches.1.1.bn1.weight
 (96)" fillcolor=lightblue]
	140587306004880 -> 140584557488016
	140584557488016 [label=AccumulateGrad]
	140584557487760 -> 140584557481296
	140587306004960 [label="stage4.2.branches.1.1.bn1.bias
 (96)" fillcolor=lightblue]
	140587306004960 -> 140584557487760
	140584557487760 [label=AccumulateGrad]
	140584557480784 -> 140584557478800
	140587306005760 [label="stage4.2.branches.1.1.conv2.weight
 (96, 96, 3, 3)" fillcolor=lightblue]
	140587306005760 -> 140584557480784
	140584557480784 [label=AccumulateGrad]
	140584557480016 -> 140584557481104
	140587306006000 [label="stage4.2.branches.1.1.bn2.weight
 (96)" fillcolor=lightblue]
	140587306006000 -> 140584557480016
	140584557480016 [label=AccumulateGrad]
	140584557480464 -> 140584557481104
	140587306006080 [label="stage4.2.branches.1.1.bn2.bias
 (96)" fillcolor=lightblue]
	140587306006080 -> 140584557480464
	140584557480464 [label=AccumulateGrad]
	140584557480080 -> 140584557479184
	140584557478224 -> 140584557479824
	140587306006880 [label="stage4.2.branches.1.2.conv1.weight
 (96, 96, 3, 3)" fillcolor=lightblue]
	140587306006880 -> 140584557478224
	140584557478224 [label=AccumulateGrad]
	140584557478608 -> 140584557452944
	140587306007120 [label="stage4.2.branches.1.2.bn1.weight
 (96)" fillcolor=lightblue]
	140587306007120 -> 140584557478608
	140584557478608 [label=AccumulateGrad]
	140584557478544 -> 140584557452944
	140587306007200 [label="stage4.2.branches.1.2.bn1.bias
 (96)" fillcolor=lightblue]
	140587306007200 -> 140584557478544
	140584557478544 [label=AccumulateGrad]
	140584557451792 -> 140584557417808
	140587306008000 [label="stage4.2.branches.1.2.conv2.weight
 (96, 96, 3, 3)" fillcolor=lightblue]
	140587306008000 -> 140584557451792
	140584557451792 [label=AccumulateGrad]
	140584557451216 -> 140584557418896
	140587306008240 [label="stage4.2.branches.1.2.bn2.weight
 (96)" fillcolor=lightblue]
	140587306008240 -> 140584557451216
	140584557451216 [label=AccumulateGrad]
	140584557449360 -> 140584557418896
	140587306008320 [label="stage4.2.branches.1.2.bn2.bias
 (96)" fillcolor=lightblue]
	140587306008320 -> 140584557449360
	140584557449360 [label=AccumulateGrad]
	140584557418448 -> 140587589188240
	140587843142352 -> 140588144555792
	140587344937600 [label="stage4.2.branches.1.3.conv1.weight
 (96, 96, 3, 3)" fillcolor=lightblue]
	140587344937600 -> 140587843142352
	140587843142352 [label=AccumulateGrad]
	140588144540048 -> 140588144538768
	140587344937840 [label="stage4.2.branches.1.3.bn1.weight
 (96)" fillcolor=lightblue]
	140587344937840 -> 140588144540048
	140588144540048 [label=AccumulateGrad]
	140588144539664 -> 140588144538768
	140587344937920 [label="stage4.2.branches.1.3.bn1.bias
 (96)" fillcolor=lightblue]
	140587344937920 -> 140588144539664
	140588144539664 [label=AccumulateGrad]
	140588144538640 -> 140588144537680
	140587344938720 [label="stage4.2.branches.1.3.conv2.weight
 (96, 96, 3, 3)" fillcolor=lightblue]
	140587344938720 -> 140588144538640
	140588144538640 [label=AccumulateGrad]
	140588144539216 -> 140588144528656
	140587344938960 [label="stage4.2.branches.1.3.bn2.weight
 (96)" fillcolor=lightblue]
	140587344938960 -> 140588144539216
	140588144539216 [label=AccumulateGrad]
	140588144538000 -> 140588144528656
	140587344939040 [label="stage4.2.branches.1.3.bn2.bias
 (96)" fillcolor=lightblue]
	140587344939040 -> 140588144538000
	140588144538000 [label=AccumulateGrad]
	140588144529104 -> 140588144527824
	140588144528464 -> 140588144527440
	140587345138944 [label="stage4.2.fuse_layers.0.1.0.weight
 (48, 96, 1, 1)" fillcolor=lightblue]
	140587345138944 -> 140588144528464
	140588144528464 [label=AccumulateGrad]
	140588144528016 -> 140587304755152
	140587345139184 [label="stage4.2.fuse_layers.0.1.1.weight
 (48)" fillcolor=lightblue]
	140587345139184 -> 140588144528016
	140588144528016 [label=AccumulateGrad]
	140588144528720 -> 140587304755152
	140587345139264 [label="stage4.2.fuse_layers.0.1.1.bias
 (48)" fillcolor=lightblue]
	140587345139264 -> 140588144528720
	140588144528720 [label=AccumulateGrad]
	140588144526992 -> 140588144526608
	140588144526992 [label=UpsampleBilinear2DBackward1]
	140587034676816 -> 140588144526992
	140587034676816 [label=NativeBatchNormBackward]
	140588144527248 -> 140587034676816
	140588144527248 [label=MkldnnConvolutionBackward]
	140588144538256 -> 140588144527248
	140588144538256 [label=ReluBackward1]
	140584557449616 -> 140588144538256
	140584557449616 [label=AddBackward0]
	140584557451472 -> 140584557449616
	140584557451472 [label=NativeBatchNormBackward]
	140588144538064 -> 140584557451472
	140588144538064 [label=MkldnnConvolutionBackward]
	140584557479376 -> 140588144538064
	140584557479376 [label=ReluBackward1]
	140584557479504 -> 140584557479376
	140584557479504 [label=NativeBatchNormBackward]
	140584557489360 -> 140584557479504
	140584557489360 [label=MkldnnConvolutionBackward]
	140587589188368 -> 140584557489360
	140587589188368 [label=ReluBackward1]
	140584557488912 -> 140587589188368
	140584557488912 [label=AddBackward0]
	140584557489040 -> 140584557488912
	140584557489040 [label=NativeBatchNormBackward]
	140584557535568 -> 140584557489040
	140584557535568 [label=MkldnnConvolutionBackward]
	140584557536272 -> 140584557535568
	140584557536272 [label=ReluBackward1]
	140584557536208 -> 140584557536272
	140584557536208 [label=NativeBatchNormBackward]
	140584557536464 -> 140584557536208
	140584557536464 [label=MkldnnConvolutionBackward]
	140584557488272 -> 140584557536464
	140584557488272 [label=ReluBackward1]
	140584557536784 -> 140584557488272
	140584557536784 [label=AddBackward0]
	140584557536976 -> 140584557536784
	140584557536976 [label=NativeBatchNormBackward]
	140584557536720 -> 140584557536976
	140584557536720 [label=MkldnnConvolutionBackward]
	140584557537360 -> 140584557536720
	140584557537360 [label=ReluBackward1]
	140584557537424 -> 140584557537360
	140584557537424 [label=NativeBatchNormBackward]
	140584557537744 -> 140584557537424
	140584557537744 [label=MkldnnConvolutionBackward]
	140584557537168 -> 140584557537744
	140584557537168 [label=ReluBackward1]
	140584557538064 -> 140584557537168
	140584557538064 [label=AddBackward0]
	140584557538256 -> 140584557538064
	140584557538256 [label=NativeBatchNormBackward]
	140584557538000 -> 140584557538256
	140584557538000 [label=MkldnnConvolutionBackward]
	140584557538640 -> 140584557538000
	140584557538640 [label=ReluBackward1]
	140584557538704 -> 140584557538640
	140584557538704 [label=NativeBatchNormBackward]
	140584557539024 -> 140584557538704
	140584557539024 [label=MkldnnConvolutionBackward]
	140584557538448 -> 140584557539024
	140584557538448 [label=ReluBackward1]
	140584557559952 -> 140584557538448
	140584557559952 [label=AddBackward0]
	140584557560080 -> 140584557559952
	140584557560080 [label=AddBackward0]
	140584557559888 -> 140584557560080
	140584557559888 [label=AddBackward0]
	140584557560400 -> 140584557559888
	140584557560400 [label=NativeBatchNormBackward]
	140584557560016 -> 140584557560400
	140584557560016 [label=MkldnnConvolutionBackward]
	140584557560784 -> 140584557560016
	140584557560784 [label=ReluBackward1]
	140584557560848 -> 140584557560784
	140584557560848 [label=NativeBatchNormBackward]
	140584557561168 -> 140584557560848
	140584557561168 [label=MkldnnConvolutionBackward]
	140587588026704 -> 140584557561168
	140584557561360 -> 140584557561168
	140587069279040 [label="stage4.1.fuse_layers.2.0.0.0.weight
 (48, 48, 3, 3)" fillcolor=lightblue]
	140587069279040 -> 140584557561360
	140584557561360 [label=AccumulateGrad]
	140584557561296 -> 140584557560848
	140587069279280 [label="stage4.1.fuse_layers.2.0.0.1.weight
 (48)" fillcolor=lightblue]
	140587069279280 -> 140584557561296
	140584557561296 [label=AccumulateGrad]
	140584557561040 -> 140584557560848
	140587069279360 [label="stage4.1.fuse_layers.2.0.0.1.bias
 (48)" fillcolor=lightblue]
	140587069279360 -> 140584557561040
	140584557561040 [label=AccumulateGrad]
	140584557560976 -> 140584557560016
	140587069317280 [label="stage4.1.fuse_layers.2.0.1.0.weight
 (192, 48, 3, 3)" fillcolor=lightblue]
	140587069317280 -> 140584557560976
	140584557560976 [label=AccumulateGrad]
	140584557560720 -> 140584557560400
	140587069317520 [label="stage4.1.fuse_layers.2.0.1.1.weight
 (192)" fillcolor=lightblue]
	140587069317520 -> 140584557560720
	140584557560720 [label=AccumulateGrad]
	140584557560656 -> 140584557560400
	140587069317600 [label="stage4.1.fuse_layers.2.0.1.1.bias
 (192)" fillcolor=lightblue]
	140587069317600 -> 140584557560656
	140584557560656 [label=AccumulateGrad]
	140584557560592 -> 140584557559888
	140584557560592 [label=NativeBatchNormBackward]
	140584557419152 -> 140584557560592
	140584557419152 [label=MkldnnConvolutionBackward]
	140588144540880 -> 140584557419152
	140584557561616 -> 140584557419152
	140587069318640 [label="stage4.1.fuse_layers.2.1.0.0.weight
 (192, 96, 3, 3)" fillcolor=lightblue]
	140587069318640 -> 140584557561616
	140584557561616 [label=AccumulateGrad]
	140584557561744 -> 140584557560592
	140587069318880 [label="stage4.1.fuse_layers.2.1.0.1.weight
 (192)" fillcolor=lightblue]
	140587069318880 -> 140584557561744
	140584557561744 [label=AccumulateGrad]
	140584557561104 -> 140584557560592
	140587069318960 [label="stage4.1.fuse_layers.2.1.0.1.bias
 (192)" fillcolor=lightblue]
	140587069318960 -> 140584557561104
	140584557561104 [label=AccumulateGrad]
	140584557351248 -> 140584557560080
	140584557560272 -> 140584557559952
	140584557560272 [label=UpsampleBilinear2DBackward1]
	140584557560464 -> 140584557560272
	140584557560464 [label=NativeBatchNormBackward]
	140584557560336 -> 140584557560464
	140584557560336 [label=MkldnnConvolutionBackward]
	140584557376784 -> 140584557560336
	140584557561488 -> 140584557560336
	140587069320000 [label="stage4.1.fuse_layers.2.3.0.weight
 (192, 384, 1, 1)" fillcolor=lightblue]
	140587069320000 -> 140584557561488
	140584557561488 [label=AccumulateGrad]
	140584557561808 -> 140584557560464
	140587069320240 [label="stage4.1.fuse_layers.2.3.1.weight
 (192)" fillcolor=lightblue]
	140587069320240 -> 140584557561808
	140584557561808 [label=AccumulateGrad]
	140584557561552 -> 140584557560464
	140587069320320 [label="stage4.1.fuse_layers.2.3.1.bias
 (192)" fillcolor=lightblue]
	140587069320320 -> 140584557561552
	140584557561552 [label=AccumulateGrad]
	140584557539216 -> 140584557539024
	140587344939920 [label="stage4.2.branches.2.0.conv1.weight
 (192, 192, 3, 3)" fillcolor=lightblue]
	140587344939920 -> 140584557539216
	140584557539216 [label=AccumulateGrad]
	140584557539152 -> 140584557538704
	140587344940160 [label="stage4.2.branches.2.0.bn1.weight
 (192)" fillcolor=lightblue]
	140587344940160 -> 140584557539152
	140584557539152 [label=AccumulateGrad]
	140584557538896 -> 140584557538704
	140587344940240 [label="stage4.2.branches.2.0.bn1.bias
 (192)" fillcolor=lightblue]
	140587344940240 -> 140584557538896
	140584557538896 [label=AccumulateGrad]
	140584557538832 -> 140584557538000
	140587344973904 [label="stage4.2.branches.2.0.conv2.weight
 (192, 192, 3, 3)" fillcolor=lightblue]
	140587344973904 -> 140584557538832
	140584557538832 [label=AccumulateGrad]
	140584557538576 -> 140584557538256
	140587344974144 [label="stage4.2.branches.2.0.bn2.weight
 (192)" fillcolor=lightblue]
	140587344974144 -> 140584557538576
	140584557538576 [label=AccumulateGrad]
	140584557538512 -> 140584557538256
	140587344974224 [label="stage4.2.branches.2.0.bn2.bias
 (192)" fillcolor=lightblue]
	140587344974224 -> 140584557538512
	140584557538512 [label=AccumulateGrad]
	140584557538448 -> 140584557538064
	140584557537936 -> 140584557537744
	140587344975104 [label="stage4.2.branches.2.1.conv1.weight
 (192, 192, 3, 3)" fillcolor=lightblue]
	140587344975104 -> 140584557537936
	140584557537936 [label=AccumulateGrad]
	140584557537872 -> 140584557537424
	140587344975344 [label="stage4.2.branches.2.1.bn1.weight
 (192)" fillcolor=lightblue]
	140587344975344 -> 140584557537872
	140584557537872 [label=AccumulateGrad]
	140584557537616 -> 140584557537424
	140587344975424 [label="stage4.2.branches.2.1.bn1.bias
 (192)" fillcolor=lightblue]
	140587344975424 -> 140584557537616
	140584557537616 [label=AccumulateGrad]
	140584557537552 -> 140584557536720
	140587344976224 [label="stage4.2.branches.2.1.conv2.weight
 (192, 192, 3, 3)" fillcolor=lightblue]
	140587344976224 -> 140584557537552
	140584557537552 [label=AccumulateGrad]
	140584557537296 -> 140584557536976
	140587344976464 [label="stage4.2.branches.2.1.bn2.weight
 (192)" fillcolor=lightblue]
	140587344976464 -> 140584557537296
	140584557537296 [label=AccumulateGrad]
	140584557537232 -> 140584557536976
	140587344976544 [label="stage4.2.branches.2.1.bn2.bias
 (192)" fillcolor=lightblue]
	140587344976544 -> 140584557537232
	140584557537232 [label=AccumulateGrad]
	140584557537168 -> 140584557536784
	140584557536656 -> 140584557536464
	140587344977344 [label="stage4.2.branches.2.2.conv1.weight
 (192, 192, 3, 3)" fillcolor=lightblue]
	140587344977344 -> 140584557536656
	140584557536656 [label=AccumulateGrad]
	140584557536592 -> 140584557536208
	140587344977584 [label="stage4.2.branches.2.2.bn1.weight
 (192)" fillcolor=lightblue]
	140587344977584 -> 140584557536592
	140584557536592 [label=AccumulateGrad]
	140584557536336 -> 140584557536208
	140587344977664 [label="stage4.2.branches.2.2.bn1.bias
 (192)" fillcolor=lightblue]
	140587344977664 -> 140584557536336
	140584557536336 [label=AccumulateGrad]
	140584557535440 -> 140584557535568
	140587345019520 [label="stage4.2.branches.2.2.conv2.weight
 (192, 192, 3, 3)" fillcolor=lightblue]
	140587345019520 -> 140584557535440
	140584557535440 [label=AccumulateGrad]
	140584557536144 -> 140584557489040
	140587345019760 [label="stage4.2.branches.2.2.bn2.weight
 (192)" fillcolor=lightblue]
	140587345019760 -> 140584557536144
	140584557536144 [label=AccumulateGrad]
	140584557535760 -> 140584557489040
	140587345019840 [label="stage4.2.branches.2.2.bn2.bias
 (192)" fillcolor=lightblue]
	140587345019840 -> 140584557535760
	140584557535760 [label=AccumulateGrad]
	140584557488272 -> 140584557488912
	140584557488720 -> 140584557489360
	140587345020640 [label="stage4.2.branches.2.3.conv1.weight
 (192, 192, 3, 3)" fillcolor=lightblue]
	140587345020640 -> 140584557488720
	140584557488720 [label=AccumulateGrad]
	140584557488336 -> 140584557479504
	140587345020880 [label="stage4.2.branches.2.3.bn1.weight
 (192)" fillcolor=lightblue]
	140587345020880 -> 140584557488336
	140584557488336 [label=AccumulateGrad]
	140584557486416 -> 140584557479504
	140587345020960 [label="stage4.2.branches.2.3.bn1.bias
 (192)" fillcolor=lightblue]
	140587345020960 -> 140584557486416
	140584557486416 [label=AccumulateGrad]
	140584557479888 -> 140588144538064
	140587345021760 [label="stage4.2.branches.2.3.conv2.weight
 (192, 192, 3, 3)" fillcolor=lightblue]
	140587345021760 -> 140584557479888
	140584557479888 [label=AccumulateGrad]
	140584557481424 -> 140584557451472
	140587345022000 [label="stage4.2.branches.2.3.bn2.weight
 (192)" fillcolor=lightblue]
	140587345022000 -> 140584557481424
	140584557481424 [label=AccumulateGrad]
	140584557478736 -> 140584557451472
	140587345022080 [label="stage4.2.branches.2.3.bn2.bias
 (192)" fillcolor=lightblue]
	140587345022080 -> 140584557478736
	140584557478736 [label=AccumulateGrad]
	140587589188368 -> 140584557449616
	140588144539536 -> 140588144527248
	140587345140144 [label="stage4.2.fuse_layers.0.2.0.weight
 (48, 192, 1, 1)" fillcolor=lightblue]
	140587345140144 -> 140588144539536
	140588144539536 [label=AccumulateGrad]
	140588144529360 -> 140587034676816
	140587345140384 [label="stage4.2.fuse_layers.0.2.1.weight
 (48)" fillcolor=lightblue]
	140587345140384 -> 140588144529360
	140588144529360 [label=AccumulateGrad]
	140588144528528 -> 140587034676816
	140587345140464 [label="stage4.2.fuse_layers.0.2.1.bias
 (48)" fillcolor=lightblue]
	140587345140464 -> 140588144528528
	140588144528528 [label=AccumulateGrad]
	140588144526800 -> 140588144526416
	140588144526800 [label=UpsampleBilinear2DBackward1]
	140584557478096 -> 140588144526800
	140584557478096 [label=NativeBatchNormBackward]
	140588144539344 -> 140584557478096
	140588144539344 [label=MkldnnConvolutionBackward]
	140584557453136 -> 140588144539344
	140584557453136 [label=ReluBackward1]
	140584557489552 -> 140584557453136
	140584557489552 [label=AddBackward0]
	140584557487312 -> 140584557489552
	140584557487312 [label=NativeBatchNormBackward]
	140584557536848 -> 140584557487312
	140584557536848 [label=MkldnnConvolutionBackward]
	140584557536528 -> 140584557536848
	140584557536528 [label=ReluBackward1]
	140584557537104 -> 140584557536528
	140584557537104 [label=NativeBatchNormBackward]
	140584557538192 -> 140584557537104
	140584557538192 [label=MkldnnConvolutionBackward]
	140584557536400 -> 140584557538192
	140584557536400 [label=ReluBackward1]
	140584557538960 -> 140584557536400
	140584557538960 [label=AddBackward0]
	140584557560208 -> 140584557538960
	140584557560208 [label=NativeBatchNormBackward]
	140584557561872 -> 140584557560208
	140584557561872 [label=MkldnnConvolutionBackward]
	140584557562064 -> 140584557561872
	140584557562064 [label=ReluBackward1]
	140584557562000 -> 140584557562064
	140584557562000 [label=NativeBatchNormBackward]
	140584557562256 -> 140584557562000
	140584557562256 [label=MkldnnConvolutionBackward]
	140584557560144 -> 140584557562256
	140584557560144 [label=ReluBackward1]
	140584557562576 -> 140584557560144
	140584557562576 [label=AddBackward0]
	140584557562768 -> 140584557562576
	140584557562768 [label=NativeBatchNormBackward]
	140584557562512 -> 140584557562768
	140584557562512 [label=MkldnnConvolutionBackward]
	140584557563152 -> 140584557562512
	140584557563152 [label=ReluBackward1]
	140584557563216 -> 140584557563152
	140584557563216 [label=NativeBatchNormBackward]
	140584557563536 -> 140584557563216
	140584557563536 [label=MkldnnConvolutionBackward]
	140584557562960 -> 140584557563536
	140584557562960 [label=ReluBackward1]
	140584557563792 -> 140584557562960
	140584557563792 [label=AddBackward0]
	140583431618768 -> 140584557563792
	140583431618768 [label=NativeBatchNormBackward]
	140583431618640 -> 140583431618768
	140583431618640 [label=MkldnnConvolutionBackward]
	140583431619152 -> 140583431618640
	140583431619152 [label=ReluBackward1]
	140583431619216 -> 140583431619152
	140583431619216 [label=NativeBatchNormBackward]
	140583431619536 -> 140583431619216
	140583431619536 [label=MkldnnConvolutionBackward]
	140583431618960 -> 140583431619536
	140583431618960 [label=ReluBackward1]
	140583431619856 -> 140583431618960
	140583431619856 [label=AddBackward0]
	140583431620048 -> 140583431619856
	140583431620048 [label=AddBackward0]
	140583431620176 -> 140583431620048
	140583431620176 [label=AddBackward0]
	140583431619984 -> 140583431620176
	140583431619984 [label=NativeBatchNormBackward]
	140583431620432 -> 140583431619984
	140583431620432 [label=MkldnnConvolutionBackward]
	140583431620752 -> 140583431620432
	140583431620752 [label=ReluBackward1]
	140583431620816 -> 140583431620752
	140583431620816 [label=NativeBatchNormBackward]
	140583431621136 -> 140583431620816
	140583431621136 [label=MkldnnConvolutionBackward]
	140583431621328 -> 140583431621136
	140583431621328 [label=ReluBackward1]
	140583431621392 -> 140583431621328
	140583431621392 [label=NativeBatchNormBackward]
	140583431621712 -> 140583431621392
	140583431621712 [label=MkldnnConvolutionBackward]
	140587588026704 -> 140583431621712
	140583431621904 -> 140583431621712
	140587305779440 [label="stage4.1.fuse_layers.3.0.0.0.weight
 (48, 48, 3, 3)" fillcolor=lightblue]
	140587305779440 -> 140583431621904
	140583431621904 [label=AccumulateGrad]
	140583431621840 -> 140583431621392
	140587305779680 [label="stage4.1.fuse_layers.3.0.0.1.weight
 (48)" fillcolor=lightblue]
	140587305779680 -> 140583431621840
	140583431621840 [label=AccumulateGrad]
	140583431621584 -> 140583431621392
	140587305779760 [label="stage4.1.fuse_layers.3.0.0.1.bias
 (48)" fillcolor=lightblue]
	140587305779760 -> 140583431621584
	140583431621584 [label=AccumulateGrad]
	140583431621520 -> 140583431621136
	140587305780720 [label="stage4.1.fuse_layers.3.0.1.0.weight
 (48, 48, 3, 3)" fillcolor=lightblue]
	140587305780720 -> 140583431621520
	140583431621520 [label=AccumulateGrad]
	140583431621264 -> 140583431620816
	140587305780960 [label="stage4.1.fuse_layers.3.0.1.1.weight
 (48)" fillcolor=lightblue]
	140587305780960 -> 140583431621264
	140583431621264 [label=AccumulateGrad]
	140583431621008 -> 140583431620816
	140587305781040 [label="stage4.1.fuse_layers.3.0.1.1.bias
 (48)" fillcolor=lightblue]
	140587305781040 -> 140583431621008
	140583431621008 [label=AccumulateGrad]
	140583431620944 -> 140583431620432
	140587305782000 [label="stage4.1.fuse_layers.3.0.2.0.weight
 (384, 48, 3, 3)" fillcolor=lightblue]
	140587305782000 -> 140583431620944
	140583431620944 [label=AccumulateGrad]
	140583431620688 -> 140583431619984
	140587305782240 [label="stage4.1.fuse_layers.3.0.2.1.weight
 (384)" fillcolor=lightblue]
	140587305782240 -> 140583431620688
	140583431620688 [label=AccumulateGrad]
	140583431620624 -> 140583431619984
	140587305782320 [label="stage4.1.fuse_layers.3.0.2.1.bias
 (384)" fillcolor=lightblue]
	140587305782320 -> 140583431620624
	140583431620624 [label=AccumulateGrad]
	140583431620560 -> 140583431620176
	140583431620560 [label=NativeBatchNormBackward]
	140584557481488 -> 140583431620560
	140584557481488 [label=MkldnnConvolutionBackward]
	140583431621648 -> 140584557481488
	140583431621648 [label=ReluBackward1]
	140583431620880 -> 140583431621648
	140583431620880 [label=NativeBatchNormBackward]
	140583431622224 -> 140583431620880
	140583431622224 [label=MkldnnConvolutionBackward]
	140588144540880 -> 140583431622224
	140583431622352 -> 140583431622224
	140587305832608 [label="stage4.1.fuse_layers.3.1.0.0.weight
 (96, 96, 3, 3)" fillcolor=lightblue]
	140587305832608 -> 140583431622352
	140583431622352 [label=AccumulateGrad]
	140583431622032 -> 140583431620880
	140587305832848 [label="stage4.1.fuse_layers.3.1.0.1.weight
 (96)" fillcolor=lightblue]
	140587305832848 -> 140583431622032
	140583431622032 [label=AccumulateGrad]
	140583431621456 -> 140583431620880
	140587305832928 [label="stage4.1.fuse_layers.3.1.0.1.bias
 (96)" fillcolor=lightblue]
	140587305832928 -> 140583431621456
	140583431621456 [label=AccumulateGrad]
	140583431622160 -> 140584557481488
	140587305833888 [label="stage4.1.fuse_layers.3.1.1.0.weight
 (384, 96, 3, 3)" fillcolor=lightblue]
	140587305833888 -> 140583431622160
	140583431622160 [label=AccumulateGrad]
	140583431622288 -> 140583431620560
	140587305834128 [label="stage4.1.fuse_layers.3.1.1.1.weight
 (384)" fillcolor=lightblue]
	140587305834128 -> 140583431622288
	140583431622288 [label=AccumulateGrad]
	140583431621072 -> 140583431620560
	140587305834208 [label="stage4.1.fuse_layers.3.1.1.1.bias
 (384)" fillcolor=lightblue]
	140587305834208 -> 140583431621072
	140583431621072 [label=AccumulateGrad]
	140583431620368 -> 140583431620048
	140583431620368 [label=NativeBatchNormBackward]
	140583431620496 -> 140583431620368
	140583431620496 [label=MkldnnConvolutionBackward]
	140584557351248 -> 140583431620496
	140583431622416 -> 140583431620496
	140587305835248 [label="stage4.1.fuse_layers.3.2.0.0.weight
 (384, 192, 3, 3)" fillcolor=lightblue]
	140587305835248 -> 140583431622416
	140583431622416 [label=AccumulateGrad]
	140583431622544 -> 140583431620368
	140587305835488 [label="stage4.1.fuse_layers.3.2.0.1.weight
 (384)" fillcolor=lightblue]
	140587305835488 -> 140583431622544
	140583431622544 [label=AccumulateGrad]
	140583431621776 -> 140583431620368
	140587305835568 [label="stage4.1.fuse_layers.3.2.0.1.bias
 (384)" fillcolor=lightblue]
	140587305835568 -> 140583431621776
	140583431621776 [label=AccumulateGrad]
	140584557376784 -> 140583431619856
	140583431619728 -> 140583431619536
	140587345055824 [label="stage4.2.branches.3.0.conv1.weight
 (384, 384, 3, 3)" fillcolor=lightblue]
	140587345055824 -> 140583431619728
	140583431619728 [label=AccumulateGrad]
	140583431619664 -> 140583431619216
	140587345056064 [label="stage4.2.branches.3.0.bn1.weight
 (384)" fillcolor=lightblue]
	140587345056064 -> 140583431619664
	140583431619664 [label=AccumulateGrad]
	140583431619408 -> 140583431619216
	140587345056144 [label="stage4.2.branches.3.0.bn1.bias
 (384)" fillcolor=lightblue]
	140587345056144 -> 140583431619408
	140583431619408 [label=AccumulateGrad]
	140583431619344 -> 140583431618640
	140587345056944 [label="stage4.2.branches.3.0.conv2.weight
 (384, 384, 3, 3)" fillcolor=lightblue]
	140587345056944 -> 140583431619344
	140583431619344 [label=AccumulateGrad]
	140583431619088 -> 140583431618768
	140587345057184 [label="stage4.2.branches.3.0.bn2.weight
 (384)" fillcolor=lightblue]
	140587345057184 -> 140583431619088
	140583431619088 [label=AccumulateGrad]
	140583431619024 -> 140583431618768
	140587345057264 [label="stage4.2.branches.3.0.bn2.bias
 (384)" fillcolor=lightblue]
	140587345057264 -> 140583431619024
	140583431619024 [label=AccumulateGrad]
	140583431618960 -> 140584557563792
	140584557563728 -> 140584557563536
	140587345058144 [label="stage4.2.branches.3.1.conv1.weight
 (384, 384, 3, 3)" fillcolor=lightblue]
	140587345058144 -> 140584557563728
	140584557563728 [label=AccumulateGrad]
	140584557563664 -> 140584557563216
	140587345058384 [label="stage4.2.branches.3.1.bn1.weight
 (384)" fillcolor=lightblue]
	140587345058384 -> 140584557563664
	140584557563664 [label=AccumulateGrad]
	140584557563408 -> 140584557563216
	140587345058464 [label="stage4.2.branches.3.1.bn1.bias
 (384)" fillcolor=lightblue]
	140587345058464 -> 140584557563408
	140584557563408 [label=AccumulateGrad]
	140584557563344 -> 140584557562512
	140587345059264 [label="stage4.2.branches.3.1.conv2.weight
 (384, 384, 3, 3)" fillcolor=lightblue]
	140587345059264 -> 140584557563344
	140584557563344 [label=AccumulateGrad]
	140584557563088 -> 140584557562768
	140587345059504 [label="stage4.2.branches.3.1.bn2.weight
 (384)" fillcolor=lightblue]
	140587345059504 -> 140584557563088
	140584557563088 [label=AccumulateGrad]
	140584557563024 -> 140584557562768
	140587345059584 [label="stage4.2.branches.3.1.bn2.bias
 (384)" fillcolor=lightblue]
	140587345059584 -> 140584557563024
	140584557563024 [label=AccumulateGrad]
	140584557562960 -> 140584557562576
	140584557562448 -> 140584557562256
	140587345093248 [label="stage4.2.branches.3.2.conv1.weight
 (384, 384, 3, 3)" fillcolor=lightblue]
	140587345093248 -> 140584557562448
	140584557562448 [label=AccumulateGrad]
	140584557562384 -> 140584557562000
	140587345093488 [label="stage4.2.branches.3.2.bn1.weight
 (384)" fillcolor=lightblue]
	140587345093488 -> 140584557562384
	140584557562384 [label=AccumulateGrad]
	140584557562128 -> 140584557562000
	140587345093568 [label="stage4.2.branches.3.2.bn1.bias
 (384)" fillcolor=lightblue]
	140587345093568 -> 140584557562128
	140584557562128 [label=AccumulateGrad]
	140584557560912 -> 140584557561872
	140587345094368 [label="stage4.2.branches.3.2.conv2.weight
 (384, 384, 3, 3)" fillcolor=lightblue]
	140587345094368 -> 140584557560912
	140584557560912 [label=AccumulateGrad]
	140584557561936 -> 140584557560208
	140587345094608 [label="stage4.2.branches.3.2.bn2.weight
 (384)" fillcolor=lightblue]
	140587345094608 -> 140584557561936
	140584557561936 [label=AccumulateGrad]
	140584557561424 -> 140584557560208
	140587345094688 [label="stage4.2.branches.3.2.bn2.bias
 (384)" fillcolor=lightblue]
	140587345094688 -> 140584557561424
	140584557561424 [label=AccumulateGrad]
	140584557560144 -> 140584557538960
	140584557538768 -> 140584557538192
	140587345095488 [label="stage4.2.branches.3.3.conv1.weight
 (384, 384, 3, 3)" fillcolor=lightblue]
	140587345095488 -> 140584557538768
	140584557538768 [label=AccumulateGrad]
	140584557538384 -> 140584557537104
	140587345095728 [label="stage4.2.branches.3.3.bn1.weight
 (384)" fillcolor=lightblue]
	140587345095728 -> 140584557538384
	140584557538384 [label=AccumulateGrad]
	140584557537680 -> 140584557537104
	140587345095808 [label="stage4.2.branches.3.3.bn1.bias
 (384)" fillcolor=lightblue]
	140587345095808 -> 140584557537680
	140584557537680 [label=AccumulateGrad]
	140584557537040 -> 140584557536848
	140587345096608 [label="stage4.2.branches.3.3.conv2.weight
 (384, 384, 3, 3)" fillcolor=lightblue]
	140587345096608 -> 140584557537040
	140584557537040 [label=AccumulateGrad]
	140584557536912 -> 140584557487312
	140587345137904 [label="stage4.2.branches.3.3.bn2.weight
 (384)" fillcolor=lightblue]
	140587345137904 -> 140584557536912
	140584557536912 [label=AccumulateGrad]
	140584557538128 -> 140584557487312
	140587345137984 [label="stage4.2.branches.3.3.bn2.bias
 (384)" fillcolor=lightblue]
	140587345137984 -> 140584557538128
	140584557538128 [label=AccumulateGrad]
	140584557536400 -> 140584557489552
	140584557489616 -> 140588144539344
	140587345141344 [label="stage4.2.fuse_layers.0.3.0.weight
 (48, 384, 1, 1)" fillcolor=lightblue]
	140587345141344 -> 140584557489616
	140584557489616 [label=AccumulateGrad]
	140588144538960 -> 140584557478096
	140587345141584 [label="stage4.2.fuse_layers.0.3.1.weight
 (48)" fillcolor=lightblue]
	140587345141584 -> 140588144538960
	140588144538960 [label=AccumulateGrad]
	140588144527120 -> 140584557478096
	140587345141664 [label="stage4.2.fuse_layers.0.3.1.bias
 (48)" fillcolor=lightblue]
	140587345141664 -> 140588144527120
	140588144527120 [label=AccumulateGrad]
	140588144526352 -> 140588144525904
	140588144526352 [label=UpsampleBilinear2DBackward1]
	140588144526672 -> 140588144526352
	140588144526672 [label=ReluBackward1]
	140584557488144 -> 140588144526672
	140584557488144 [label=AddBackward0]
	140588144526928 -> 140584557488144
	140588144526928 [label=AddBackward0]
	140584557539088 -> 140588144526928
	140584557539088 [label=AddBackward0]
	140584557539280 -> 140584557539088
	140584557539280 [label=NativeBatchNormBackward]
	140584557562640 -> 140584557539280
	140584557562640 [label=MkldnnConvolutionBackward]
	140588144526864 -> 140584557562640
	140584557562192 -> 140584557562640
	140587345179664 [label="stage4.2.fuse_layers.1.0.0.0.weight
 (96, 48, 3, 3)" fillcolor=lightblue]
	140587345179664 -> 140584557562192
	140584557562192 [label=AccumulateGrad]
	140584557561232 -> 140584557539280
	140587345179904 [label="stage4.2.fuse_layers.1.0.0.1.weight
 (96)" fillcolor=lightblue]
	140587345179904 -> 140584557561232
	140584557561232 [label=AccumulateGrad]
	140584557561680 -> 140584557539280
	140587345179984 [label="stage4.2.fuse_layers.1.0.0.1.bias
 (96)" fillcolor=lightblue]
	140587345179984 -> 140584557561680
	140584557561680 [label=AccumulateGrad]
	140588144528144 -> 140584557539088
	140584557537808 -> 140588144526928
	140584557537808 [label=UpsampleBilinear2DBackward1]
	140584557535376 -> 140584557537808
	140584557535376 [label=NativeBatchNormBackward]
	140584557562896 -> 140584557535376
	140584557562896 [label=MkldnnConvolutionBackward]
	140588144538256 -> 140584557562896
	140584557562320 -> 140584557562896
	140587345181024 [label="stage4.2.fuse_layers.1.2.0.weight
 (96, 192, 1, 1)" fillcolor=lightblue]
	140587345181024 -> 140584557562320
	140584557562320 [label=AccumulateGrad]
	140584557563280 -> 140584557535376
	140587345181264 [label="stage4.2.fuse_layers.1.2.1.weight
 (96)" fillcolor=lightblue]
	140587345181264 -> 140584557563280
	140584557563280 [label=AccumulateGrad]
	140584557562832 -> 140584557535376
	140587345181344 [label="stage4.2.fuse_layers.1.2.1.bias
 (96)" fillcolor=lightblue]
	140587345181344 -> 140584557562832
	140584557562832 [label=AccumulateGrad]
	140588144526736 -> 140584557488144
	140588144526736 [label=UpsampleBilinear2DBackward1]
	140584557538320 -> 140588144526736
	140584557538320 [label=NativeBatchNormBackward]
	140584557563600 -> 140584557538320
	140584557563600 [label=MkldnnConvolutionBackward]
	140584557453136 -> 140584557563600
	140583431618896 -> 140584557563600
	140587345182224 [label="stage4.2.fuse_layers.1.3.0.weight
 (96, 384, 1, 1)" fillcolor=lightblue]
	140587345182224 -> 140583431618896
	140583431618896 [label=AccumulateGrad]
	140584557563472 -> 140584557538320
	140587345182464 [label="stage4.2.fuse_layers.1.3.1.weight
 (96)" fillcolor=lightblue]
	140587345182464 -> 140584557563472
	140584557563472 [label=AccumulateGrad]
	140584557562704 -> 140584557538320
	140587345182544 [label="stage4.2.fuse_layers.1.3.1.bias
 (96)" fillcolor=lightblue]
	140587345182544 -> 140584557562704
	140584557562704 [label=AccumulateGrad]
	140588144526288 -> 140588144525904
	140588144526288 [label=UpsampleBilinear2DBackward1]
	140588144527376 -> 140588144526288
	140588144527376 [label=ReluBackward1]
	140584557537488 -> 140588144527376
	140584557537488 [label=AddBackward0]
	140584557560528 -> 140584557537488
	140584557560528 [label=AddBackward0]
	140583431621200 -> 140584557560528
	140583431621200 [label=AddBackward0]
	140583431619920 -> 140583431621200
	140583431619920 [label=NativeBatchNormBackward]
	140583431618832 -> 140583431619920
	140583431618832 [label=MkldnnConvolutionBackward]
	140583431621968 -> 140583431618832
	140583431621968 [label=ReluBackward1]
	140583431622480 -> 140583431621968
	140583431622480 [label=NativeBatchNormBackward]
	140583431655632 -> 140583431622480
	140583431655632 [label=MkldnnConvolutionBackward]
	140588144526864 -> 140583431655632
	140583431655824 -> 140583431655632
	140587399975872 [label="stage4.2.fuse_layers.2.0.0.0.weight
 (48, 48, 3, 3)" fillcolor=lightblue]
	140587399975872 -> 140583431655824
	140583431655824 [label=AccumulateGrad]
	140583431655760 -> 140583431622480
	140587399976112 [label="stage4.2.fuse_layers.2.0.0.1.weight
 (48)" fillcolor=lightblue]
	140587399976112 -> 140583431655760
	140583431655760 [label=AccumulateGrad]
	140583431655504 -> 140583431622480
	140587399976192 [label="stage4.2.fuse_layers.2.0.0.1.bias
 (48)" fillcolor=lightblue]
	140587399976192 -> 140583431655504
	140583431655504 [label=AccumulateGrad]
	140583431622096 -> 140583431618832
	140587399977152 [label="stage4.2.fuse_layers.2.0.1.0.weight
 (192, 48, 3, 3)" fillcolor=lightblue]
	140587399977152 -> 140583431622096
	140583431622096 [label=AccumulateGrad]
	140583431620112 -> 140583431619920
	140587399977392 [label="stage4.2.fuse_layers.2.0.1.1.weight
 (192)" fillcolor=lightblue]
	140587399977392 -> 140583431620112
	140583431620112 [label=AccumulateGrad]
	140583431619792 -> 140583431619920
	140587399977472 [label="stage4.2.fuse_layers.2.0.1.1.bias
 (192)" fillcolor=lightblue]
	140587399977472 -> 140583431619792
	140583431619792 [label=AccumulateGrad]
	140583431620240 -> 140583431621200
	140583431620240 [label=NativeBatchNormBackward]
	140583431619280 -> 140583431620240
	140583431619280 [label=MkldnnConvolutionBackward]
	140588144528144 -> 140583431619280
	140583431656208 -> 140583431619280
	140587399978512 [label="stage4.2.fuse_layers.2.1.0.0.weight
 (192, 96, 3, 3)" fillcolor=lightblue]
	140587399978512 -> 140583431656208
	140583431656208 [label=AccumulateGrad]
	140583431618704 -> 140583431620240
	140587399978752 [label="stage4.2.fuse_layers.2.1.0.1.weight
 (192)" fillcolor=lightblue]
	140587399978752 -> 140583431618704
	140583431618704 [label=AccumulateGrad]
	140583431622608 -> 140583431620240
	140587399978832 [label="stage4.2.fuse_layers.2.1.0.1.bias
 (192)" fillcolor=lightblue]
	140587399978832 -> 140583431622608
	140583431622608 [label=AccumulateGrad]
	140588144538256 -> 140584557560528
	140584557490064 -> 140584557537488
	140584557490064 [label=UpsampleBilinear2DBackward1]
	140583431620304 -> 140584557490064
	140583431620304 [label=NativeBatchNormBackward]
	140583431656144 -> 140583431620304
	140583431656144 [label=MkldnnConvolutionBackward]
	140584557453136 -> 140583431656144
	140583431655952 -> 140583431656144
	140587400016832 [label="stage4.2.fuse_layers.2.3.0.weight
 (192, 384, 1, 1)" fillcolor=lightblue]
	140587400016832 -> 140583431655952
	140583431655952 [label=AccumulateGrad]
	140583431656272 -> 140583431620304
	140587400017072 [label="stage4.2.fuse_layers.2.3.1.weight
 (192)" fillcolor=lightblue]
	140587400017072 -> 140583431656272
	140583431656272 [label=AccumulateGrad]
	140583431656016 -> 140583431620304
	140587400017152 [label="stage4.2.fuse_layers.2.3.1.bias
 (192)" fillcolor=lightblue]
	140587400017152 -> 140583431656016
	140583431656016 [label=AccumulateGrad]
	140588144526224 -> 140588144525904
	140588144526224 [label=UpsampleBilinear2DBackward1]
	140584557536080 -> 140588144526224
	140584557536080 [label=ReluBackward1]
	140583431619472 -> 140584557536080
	140583431619472 [label=AddBackward0]
	140583431656400 -> 140583431619472
	140583431656400 [label=AddBackward0]
	140583431656528 -> 140583431656400
	140583431656528 [label=AddBackward0]
	140583431656336 -> 140583431656528
	140583431656336 [label=NativeBatchNormBackward]
	140583431656592 -> 140583431656336
	140583431656592 [label=MkldnnConvolutionBackward]
	140583431656912 -> 140583431656592
	140583431656912 [label=ReluBackward1]
	140583431656976 -> 140583431656912
	140583431656976 [label=NativeBatchNormBackward]
	140583431657296 -> 140583431656976
	140583431657296 [label=MkldnnConvolutionBackward]
	140583431657488 -> 140583431657296
	140583431657488 [label=ReluBackward1]
	140583431657552 -> 140583431657488
	140583431657552 [label=NativeBatchNormBackward]
	140583431657872 -> 140583431657552
	140583431657872 [label=MkldnnConvolutionBackward]
	140588144526864 -> 140583431657872
	140583431658064 -> 140583431657872
	140587400018192 [label="stage4.2.fuse_layers.3.0.0.0.weight
 (48, 48, 3, 3)" fillcolor=lightblue]
	140587400018192 -> 140583431658064
	140583431658064 [label=AccumulateGrad]
	140583431658000 -> 140583431657552
	140587400018432 [label="stage4.2.fuse_layers.3.0.0.1.weight
 (48)" fillcolor=lightblue]
	140587400018432 -> 140583431658000
	140583431658000 [label=AccumulateGrad]
	140583431657744 -> 140583431657552
	140587400018512 [label="stage4.2.fuse_layers.3.0.0.1.bias
 (48)" fillcolor=lightblue]
	140587400018512 -> 140583431657744
	140583431657744 [label=AccumulateGrad]
	140583431657680 -> 140583431657296
	140587400019472 [label="stage4.2.fuse_layers.3.0.1.0.weight
 (48, 48, 3, 3)" fillcolor=lightblue]
	140587400019472 -> 140583431657680
	140583431657680 [label=AccumulateGrad]
	140583431657424 -> 140583431656976
	140587400019712 [label="stage4.2.fuse_layers.3.0.1.1.weight
 (48)" fillcolor=lightblue]
	140587400019712 -> 140583431657424
	140583431657424 [label=AccumulateGrad]
	140583431657168 -> 140583431656976
	140587400019792 [label="stage4.2.fuse_layers.3.0.1.1.bias
 (48)" fillcolor=lightblue]
	140587400019792 -> 140583431657168
	140583431657168 [label=AccumulateGrad]
	140583431657104 -> 140583431656592
	140587400061808 [label="stage4.2.fuse_layers.3.0.2.0.weight
 (384, 48, 3, 3)" fillcolor=lightblue]
	140587400061808 -> 140583431657104
	140583431657104 [label=AccumulateGrad]
	140583431656848 -> 140583431656336
	140587400062048 [label="stage4.2.fuse_layers.3.0.2.1.weight
 (384)" fillcolor=lightblue]
	140587400062048 -> 140583431656848
	140583431656848 [label=AccumulateGrad]
	140583431656784 -> 140583431656336
	140587400062128 [label="stage4.2.fuse_layers.3.0.2.1.bias
 (384)" fillcolor=lightblue]
	140587400062128 -> 140583431656784
	140583431656784 [label=AccumulateGrad]
	140583431656720 -> 140583431656528
	140583431656720 [label=NativeBatchNormBackward]
	140583431655568 -> 140583431656720
	140583431655568 [label=MkldnnConvolutionBackward]
	140583431658256 -> 140583431655568
	140583431658256 [label=ReluBackward1]
	140583431657360 -> 140583431658256
	140583431657360 [label=NativeBatchNormBackward]
	140583431658384 -> 140583431657360
	140583431658384 [label=MkldnnConvolutionBackward]
	140588144528144 -> 140583431658384
	140583431658512 -> 140583431658384
	140587400063168 [label="stage4.2.fuse_layers.3.1.0.0.weight
 (96, 96, 3, 3)" fillcolor=lightblue]
	140587400063168 -> 140583431658512
	140583431658512 [label=AccumulateGrad]
	140583431655888 -> 140583431657360
	140587400063408 [label="stage4.2.fuse_layers.3.1.0.1.weight
 (96)" fillcolor=lightblue]
	140587400063408 -> 140583431655888
	140583431655888 [label=AccumulateGrad]
	140583431657936 -> 140583431657360
	140587400063488 [label="stage4.2.fuse_layers.3.1.0.1.bias
 (96)" fillcolor=lightblue]
	140587400063488 -> 140583431657936
	140583431657936 [label=AccumulateGrad]
	140583431658320 -> 140583431655568
	140587400064448 [label="stage4.2.fuse_layers.3.1.1.0.weight
 (384, 96, 3, 3)" fillcolor=lightblue]
	140587400064448 -> 140583431658320
	140583431658320 [label=AccumulateGrad]
	140583431658448 -> 140583431656720
	140587400064688 [label="stage4.2.fuse_layers.3.1.1.1.weight
 (384)" fillcolor=lightblue]
	140587400064688 -> 140583431658448
	140583431658448 [label=AccumulateGrad]
	140583431657616 -> 140583431656720
	140587400064768 [label="stage4.2.fuse_layers.3.1.1.1.bias
 (384)" fillcolor=lightblue]
	140587400064768 -> 140583431657616
	140583431657616 [label=AccumulateGrad]
	140583431656080 -> 140583431656400
	140583431656080 [label=NativeBatchNormBackward]
	140583431656656 -> 140583431656080
	140583431656656 [label=MkldnnConvolutionBackward]
	140588144538256 -> 140583431656656
	140583431658896 -> 140583431656656
	140587400106864 [label="stage4.2.fuse_layers.3.2.0.0.weight
 (384, 192, 3, 3)" fillcolor=lightblue]
	140587400106864 -> 140583431658896
	140583431658896 [label=AccumulateGrad]
	140583431658704 -> 140583431656080
	140587400107104 [label="stage4.2.fuse_layers.3.2.0.1.weight
 (384)" fillcolor=lightblue]
	140587400107104 -> 140583431658704
	140583431658704 [label=AccumulateGrad]
	140583431657808 -> 140583431656080
	140587400107184 [label="stage4.2.fuse_layers.3.2.0.1.bias
 (384)" fillcolor=lightblue]
	140587400107184 -> 140583431657808
	140583431657808 [label=AccumulateGrad]
	140584557453136 -> 140583431619472
	140588144526032 -> 140588144525712
	140587400108784 [label="last_layer.0.weight
 (720, 720, 1, 1)" fillcolor=lightblue]
	140587400108784 -> 140588144526032
	140588144526032 [label=AccumulateGrad]
	140588144525584 -> 140588144525712
	140587400108864 [label="last_layer.0.bias
 (720)" fillcolor=lightblue]
	140587400108864 -> 140588144525584
	140588144525584 [label=AccumulateGrad]
	140588144525840 -> 140588144525456
	140587400109104 [label="last_layer.1.weight
 (720)" fillcolor=lightblue]
	140587400109104 -> 140588144525840
	140588144525840 [label=AccumulateGrad]
	140588144525520 -> 140588144525456
	140587400109184 [label="last_layer.1.bias
 (720)" fillcolor=lightblue]
	140587400109184 -> 140588144525520
	140588144525520 [label=AccumulateGrad]
	140587849424720 -> 140587849424528
	140587400109904 [label="last_layer.3.weight
 (19, 720, 1, 1)" fillcolor=lightblue]
	140587400109904 -> 140587849424720
	140587849424720 [label=AccumulateGrad]
	140587849424784 -> 140587849424528
	140587400109984 [label="last_layer.3.bias
 (19)" fillcolor=lightblue]
	140587400109984 -> 140587849424784
	140587849424784 [label=AccumulateGrad]
	140587849424464 -> 140587589136048
}
